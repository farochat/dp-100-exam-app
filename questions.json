[
    {
      "question_id": "1",
      "type": "multiple_choice_single_answer",
      "question": "You train a machine learning model. You must deploy the model as a real-time inference service for testing. The service requires low CPU utilization and less than 48 MB of RAM. The compute target for the deployed service must initialize automatically while minimizing cost and administrative overhead. Which compute target should you use?",
      "options": [
        {
          "id": "A",
          "text": "Azure Machine Learning compute cluster",
          "is_correct": false,
          "explanation": "Azure Machine Learning compute cluster is more suited for training machine learning models and running batch inference jobs rather than real-time inference services. It may not be the most cost-effective or efficient option for a service that requires low CPU utilization and minimal memory usage."
        },
        {
          "id": "B",
          "text": "Azure Container Instance (ACI)",
          "is_correct": true,
          "explanation": "Azure Container Instance (ACI) is the correct choice for this scenario as it allows you to deploy containers quickly without managing the underlying infrastructure. ACI is lightweight, cost-effective, and can be automatically scaled based on demand, making it ideal for real-time inference services with low CPU and memory requirements."
        },
        {
          "id": "C",
          "text": "Azure Kubernetes Service (AKS) inference cluster",
          "is_correct": false,
          "explanation": "Azure Kubernetes Service (AKS) inference cluster is designed for managing containerized applications and services at scale. While it provides scalability and flexibility, it may introduce unnecessary complexity and administrative overhead for a simple real-time inference service with low resource requirements."
        },
        {
          "id": "D",
          "text": "Attached Azure Databricks cluster",
          "is_correct": false,
          "explanation": "Using an Attached Azure Databricks cluster is not the best option for this scenario as it is more suitable for distributed data engineering and machine learning workloads. It may not be cost-effective or efficient for deploying a real-time inference service with low CPU and memory requirements."
        }
      ],
      "feedback": "ACI is the best option for this scenario due to its simplicity and cost-effectiveness."
    },
    {
      "question_id": "2",
      "type": "true_false",
      "question": "You are planning to make use of Azure Machine Learning designer to train models. You need choose a suitable compute type. Recommendation: You choose Compute cluster. Will the requirements be satisfied?",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": true,
          "explanation": "Azure Machine Learning Designer requires a compute cluster to execute training pipelines. Compute clusters are scalable and can be automatically provisioned, making them the correct choice for this scenario."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": false,
          "explanation": "A compute cluster is actually the recommended and supported compute type for training models in Azure ML Designer. Selecting 'No' would be incorrect."
        }
      ],
      "feedback": "Azure ML Designer runs training jobs using compute clusters, which are ideal for parallelized and scalable training tasks."
    },
    {
      "question_id": "3",
      "type": "multiple_choice_single_answer",
      "question": "After training a model, it is important to evaluate its performance. There are many performance metrics and methodologies for evaluating how well a model makes predictions. Which evaluation technique is best described by 'A metric between 0 and 1 based on the square of the differences between predicted and true values. The closer to 0 this metric is, the better the model is performing. Because this metric is relative, it can be used to compare models where the labels are in different units.'?",
      "options": [
        {
          "id": "A",
          "text": "Coefficient of Determination (R2)",
          "is_correct": false,
          "explanation": "R2 measures the proportion of variance explained by the model, but does not provide a relative error metric in the 0-1 range as described."
        },
        {
          "id": "B",
          "text": "Root Mean Squared Error (RMSE)",
          "is_correct": false,
          "explanation": "RMSE gives absolute error in the same units as the target variable, but is not scaled between 0 and 1 or considered relative."
        },
        {
          "id": "C",
          "text": "Mean Absolute Error (MAE)",
          "is_correct": false,
          "explanation": "MAE measures average absolute difference between prediction and truth, but is not a relative metric or scaled between 0 and 1."
        },
        {
          "id": "D",
          "text": "Relative Squared Error (RSE)",
          "is_correct": true,
          "explanation": "Relative Squared Error (RSE) is a metric that calculates the relative performance of a model based on the square of the differences between predicted and true values. It provides a metric between 0 and 1, where lower values indicate better model performance, and it can be used to compare models with labels in different units."
        }
      ],
      "feedback": "RSE is useful when comparing model performance across datasets with different scales or units."
    },
    {
      "question_id": "4",
      "type": "multiple_choice_single_answer",
      "question": "TO Bike Rentals is a bicycle rental company located in downtown Toronto Canada with several locations around the greater Toronto area and they are planning to use Azure Machine Learning models based on seasonal and meteorological features. Which of the following represent specific data files or tables that you plan to work with within Azure ML?",
      "options": [
        {
          "id": "A",
          "text": "Datasets",
          "is_correct": true,
          "explanation": "Datasets are specific data files or tables that you work with in Azure ML. They contain the structured or unstructured data that you will use to train machine learning models."
        },
        {
          "id": "B",
          "text": "Rows",
          "is_correct": false,
          "explanation": "Rows are the horizontal structures within a dataset that represent individual data points or observations. While crucial for data analysis, they are not standalone data files or tables in Azure ML."
        },
        {
          "id": "C",
          "text": "Headers",
          "is_correct": false,
          "explanation": "Headers represent the column names or feature identifiers in a dataset. They are part of a dataset structure but not standalone files or tables in Azure ML."
        },
        {
          "id": "D",
          "text": "Columns",
          "is_correct": false,
          "explanation": "Columns refer to the vertical structures within a dataset that represent specific variables or features. While important for data analysis, they are not standalone data files or tables in Azure ML."
        },
        {
          "id": "E",
          "text": "Environments",
          "is_correct": false,
          "explanation": "Environments in Azure ML refer to the configurations and dependencies needed to run machine learning experiments. They are not specific data files or tables that you work with in Azure ML."
        },
        {
          "id": "F",
          "text": "VMs",
          "is_correct": false,
          "explanation": "VMs (Virtual Machines) provide compute power but are not data files or tables. They are part of the infrastructure used to process data."
        }
      ],
      "feedback": "In Azure ML, datasets are the objects that represent the actual data used for training and evaluation."
    },
    {
      "question_id": "5",
      "type": "multiple_choice_single_answer",
      "question": "You are solving a classification task. You must evaluate your model on a limited data sample by using k-fold cross-validation. You start by configuring a k parameter as the number of splits. You need to configure the k parameter for the cross-validation. Which value should you use?",
      "options": [
        {
          "id": "A",
          "text": "K=0.5",
          "is_correct": false,
          "explanation": "K must be a positive integer representing the number of splits. A decimal value like 0.5 is invalid for k-fold cross-validation."
        },
        {
          "id": "B",
          "text": "K=0.01",
          "is_correct": false,
          "explanation": "K must be an integer greater than 1. A value of 0.01 is invalid for defining the number of folds in cross-validation."
        },
        {
          "id": "C",
          "text": "K=1",
          "is_correct": false,
          "explanation": "K=1 would not perform any splitting, which makes it unsuitable for cross-validation. Minimum valid value is 2."
        },
        {
          "id": "D",
          "text": "K=5",
          "is_correct": true,
          "explanation": "K=5 is a commonly used and valid setting for k-fold cross-validation. It provides a good balance between bias and variance, especially when working with limited data."
        }
      ],
      "feedback": "In k-fold cross-validation, k must be an integer ≥2. A value of 5 is standard and recommended in many cases."
    },
    {
      "question_id": "6",
      "type": "multiple_choice_single_answer",
      "question": "You plan to use hyperparameter tuning to find optimal discrete values for a set of hyperparameters. You want to try every possible combination of a set of specified discrete values. Which kind of sampling should you use?",
      "options": [
        {
          "id": "A",
          "text": "Bayesian Sampling",
          "is_correct": false,
          "explanation": "Bayesian Sampling uses past evaluation results to decide which hyperparameter values to try next, aiming to find the optimal values more efficiently. However, it does not ensure that every possible combination of discrete values will be explored, making it less suitable for exhaustive grid search."
        },
        {
          "id": "B",
          "text": "Grid Sampling",
          "is_correct": true,
          "explanation": "Grid Sampling is the correct choice for trying every possible combination of specified discrete values for hyperparameters. It systematically explores all combinations in a grid-like fashion, ensuring that no combination is missed during the hyperparameter tuning process."
        },
        {
          "id": "C",
          "text": "Random Sampling",
          "is_correct": false,
          "explanation": "Random Sampling involves randomly selecting combinations of hyperparameter values from the specified discrete values. While it can be effective in some cases, it does not guarantee that every possible combination will be tried, which is essential for grid search hyperparameter tuning."
        }
      ],
      "feedback": "Grid Sampling is used when exhaustive exploration of all discrete combinations is required."
    },
    {
      "question_id": "7",
      "type": "multiple_choice_single_answer",
      "question": "You have trained a model using a dataset containing data that was collected last year. As this year progresses, you will collect new data. You want to track any changing data trends that might affect the performance of the model. What should you do?",
      "options": [
        {
          "id": "A",
          "text": "Collect the new data in a new version of the existing training dataset, and profile both datasets.",
          "is_correct": false,
          "explanation": "Creating a new version of the training dataset does not support effective tracking of data drift, as this approach does not isolate the datasets for comparison."
        },
        {
          "id": "B",
          "text": "Replace the training dataset with a new dataset that contains both the original training data and the new data.",
          "is_correct": false,
          "explanation": "By combining all data into a single dataset, you lose the ability to isolate the impact of the new data, making it harder to detect data drift accurately."
        },
        {
          "id": "C",
          "text": "Collect the new data in a separate dataset and create a Data Drift Monitor with the training dataset as a baseline and the new dataset as a target.",
          "is_correct": true,
          "explanation": "This is the correct approach. Data Drift Monitor allows you to compare two datasets and detect any significant changes in the distribution of data over time."
        }
      ],
      "feedback": "Collecting the new data in a separate dataset and creating a Data Drift Monitor with the training dataset as a baseline and the new dataset as a target is the correct approach. This method allows you to compare the performance of the model with the original training data against the new data, enabling you to track any changing data trends that might affect the model's performance. This functionality is natively supported in Azure ML SDK v2 using `azure.ai.ml.monitor.DataDriftSignal` and `MonitorSchedule`."
    },
    {
      "question_id": "8",
      "type": "multiple_choice_single_answer",
      "question": "You are using Azure Machine Learning designer to create a training pipeline for a binary classification model. You have added a dataset containing features and labels, a Two-Class Decision Forest module, and a Train Model module. You plan to use Score Model and Evaluate Model modules to test the trained model with a subset of the dataset that was not used for training. Which additional kind of module should you add?",
      "options": [
        {
          "id": "A",
          "text": "Join Data",
          "is_correct": false,
          "explanation": "Join Data is used to merge datasets. However, in this case, you need to split data for training/testing, not merge datasets."
        },
        {
          "id": "B",
          "text": "Select Columns in Dataset",
          "is_correct": false,
          "explanation": "While column selection is part of data preprocessing, it is not the most immediate step required to enable evaluation. Splitting the data is more critical at this stage."
        },
        {
          "id": "C",
          "text": "Clustering",
          "is_correct": false,
          "explanation": "Clustering is used for unsupervised learning. This scenario is for a supervised binary classification task."
        },
        {
          "id": "D",
          "text": "Split Data",
          "is_correct": true,
          "explanation": "Split Data is essential for dividing the dataset into training and testing subsets. This is crucial to evaluate the model’s performance on unseen data and prevent overfitting."
        }
      ],
      "sdk_version": "v2",
      "feedback": "Esta pregunta hace parte del ecosistema de Azure Machine Learning SDK v2, específicamente usando el entorno visual 'Azure Machine Learning designer'. Aunque no requiere directamente código en Python, el módulo 'Split Data' es parte fundamental del proceso de entrenamiento y evaluación. En Azure ML Designer (y en trabajos definidos con SDK v2), dividir los datos permite entrenar con una porción del dataset y evaluar con otra, asegurando una estimación honesta del rendimiento del modelo. Este paso es obligatorio para evitar overfitting y se integra con módulos como 'Score Model' y 'Evaluate Model'. Referencia oficial: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-designer",
      "correct_option": "D"
    },
    {
      "question_id": "9",
      "type": "multiple_choice_single_answer",
      "question": "Peter plans to have the IT team run the Hyperopt function `fmin()`. Which arguments are needed to run this function?",
      "options": [
        {
          "id": "A",
          "text": "The evaluation metric, the model, and the data.",
          "is_correct": false,
          "explanation": "These are general ML pipeline components but are not the specific arguments required by the `fmin()` function in Hyperopt."
        },
        {
          "id": "B",
          "text": "The evaluation metric, the model, objective function, and the model.",
          "is_correct": false,
          "explanation": "This option redundantly mentions the model and includes irrelevant elements like the evaluation metric. `fmin()` requires an objective function, search space, and search algorithm."
        },
        {
          "id": "C",
          "text": "The objective function, the search space, and the model.",
          "is_correct": false,
          "explanation": "Although this option includes two of the required arguments, it misses the `algo` parameter, which defines the search algorithm to be used."
        },
        {
          "id": "D",
          "text": "The objective function, the search space, and the search algorithm.",
          "is_correct": true,
          "explanation": "These are the required parameters for the `fmin()` function: an objective function to minimize, a search space defining the hyperparameters, and a search algorithm (e.g., `tpe.suggest`, `random.suggest`)."
        }
      ],
      "explanation": "To use the Hyperopt `fmin()` function effectively, three arguments are mandatory: `fn` (the objective function), `space` (the search space), and `algo` (the search algorithm). This function is part of the Hyperopt library, typically used in hyperparameter tuning workflows where the goal is to minimize a loss function by evaluating different hyperparameter configurations.\n\nFor example:\n```python\nfrom hyperopt import fmin, tpe, hp\n\ndef objective(params):\n    loss = some_model_training_and_evaluation(params)\n    return loss\n\nbest = fmin(fn=objective, space={'lr': hp.uniform('lr', 0.001, 0.1)}, algo=tpe.suggest, max_evals=100)\n```\nThis function is **not** part of the Azure Machine Learning Python SDK v2, but can be used **within** an AzureML pipeline or job by embedding it into a script step. The logic behind this question is important to understand automated hyperparameter tuning when designing custom training logic outside of built-in AutoML.",
      "sdk_version": "not part of Azure SDK v2",
      "exclude_from_exam": true
    },
    {
      "question_id": "10",
      "type": "multiple_choice_single_answer",
      "question": "Peter plans to have the IT team run the Hyperopt function fmin(). Which arguments are needed to run this function?",
      "options": [
        {
          "id": "A",
          "text": "The evaluation metric, the model, and the data.",
          "is_correct": false,
          "explanation": "These are important components in ML, but not the required arguments for Hyperopt's fmin()."
        },
        {
          "id": "B",
          "text": "The evaluation metric, the model, objective function, and the model.",
          "is_correct": false,
          "explanation": "This includes repeated or unnecessary elements. The correct arguments are the objective function, search space, and search algorithm."
        },
        {
          "id": "C",
          "text": "The objective function, the search space, and the model.",
          "is_correct": false,
          "explanation": "This omits the search algorithm, which is required by Hyperopt's fmin() to run optimization."
        },
        {
          "id": "D",
          "text": "The objective function, the search space, and the search algorithm.",
          "is_correct": true,
          "explanation": "These are the required arguments for the Hyperopt function fmin()."
        }
      ],
      "correct_option": "D",
      "sdk_version": "python_v2",
      "feedback": "The `fmin()` function in Hyperopt is used to minimize an objective function over a search space using a specified search algorithm. The required arguments are:\n\n- `fn`: the objective function to minimize\n- `space`: the hyperparameter search space (defined with `hp.uniform`, `hp.choice`, etc.)\n- `algo`: the search algorithm (e.g., `tpe.suggest` or `rand.suggest`)\n\nIn the context of Azure ML SDK v2, you typically use Hyperopt through a script or notebook that defines a custom sweep job. Azure ML wraps this behavior through `SearchSpace`, `Objective`, and `SearchAlgorithm` components inside a `SweepJob`. See official docs for [hyperparameter tuning with v2](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters-v2).",
      "exclude_from_exam": true
    },
    {
      "question_id": "11",
      "type": "multiple_choice_single_answer",
      "question": "You have used the Python SDK for Azure Machine Learning to create a pipeline that trains a model. What do you need to do so that a client application can invoke the pipeline through an HTTP REST endpoint?",
      "options": [
        {
          "id": "A",
          "text": "Create an inference cluster compute target.",
          "is_correct": false,
          "explanation": "Creating an inference cluster compute target is not directly related to enabling a client application to invoke the pipeline through an HTTP REST endpoint. An inference cluster is typically used for model deployment, not for invoking a pipeline."
        },
        {
          "id": "B",
          "text": "Publish the pipeline.",
          "is_correct": true,
          "explanation": "Publishing the pipeline is required to make it accessible via a REST endpoint. Once published, the pipeline can be invoked programmatically using an HTTP POST request, enabling external applications to run the pipeline remotely."
        },
        {
          "id": "C",
          "text": "Rename the pipeline to pipeline_name-production.",
          "is_correct": false,
          "explanation": "Renaming a pipeline does not affect its accessibility via a REST endpoint. Only publishing it exposes it as an HTTP triggerable service."
        }
      ],
      "feedback": "In Azure ML SDK v2, to make a pipeline available for external invocation, it must be published using `ml_client.jobs.create_or_update()` followed by `ml_client.jobs.publish()` or by specifying `is_published=True` in the job definition YAML. This step registers the pipeline as a REST endpoint, allowing it to be triggered by client apps or services. Without publishing, the pipeline is only available internally and cannot be accessed through an HTTP endpoint. Referencia: [Microsoft Docs - Publish a pipeline](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines-v2#publish-a-pipeline)."
    },
    {
      "question_id": "12",
      "type": "multiple_choice_single_answer",
      "question": "You are senior Azure Machine Learning Associate of your company. You are building a recurrent neural network to perform a binary classification. The training loss, validation loss, training accuracy, and validation accuracy of each training epoch has been provided. You need to identify whether the classification model is overfitted. Which of the following is correct?",
      "options": [
        {
          "id": "A",
          "text": "The training loss increases while the validation loss decreases when training the model",
          "is_correct": false,
          "explanation": "This would indicate model underfitting, where the model fails to learn from training data."
        },
        {
          "id": "B",
          "text": "The training loss stays constant and the validation loss decreases when training the model",
          "is_correct": false,
          "explanation": "This suggests stable performance and generalization, but does not indicate overfitting."
        },
        {
          "id": "C",
          "text": "The training loss stays constant and the validation loss stays on a constant value and close to the training loss value when training the model",
          "is_correct": false,
          "explanation": "This indicates that the model is not improving over time and might lack complexity, which implies underfitting rather than overfitting."
        },
        {
          "id": "D",
          "text": "The training loss decreases while the validation loss increases when training the model",
          "is_correct": true,
          "explanation": "This pattern clearly shows overfitting: the model fits the training data increasingly well, but performs worse on unseen validation data."
        }
      ],
      "feedback": "Overfitting occurs when a model learns patterns specific to the training data and fails to generalize to unseen data. In this question, the training loss is decreasing (which means the model is learning well on the training data), but the validation loss increases (meaning the model is not performing well on new, unseen data). This divergence between training and validation losses is a key indicator of overfitting. In real-world scenarios using Azure Machine Learning SDK v2, this kind of monitoring is done through metrics logging in training jobs using MLflow or through visualizations in Azure ML Studio. Techniques to mitigate overfitting include early stopping, regularization, and increasing training data or using data augmentation.",
      "correct_option": "D"
    },
    {
      "question_id": "13",
      "type": "multiple_choice_single_answer",
      "question": "The Eat-More Corporation is a U.S.-based fast-food restaurant chain headed by Teresa Payton. They are building a team data science environment. Requirements include: models must be built using Caffe2 or Chainer, work in disconnected environments, and allow pipeline updates when online. What environment should they use?",
      "options": [
        {
          "id": "A",
          "text": "Azure Databricks",
          "is_correct": false,
          "explanation": "Azure Databricks offers powerful data processing capabilities, but it does not support training with Caffe2 or Chainer directly, nor is it designed to operate in disconnected environments or on personal devices."
        },
        {
          "id": "B",
          "text": "Azure Kubernetes Service (AKS)",
          "is_correct": false,
          "explanation": "AKS is useful for deployment and scaling containerized ML models, but it lacks the full flexibility needed for offline development or custom training environments on personal devices."
        },
        {
          "id": "C",
          "text": "Azure Machine Learning Designer",
          "is_correct": false,
          "explanation": "Azure ML Designer is a low-code tool and does not support custom frameworks like Caffe2 or Chainer. It also cannot run on disconnected devices or enable offline training."
        },
        {
          "id": "D",
          "text": "Azure Machine Learning Service",
          "is_correct": true,
          "explanation": "Azure Machine Learning Service supports custom training scripts using frameworks like Caffe2 or Chainer, and it enables local experimentation on personal devices with offline support and syncing when reconnected."
        }
      ],
      "feedback": "La opción correcta es **Azure Machine Learning Service** porque permite crear entornos personalizados para entrenamiento de modelos ML utilizando cualquier framework compatible con Python, incluidos Caffe2 o Chainer. A diferencia de otros servicios como Azure ML Designer (limitado a módulos predefinidos) o Azure Databricks (enfocado a Spark y big data), el servicio de Azure ML completo soporta ejecución local, sincronización en la nube y desarrollo offline. Esto lo hace ideal para científicos de datos que trabajan en sus propios dispositivos, con necesidad de operar tanto en red como sin conexión, y que luego deben sincronizar sus pipelines o modelos en la nube. Este enfoque es compatible con el SDK v2, donde puedes usar entornos personalizados (`Environment`), pipelines (`PipelineJob`), y programación distribuida cuando estés en línea nuevamente."
    },
    {
      "question_id": "14",
      "type": "multiple_choice_single_answer",
      "question": "You are creating a new Azure Machine Learning pipeline using the designer. The pipeline must train a model using data in a comma-separated values (CSV) file that is published on a website. You have not created a dataset for this file. You need to ingest the data from the CSV file into the designer pipeline using the minimal administrative effort. Which module should you add to the pipeline in Designer?",
      "options": [
        {
          "id": "A",
          "text": "Dataset",
          "is_correct": false,
          "explanation": "The 'Dataset' option requires you to register the dataset manually in Azure ML before using it. Since the question emphasizes minimal administrative effort, this option is not the most efficient for the task."
        },
        {
          "id": "B",
          "text": "Enter Data Manually",
          "is_correct": false,
          "explanation": "Entering data manually is impractical for a CSV file and not suitable for automating data ingestion into a pipeline."
        },
        {
          "id": "C",
          "text": "Import Data",
          "is_correct": true,
          "explanation": "The 'Import Data' module is the correct choice for ingesting data from an external CSV file into the Azure Machine Learning pipeline designer. This module allows you to import data from various sources, including CSV files, with minimal administrative effort, making it the suitable option for this scenario."
        },
        {
          "id": "D",
          "text": "Convert to CSV",
          "is_correct": false,
          "explanation": "The 'Convert to CSV' module is used to convert data into CSV format, not for ingesting an already existing CSV file from an external location."
        }
      ],
      "correct_answer": "C",
      "feedback": "The 'Import Data' module in Azure Machine Learning Designer is designed for bringing external data—such as CSV files hosted on websites—into your pipeline without the need to first register it manually as a dataset. This greatly reduces setup complexity and supports scenarios where agility and minimal overhead are required. This approach is recommended when dealing with simple data ingestion pipelines, especially in early prototyping or when dynamically sourcing data. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data"
    },
    {
      "question_id": "15",
      "type": "multiple_choice_single_answer",
      "question": "You are deploying a real-time inference endpoint using the Azure ML SDK v2. You want to emit custom metrics (e.g., latency or prediction count) from your scoring script and analyze them using Application Insights for monitoring purposes. What should you do inside the scoring script?",
      "options": [
        {
          "id": "A",
          "text": "Use mlflow.log_metric() to log the custom metrics.",
          "is_correct": false,
          "explanation": "mlflow.log_metric() is used during training jobs to log experiment metrics. It is not used within the scoring script of a deployed endpoint."
        },
        {
          "id": "B",
          "text": "Use print statements or logging to STDOUT in the scoring script.",
          "is_correct": true,
          "explanation": "STDOUT output from the scoring script is captured by Azure ML and automatically routed to Application Insights. This is the recommended way to log telemetry during inference."
        },
        {
          "id": "C",
          "text": "Save metrics in a local file inside the /outputs directory.",
          "is_correct": false,
          "explanation": "Saving to the /outputs directory is used for artifacts in training jobs, not telemetry in inference endpoints."
        },
        {
          "id": "D",
          "text": "Use run.log() from azureml.core.Run inside the script.",
          "is_correct": false,
          "explanation": "run.log() is a v1 SDK method used in training scripts, and is not supported in the SDK v2 or in deployed endpoints."
        }
      ],
      "explanation": "In SDK v2, when deploying a model as a real-time endpoint, the proper way to emit telemetry (such as latency or prediction statistics) is by writing to STDOUT using print() or logging. Azure automatically collects these logs and sends them to Application Insights, allowing you to monitor and analyze them for health checks and diagnostics.",
      "sdk_version": "v2"
    },
    {
      "question_id": "16",
      "type": "multiple_choice_single_answer",
      "question": "You deploy a real-time inference service for a trained model. The deployed model supports a business-critical application, and it is important to be able to monitor the data submitted to the web service and the predictions the data generates. You need to implement a monitoring solution for the deployed model using minimal administrative effort. What should you do?",
      "options": [
        {
          "id": "A",
          "text": "View the log files generated by the experiment used to train the model.",
          "is_correct": false,
          "explanation": "Training experiment logs provide insights into the training process but do not allow monitoring of real-time inference requests or outputs once the model is deployed."
        },
        {
          "id": "B",
          "text": "View the explanations for the registered model in Azure ML studio.",
          "is_correct": false,
          "explanation": "Model explanations help you interpret model predictions, but they are not intended for monitoring the web service's request/response flow or telemetry."
        },
        {
          "id": "C",
          "text": "Create an ML Flow tracking URI that references the endpoint, and view the data logged by ML Flow.",
          "is_correct": false,
          "explanation": "MLflow logging is effective during training and experimentation. However, for real-time model deployment monitoring, Application Insights is the preferred minimal-effort solution."
        },
        {
          "id": "D",
          "text": "Enable Azure Application Insights for the service endpoint and view logged data in the Azure portal.",
          "is_correct": true,
          "explanation": "Azure Application Insights is the recommended way to monitor real-time endpoints in Azure Machine Learning. It captures request/response telemetry and logs automatically when enabled, with minimal administrative effort."
        }
      ],
      "feedback": "To monitor real-time inferencing services deployed in Azure Machine Learning, the best practice is to enable Azure Application Insights on the endpoint. This allows telemetry such as request count, latency, and errors to be captured and viewed in the Azure portal. This monitoring helps ensure the health and performance of business-critical ML applications with minimal setup."
    },
    {
      "question_id": "17",
      "type": "multiple_choice_single_answer",
      "question": "You are a data scientist of your company and are asked implementing a machine learning model to predict stock prices. The model uses a PostgreSQL database and requires GPU processing. You need to create a virtual machine that is pre-configured with the required tools. What should you do?",
      "options": [
        {
          "id": "A",
          "text": "Create a Geo AI Data Science Virtual Machine (Geo-DSVM) Windows edition.",
          "is_correct": false,
          "explanation": "Geo-DSVMs are tailored for geospatial analytics and not designed for GPU-intensive deep learning tasks."
        },
        {
          "id": "B",
          "text": "Create a Deep Learning Virtual Machine (DLVM) Linux edition.",
          "is_correct": true,
          "explanation": "The DLVM Linux edition is optimized for deep learning workloads and comes pre-configured with GPU support and popular frameworks. Linux also offers better compatibility and performance for GPU tasks."
        },
        {
          "id": "C",
          "text": "Create a Data Science Virtual Machine (DSVM) Windows edition.",
          "is_correct": false,
          "explanation": "While DSVM is a good general-purpose option, it is not optimized for deep learning tasks that require GPU acceleration."
        },
        {
          "id": "D",
          "text": "Create a Deep Learning Virtual Machine (DLVM) Windows edition.",
          "is_correct": false,
          "explanation": "The Windows edition of DLVM is less commonly used for deep learning due to compatibility and performance issues with GPU frameworks compared to Linux."
        }
      ],
      "feedback": "The DLVM Linux edition is the recommended choice for deep learning workloads involving GPU acceleration. It supports major frameworks like TensorFlow, PyTorch, and others, and is optimized for use in research and production environments. Linux provides superior driver and hardware compatibility, making it ideal for intensive ML tasks such as stock price prediction."
    },
    {
      "question_id": "18",
      "type": "true_false",
      "question": "You have been tasked with employing a machine learning model, which makes use of a PostgreSQL database and needs GPU processing, to forecast prices. You are preparing to create a virtual machine that has the necessary tools built into it. You need to make use of the correct virtual machine type.\n\nRecommendation: You make use of a Geo AI Data Science Virtual Machine (Geo-DSVM) Windows edition.\n\nWill the requirements be satisfied?",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": false,
          "explanation": "Geo-DSVMs are designed primarily for geospatial analytics and do not provide the necessary GPU support or tooling for deep learning tasks like time series forecasting using PostgreSQL data."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": true,
          "explanation": "Geo-DSVM is not suitable for GPU-based deep learning workloads. A better alternative would be the Deep Learning Virtual Machine (DLVM) Linux edition, which includes GPU drivers and frameworks optimized for machine learning tasks."
        }
      ],
      "feedback": "The Geo AI Data Science Virtual Machine is intended for geospatial tasks and lacks optimized support for GPU-accelerated deep learning. For forecasting models requiring PostgreSQL integration and GPU support, a DLVM Linux edition is more appropriate. It includes the necessary drivers, libraries, and frameworks for such ML scenarios."
    },
    {
      "question_id": "19",
      "type": "true_false",
      "question": "True or False: Ordinal encoding is often the recommended approach, and it involves transforming each categorical value into n (= number of categories) binary values, with one of them 1, and all others 0.",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": false,
          "explanation": "This statement describes One-Hot Encoding, not Ordinal Encoding. Ordinal Encoding assigns a unique integer to each category, preserving some order (if it exists), while One-Hot Encoding creates n binary columns where each column represents one category."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": true,
          "explanation": "Correct. The description given corresponds to One-Hot Encoding, not Ordinal Encoding. Ordinal Encoding is typically used when the categorical data has an inherent order, such as 'low', 'medium', 'high'."
        }
      ],
      "feedback": "It's important to distinguish between One-Hot Encoding and Ordinal Encoding. While One-Hot Encoding creates binary variables and is recommended when there is no natural order among categories, Ordinal Encoding is better suited for ordered categories. Misapplying these encodings can lead to incorrect model assumptions and degraded performance."
    },
    {
      "question_id": "20",
      "type": "multiple_choice_single_answer",
      "question": "You create a deep learning model for image recognition on Azure Machine Learning service using GPU-based training. You must deploy the model to a context that allows for real-time GPU-based inferencing. You need to configure compute resources for model inferencing. Which compute type should you use?",
      "options": [
        {
          "id": "A",
          "text": "Azure Kubernetes Service",
          "is_correct": true,
          "explanation": "Azure Kubernetes Service (AKS) supports GPU-based inferencing and is designed for scalable, real-time deployment of containerized models. It is ideal for scenarios that require real-time GPU inference such as deep learning."
        },
        {
          "id": "B",
          "text": "Machine Learning Compute",
          "is_correct": false,
          "explanation": "This resource is optimized for training workloads and not ideal for real-time inferencing, especially when GPU support is required."
        },
        {
          "id": "C",
          "text": "Azure Container Instance",
          "is_correct": false,
          "explanation": "ACI does not support GPU-based inference, making it unsuitable for deploying deep learning models that require such compute capabilities."
        },
        {
          "id": "D",
          "text": "Field Programmable Gate Array",
          "is_correct": false,
          "explanation": "FPGAs are not the standard or most practical option for real-time deployment of deep learning models requiring GPU-based inferencing."
        }
      ],
      "feedback": "AKS is the preferred deployment target for real-time GPU inference in Azure Machine Learning because it provides the scalability, container management, and GPU support necessary for production-level deep learning models. Refer to: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where for details on compute targets."
    },
    {
      "question_id": "21",
      "type": "true_false",
      "question": "You train and register a machine learning model. You plan to deploy the model as a real-time web service. Applications must use key-based authentication to use the model. You need to deploy the web service.\n\nSolution:\n- Create an AksWebservice instance.\n- Set the value of the auth_enabled property to False.\n- Set the value of the token_auth_enabled property to True.\n- Deploy the model to the service.\n\nDoes the solution meet the goal?",
      "options": [
        {
          "id": "A",
          "text": "No",
          "is_correct": true,
          "explanation": "The requirement is key-based authentication. Setting `auth_enabled=False` disables key-based auth. Setting `token_auth_enabled=True` enables token-based authentication. This setup does not meet the requirement for key-based authentication."
        },
        {
          "id": "B",
          "text": "Yes",
          "is_correct": false,
          "explanation": "Enabling token-based authentication while disabling key-based auth does not fulfill the requirement. The correct configuration would be `auth_enabled=True` and `token_auth_enabled=False` to use key-based authentication."
        }
      ],
      "feedback": "For key-based authentication on an AKS deployment using Python SDK v1, you must set `auth_enabled=True`. The proposed configuration enables token-based auth instead. Since the scenario requires key-based auth, the solution does **not** meet the goal. Refer to: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where#authentication"
    },
    {
      "question_id": "22",
      "type": "multiple_choice_single_answer",
      "question": "It's common practice to train the model using all available data, then reuse some of the training data to test the trained model.",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": false,
          "explanation": "Reutilizar los mismos datos usados para entrenar el modelo al evaluarlo puede llevar a sobreajuste y resultados engañosos. La evaluación debe realizarse sobre datos nunca vistos durante el entrenamiento para medir el rendimiento real del modelo."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": true,
          "explanation": "La buena práctica es dividir el dataset en conjuntos separados: entrenamiento, validación y prueba. Esto garantiza que el modelo se evalúe correctamente sobre datos nuevos y generalice bien."
        }
      ],
      "sdk_version": "agnostic",
      "include_in_bank": true,
      "feedback": "Nunca se debe usar el mismo conjunto de datos para entrenar y probar un modelo, ya que no permite medir su capacidad de generalización. En el contexto de Azure Machine Learning, se recomienda separar los datos usando técnicas como `train_test_split` o registros distintos dentro del asset MLTable.",
      "correct_option": "B",
      "exclude_from_exam": true
    },
    {
      "question_id": "23",
      "type": "true_false",
      "question": "It’s common practice to train the model using all available data, then reuse some of the training data to test the trained model.",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": false,
          "explanation": "Reusing training data for testing leads to biased evaluation results, as the model has already seen this data."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": true,
          "explanation": "Best practices recommend separating data into training and testing sets to ensure unbiased evaluation of the model’s performance."
        }
      ],
      "feedback": "Reusing training data to evaluate a model causes data leakage, leading to overly optimistic performance metrics. It is a fundamental principle in machine learning to separate training and test sets so that the evaluation reflects how the model performs on unseen data. This practice ensures generalizability and prevents overfitting."
    },
    {
      "question_id": "24",
      "type": "multiple_choice_single_answer",
      "question": "You plan to run a script as an experiment using a Script Run Configuration. The script uses modules from the scipy library as well as several Python packages that are not typically installed in a default conda environment. You plan to run the experiment on your local workstation for small datasets and scale out the experiment by running it on more powerful remote compute clusters for larger datasets. You need to ensure that the experiment runs successfully on local and remote compute with the least administrative effort. What should you do?",
      "options": [
        {
          "id": "A",
          "text": "Create a virtual machine (VM) with the required Python configuration and attach the VM as a compute target. Use this compute target for all experiment runs.",
          "is_correct": false,
          "explanation": "This approach works but requires significantly more administrative effort than necessary."
        },
        {
          "id": "B",
          "text": "Create and register an Environment that includes the required packages. Use this Environment for all experiment runs.",
          "is_correct": true,
          "explanation": "This is the most efficient approach. It allows consistent experiment runs across compute targets with minimal overhead."
        },
        {
          "id": "C",
          "text": "Create a config.yaml file defining the conda packages that are required and save the file in the experiment folder.",
          "is_correct": false,
          "explanation": "Manually maintaining a config.yaml file can lead to inconsistencies and extra effort when reused across experiments."
        },
        {
          "id": "D",
          "text": "Always run the experiment with an Estimator by using the default packages.",
          "is_correct": false,
          "explanation": "The default packages may not include the required libraries like scipy, leading to failures during execution."
        },
        {
          "id": "E",
          "text": "Do not specify an environment in the run configuration for the experiment. Run the experiment by using the default environment.",
          "is_correct": false,
          "explanation": "The default environment is unlikely to include required libraries such as scipy, so this may result in failed runs."
        }
      ],
      "feedback": "Registering an Azure ML Environment is the recommended approach when working with custom dependencies, particularly when experiments must run consistently across different compute targets (e.g., local and remote). By registering the environment, you can version, reuse, and manage it without manually replicating dependencies in each compute node. This approach is efficient, reduces errors due to inconsistent environments, and integrates seamlessly with both ScriptRunConfig and command jobs in Azure ML SDK v2.\n\nIn SDK v2, you typically define an environment in a YAML file or directly in Python using the `Environment` class, and then register it with the ML client:\n\n```python\nfrom azure.ai.ml.entities import Environment\nfrom azure.ai.ml import MLClient\n\nenv = Environment(\n    name=\"my-env\",\n    image=\"mcr.microsoft.com/azureml/sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    conda_file=\"envs/conda.yaml\"\n)\nml_client.environments.create_or_update(env)\n```\n\nOnce created, you can reference this environment in any job. This aligns with best practices for MLOps and reduces administrative burden.\n\n[Learn more](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-environments-v2?tabs=python)"
    },
    {
      "question_id": "25",
      "type": "multiple_choice_single_answer",
      "question": "You are holding a workgroup session and the topic is how to cache data into the memory of the local executor for instant access.\nWhich of the following is the correct method?",
      "options": [
        {
          "id": "A",
          "text": ".cache().inMemory()",
          "is_correct": false,
          "explanation": "The method .cache().inMemory() does not conform to the standard practice of caching data into the memory of the local executor for instant access in Azure Machine Learning. It appears to mix up the order of operations and does not clearly indicate the caching process."
        },
        {
          "id": "B",
          "text": ".cache()",
          "is_correct": true,
          "explanation": "The method .cache() is the correct approach for caching data into the memory of the local executor for instant access in Azure Machine Learning. It is the standard method used to store data in memory for quick retrieval and processing."
        },
        {
          "id": "C",
          "text": ".inMemory().save()",
          "is_correct": false,
          "explanation": "The method .inMemory().save() is invalid in PySpark or Spark. It does not represent any recognized caching pattern."
        },
        {
          "id": "D",
          "text": ".inMemory().cache()",
          "is_correct": false,
          "explanation": "This method chaining is incorrect. The function .inMemory() does not exist as a callable method for DataFrames."
        },
        {
          "id": "E",
          "text": ".save().inMemory()",
          "is_correct": false,
          "explanation": "This is not valid Spark syntax. Caching is performed with .cache() before an action, not after a save operation."
        }
      ],
      "feedback": "In Apache Spark, using `.cache()` is the standard way to store a DataFrame or RDD in memory on the local executor for faster subsequent actions. Once `.cache()` is applied and an action like `.count()` or `.show()` is triggered, Spark will store the results in memory. This helps reduce recomputation in iterative processes and is commonly used in Azure Synapse, Databricks, and other Spark-based environments.\n\n```python\ndf = spark.read.csv(\"data.csv\")\ndf.cache()\ndf.show()  # triggers caching\n```\n\nCaching is especially useful when you need low-latency access to reused data during training or data transformation stages."
    },
    {
      "question_id": "26",
      "type": "multiple_choice_single_answer",
      "question": "At its core, Azure Machine Learning is a service for training and managing machine learning models, for which you need compute resources on which to run the training process.\n\nWhich of the following are cloud-based resources on which you can run model training and data exploration processes?",
      "options": [
        {
          "id": "A",
          "text": "Notebooks",
          "is_correct": false,
          "explanation": "Notebooks are development tools used to write and run code, but they are not the compute resources where training and data exploration are executed."
        },
        {
          "id": "B",
          "text": "Workspaces",
          "is_correct": false,
          "explanation": "Workspaces are logical containers for Azure ML assets (like models, environments, compute), but they are not compute resources themselves."
        },
        {
          "id": "C",
          "text": "Compute targets",
          "is_correct": true,
          "explanation": "Compute targets are cloud-based resources in Azure Machine Learning used to run model training and data exploration workloads. These can include virtual machines, Azure Databricks, or other compute clusters that provide the necessary processing power."
        },
        {
          "id": "D",
          "text": "Workloads",
          "is_correct": false,
          "explanation": "Workloads refer to the tasks or jobs being executed, not the compute infrastructure used to run them."
        }
      ],
      "feedback": "The correct answer is 'Compute targets'. These are the actual cloud-based resources where Azure ML executes training and data exploration code. They can include Azure Machine Learning Compute clusters, virtual machines, or attached compute like Azure Databricks. Notebooks and workspaces support development and organization, but they do not perform execution. Understanding the role of compute targets is essential for effective resource management in cloud-based ML workflows."
    },
    {
      "question_id": "27",
      "type": "multiple_choice_single_answer",
      "question": "Scenario: Pym Tech is a U.S.-based Technology manufacturer headed by Hank Pym. Their headquarters is located at Treasure Island, San Francisco California and business is booming. The expansion plans are underway which have presented several IT challenges which Hank has contracted you to advise his IT staff on. At the moment, the topic is building a data engineering and data science development environment with the following requirements.\n\nRequired:\n• Environment to support Python and Scala\n• Environment to compose data storage, movement, and processing services into automated data pipelines\n• The same tool should be used for the orchestration of both data engineering and data science\n• Environment to support workload isolation and interactive workloads\n• Environment to enable scaling across a cluster of machines\n\nThe team’s task is to create the environment. Which of the following should they do?",
      "options": [
        {
          "id": "A",
          "text": "Build the environment in Azure Databricks and use Azure Container Instances for orchestration.",
          "is_correct": false,
          "explanation": "Azure Container Instances (ACI) is suitable for lightweight deployments, but it is not designed for complex orchestration of both data engineering and data science workflows. It lacks the integration and orchestration capabilities required for enterprise-level automation."
        },
        {
          "id": "B",
          "text": "Build the environment in Apache Spark for HDInsight and use Azure Container Instances for orchestration.",
          "is_correct": false,
          "explanation": "Although HDInsight with Apache Spark supports Python, Scala, workload isolation, and scaling, using Azure Container Instances for orchestration fails to meet the requirement of using a single tool for both data engineering and data science."
        },
        {
          "id": "C",
          "text": "Build the environment in Apache Hive for HDInsight and use Azure Data Factory for orchestration.",
          "is_correct": false,
          "explanation": "Apache Hive supports some batch processing workloads but is not well suited for building scalable, interactive data science environments or composing complex pipelines. While Azure Data Factory is a good orchestration tool, Hive doesn't fully meet the interactive and ML-specific needs."
        },
        {
          "id": "D",
          "text": "Build the environment in Azure Databricks and use Azure Data Factory for orchestration.",
          "is_correct": true,
          "explanation": "Azure Databricks is a unified analytics platform built on Apache Spark that supports both Python and Scala. It enables interactive workloads, workload isolation, and scalable processing across a cluster. Combined with Azure Data Factory, which orchestrates pipelines for both data engineering and data science, it fulfills all the scenario's requirements."
        }
      ],
      "feedback": "Azure Databricks is designed for modern big data and machine learning workflows. It supports both Python and Scala, enables rapid scaling, and integrates tightly with Azure Data Factory for orchestrating complex data pipelines. This combination is particularly powerful for hybrid workloads (data engineering + data science) and supports modular, automated workflows. Azure Data Factory provides robust orchestration capabilities that can handle ingestion, transformation, and model training tasks. Therefore, choosing Azure Databricks with Azure Data Factory ensures an enterprise-ready, scalable, and integrated solution that fulfills all outlined needs."
    },
    {
      "question_id": "29",
      "type": "multiple_choice_single_answer",
      "question": "Your manager asked you to solve a classification task. The dataset is imbalanced. You need to select an Azure Machine Learning Studio module to improve the classification accuracy. Which module should you use?",
      "options": [
        {
          "id": "A",
          "text": "Fisher Linear Discriminant Analysis",
          "is_correct": false,
          "explanation": "Fisher Linear Discriminant Analysis is a dimensionality reduction technique that aims to find the linear combination of features that best separates different classes. While it can be useful for feature transformation, it does not directly address the issue of imbalanced datasets or improve classification accuracy in such scenarios."
        },
        {
          "id": "B",
          "text": "Filter Based Feature Selection",
          "is_correct": false,
          "explanation": "Filter Based Feature Selection is not directly related to addressing imbalanced datasets. It is used to select the most relevant features for the model based on statistical measures, but it does not specifically target improving classification accuracy in the presence of imbalanced classes."
        },
        {
          "id": "C",
          "text": "Synthetic Minority Oversampling Technique (SMOTE)",
          "is_correct": true,
          "explanation": "Synthetic Minority Oversampling Technique (SMOTE) is a popular technique used to address imbalanced datasets by generating synthetic samples from the minority class. By oversampling the minority class, SMOTE helps to balance the class distribution and improve the classification accuracy of the model."
        },
        {
          "id": "D",
          "text": "Permutation Feature Importance",
          "is_correct": false,
          "explanation": "Permutation Feature Importance is a method used to evaluate the importance of features in a model by permuting the values of each feature and measuring the impact on the model's performance. While feature importance is important for model interpretability, it does not directly tackle the challenge of imbalanced datasets and improving classification accuracy in such cases."
        }
      ],
      "correct_answer": "C",
      "feedback": "When working with imbalanced classification tasks, the distribution of classes can heavily bias the model toward the majority class, leading to poor predictive performance for the minority class. SMOTE addresses this by synthetically generating new samples for the minority class using a k-nearest neighbors approach. This improves the model's ability to learn decision boundaries that include minority class instances. It's a standard preprocessing step when accuracy, recall, or F1-score for minority classes needs to be improved."
    },
    {
      "question_id": "30",
      "type": "true_false",
      "question": "You are using Azure Machine Learning Studio to perform feature engineering on a dataset.\n\nYou need to normalize values to produce a feature column grouped into bins.\n\nSolution: Apply an Entropy Minimum Description Length (MDL) binning mode.\n\nDoes the solution meet the goal?",
      "options": [
        {
          "id": "A",
          "text": "Yes",
          "is_correct": false,
          "explanation": "Applying Entropy MDL binning is a discretization technique used to bin continuous numerical values based on information gain, not normalization. Normalization transforms values into a specific scale (such as [0, 1] or standard Gaussian), which is a different preprocessing goal. Therefore, this solution does not meet the goal of normalizing values."
        },
        {
          "id": "B",
          "text": "No",
          "is_correct": true,
          "explanation": "Normalization and binning are different operations. Normalization scales numerical values to a specific range, commonly [0, 1] or standard normal distribution. Entropy Minimum Description Length (MDL) binning, on the other hand, is a discretization technique that groups continuous values into bins using entropy-based criteria. Thus, the solution does not satisfy the goal of normalizing values."
        }
      ],
      "correct_answer": "B",
      "feedback": "The use of MDL binning does not fulfill the requirement of normalization. While MDL is a valid method for discretizing continuous variables into categorical bins, normalization aims to adjust the scale of numerical features without converting them into categories. Azure Machine Learning Studio provides separate modules for normalization, such as MinMax or Z-score normalization, which are more appropriate when the goal is to scale features rather than discretize them."
    },
    {
      "question_id": "31",
      "type": "multiple_choice_single_answer",
      "question": "Scenario: You have been contracted by Wayne Enterprises, a company owned by Bruce Wayne with a market value of over twenty-seven million dollars.\n\nBruce founded Wayne Enterprises shortly after he created the Wayne Foundation and he became the president and chairman of the company.\n\nBruce has come to you because his IT team plans to use Microsoft Azure Machine Learning and your expertise is required.\n\nYou are holding a workgroup session and discussing Spark is a distributed computing environment and work is parallelized across executors.\n\nA question was asked by one of the team members as to which two levels does this parallelization occur.\n\nWhich of the following is the answer you should provide them?",
      "options": [
        {
          "id": "A",
          "text": "The Slot and the Task",
          "is_correct": false,
          "explanation": "The parallelization in Spark does not occur between the Slot and the Task. A Slot is a resource allocation on an Executor for task execution, while a Task is a unit of work that will be sent to one Executor to be executed."
        },
        {
          "id": "B",
          "text": "The Driver and the Executor",
          "is_correct": false,
          "explanation": "The Driver and the Executor are components of the Spark architecture. The Driver is responsible for managing the job execution, and the Executors perform the tasks. However, parallelization itself occurs within Executors and their resource allocations (Slots)."
        },
        {
          "id": "C",
          "text": "The Executor and the Slot",
          "is_correct": true,
          "explanation": "Parallelization in Apache Spark occurs between the Executor and the Slot. An Executor is a process launched on a worker node that runs tasks and stores data in memory or disk. A Slot is a unit of resource allocation on an Executor where a Task runs. Each Executor can run multiple Tasks in parallel depending on the number of available Slots."
        },
        {
          "id": "D",
          "text": "The Cache and the Slot",
          "is_correct": false,
          "explanation": "The Cache is used for storing data in memory to improve performance, not for parallel task execution. Parallelization does not occur between the Cache and the Slot."
        }
      ],
      "is_from_sdk_v2": true,
      "feedback": "In Apache Spark, parallelization occurs across two levels: Executors and Slots. Executors are distributed worker processes that run on cluster nodes and execute the actual tasks. Within each Executor, resources are divided into Slots, and each Slot can run a task concurrently. This architecture enables efficient distribution and parallel processing of data across the Spark cluster. The Driver coordinates this process but is not directly involved in parallel execution. Understanding this is essential when optimizing workloads in distributed environments, such as those used in Azure Synapse or Azure Databricks clusters integrated with Azure Machine Learning.",
      "correct_option": "C"
    },
    {
      "question_id": "32",
      "type": "multiple_choice_single_answer",
      "question": "At the heart of deep learnings success is a kind of model called a convolutional neural network, or CNN.\n\nA CNN typically works by extracting features from images, and then feeding those features into a fully connected neural network to generate a prediction.\n\nThe feature extraction layers in the network have the effect of reducing the number of features from the potentially huge array of individual pixel values to a smaller feature set that supports label prediction.\n\nCNNs consist of multiple layers, each performing a specific task in extracting features or predicting labels.\n\nWhich layer is described as:\n“One of the most difficult challenges in a CNN is the avoidance of overfitting, where the resulting model performs well with the training data but doesn‘t generalize well to new data on which it wasn‘t trained.\nOne technique you can use to mitigate overfitting is to include layers in which the training process modifies feature maps.\nThis may seem counterintuitive, but it’s an effective way to ensure that the model doesn’t learn to be over-dependent on the training images.”",
      "options": [
        {
          "id": "A",
          "text": "Flattening layers",
          "is_correct": false,
          "explanation": "Flattening layers transform multidimensional tensors into flat vectors before they are passed to dense layers. They do not help prevent overfitting."
        },
        {
          "id": "B",
          "text": "Convolution layers",
          "is_correct": false,
          "explanation": "Convolution layers extract spatial features from input data. Although crucial in CNNs, they are not used primarily to prevent overfitting."
        },
        {
          "id": "C",
          "text": "Fully connected layers",
          "is_correct": false,
          "explanation": "Fully connected layers connect all neurons in one layer to every neuron in the next layer and often lead to overfitting if not regulated."
        },
        {
          "id": "D",
          "text": "Dropping layers",
          "is_correct": true,
          "explanation": "Dropping layers (Dropout layers) randomly deactivate a subset of neurons during training. This prevents the model from becoming overly dependent on specific neurons, improving generalization and reducing overfitting."
        },
        {
          "id": "E",
          "text": "Pooling layers",
          "is_correct": false,
          "explanation": "Pooling layers reduce the spatial size of the representation but do not modify feature maps to prevent overfitting."
        },
        {
          "id": "F",
          "text": "Normalizing layers",
          "is_correct": false,
          "explanation": "Normalizing layers standardize inputs, which helps with training stability, but they are not designed specifically to mitigate overfitting."
        }
      ],
      "feedback": "The correct answer is 'Dropping layers', referring to Dropout layers. These are specifically designed to reduce overfitting in convolutional neural networks (CNNs). During training, dropout randomly deactivates a fraction of the neurons in a layer, which prevents the network from relying too heavily on any single feature. This promotes redundancy and forces the network to learn more robust patterns that generalize better to new, unseen data. This strategy is especially effective in CNNs, where the model might otherwise memorize training data patterns. Dropout is considered a regularization technique and is commonly used in combination with convolutional and fully connected layers to improve model generalization."
    },
    {
        "question_id": "34",
        "type": "multiple_choice_single_answer",
        "question": "You use Azure Machine Learning SDK v2 to define a training job.\n\nYou have already registered a tabular dataset in your workspace with the name 'training_data'.\n\nYou want to reference this dataset in your training job and make it available inside the script as an input named 'training_data'.\n\nWhich code should you use to define the input correctly?",
        "options": [
          {
            "id": "A",
            "text": "inputs={'training_data': Dataset.get_by_name(ws, 'training_data')}",
            "is_correct": false,
            "explanation": "This syntax is used in SDK v1 and is not compatible with SDK v2. SDK v2 uses the Input class for dataset binding."
          },
          {
            "id": "B",
            "text": "inputs={'training_data': Input(type='uri_folder', path='azureml:training_data:1')}",
            "is_correct": true,
            "explanation": "This is the correct syntax for referencing a registered dataset in SDK v2 using the Input class. It binds the dataset as a named input available inside the script."
          },
          {
            "id": "C",
            "text": "training_data = ml_client.datasets.get('training_data')",
            "is_correct": false,
            "explanation": "This is not a valid method in SDK v2. The `ml_client.datasets` interface does not exist in this form."
          },
          {
            "id": "D",
            "text": "inputs=[Dataset.File.from_path('training_data')]",
            "is_correct": false,
            "explanation": "This method resembles v1-style syntax and lacks the proper binding and structure for SDK v2 training jobs."
          }
        ],
        "feedback": "In SDK v2, the correct way to bind a registered dataset to a training job is by using the `Input` class from `azure.ai.ml`. You must specify the input type (e.g., 'uri_file' or 'uri_folder') and use the path to the registered dataset, such as `azureml:training_data:1`. This ensures the dataset is properly mounted or downloaded and accessible inside your training script."
      },
      
      {
        "question_id": "35",
        "type": "true_false",
        "question": "You have been tasked with constructing a machine learning model that translates language text into a different language text.\n\nThe machine learning model must be constructed and trained to learn the sequence of the input.\n\nSolution: You make use of Convolutional Neural Networks (CNNs).\n\nWill the requirements be satisfied?",
        "options": [
          {
            "id": "A",
            "text": "Yes",
            "is_correct": false,
            "explanation": "CNNs are powerful for image data and have been used in NLP tasks for feature extraction, but they are not well-suited for sequence modeling tasks such as language translation. Tasks that require understanding of sequential dependencies, like language translation, are better handled by architectures such as RNNs, LSTMs, or Transformers."
          },
          {
            "id": "B",
            "text": "No",
            "is_correct": true,
            "explanation": "CNNs are not designed to model long-range dependencies in sequences, which are critical for tasks like language translation. Recurrent models or Transformers are more appropriate because they capture the order and context of words in a sequence."
          }
        ],
        "feedback": "The correct answer is 'No'. While CNNs can be adapted for certain NLP tasks, they do not inherently model sequential dependencies over long ranges, which is essential for accurate language translation. Sequence-to-sequence models with attention mechanisms, like the Transformer architecture, are state-of-the-art for translation problems. Azure ML supports training such models using PyTorch or TensorFlow through custom jobs with MLflow tracking for reproducibility."
      },

    {
    "question_id": "36",
    "type": "multiple_choice_single_answer",
    "question": "In the context of Azure Machine Learning and distributed deep learning with Horovod, what is the primary purpose of the following callback in the training script?\n\n`BroadcastGlobalVariablesCallback(0)`",
    "options": [
        {
        "id": "A",
        "text": "To average metrics among workers at the end of every epoch.",
        "is_correct": false,
        "explanation": "Averaging metrics is typically handled by Horovod’s `hvd.allreduce()` function, not by the broadcast callback."
        },
        {
        "id": "B",
        "text": "Broadcast final variable states from rank 0 to all other processes.",
        "is_correct": false,
        "explanation": "While this is related, the focus of the callback is on consistent initialization, not on broadcasting final states."
        },
        {
        "id": "C",
        "text": "Reduce the learning rate if training plateaus.",
        "is_correct": false,
        "explanation": "This functionality is handled by learning rate schedulers like `ReduceLROnPlateau`, not by the Horovod broadcast callback."
        },
        {
        "id": "D",
        "text": "To ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.",
        "is_correct": true,
        "explanation": "This is the main purpose of `BroadcastGlobalVariablesCallback(0)`: to synchronize the initial model weights across all workers to ensure they start training identically."
        }
    ],
    "feedback": "In distributed training using Horovod and Azure Machine Learning SDK v2, each worker runs independently on its own process or GPU. If model initialization is random or restored from a checkpoint, it's critical that all workers begin with the same model weights.\n\nThe callback `BroadcastGlobalVariablesCallback(0)` ensures synchronization by broadcasting the model variables (weights) from the root worker (rank 0) to all other workers at the start of training. This avoids divergence in model updates caused by inconsistent starting points.\n\nThis callback is widely used in distributed training scripts configured via `command()` jobs or custom components with `distribution=HorovodConfiguration()` in SDK v2.\n\n```python\nfrom horovod.tensorflow.keras import BroadcastGlobalVariablesCallback\nmodel.fit(..., callbacks=[BroadcastGlobalVariablesCallback(0)])\n```\n\nThis technique maintains training integrity and is essential when performing large-scale distributed training jobs with Azure ML and Horovod."
    },

    {
        "question_id": "37",
        "type": "ordering",
        "question": "You are building a monthly retraining pipeline for a multi-class image classification model using the PyTorch framework in Azure Machine Learning. The pipeline should minimize cost and training time by selecting the appropriate compute targets. The experiment runs on a GPU-enabled compute cluster.\n\nUsing the Azure ML Python SDK v2, define a pipeline with three command components that:\n1. Ingest new image data from a public web portal\n2. Resize the images\n3. Train the model using the PyTorch script on a GPU cluster\n\nIn what order should you run these components?",
        "options": [
          {
            "id": "A",
            "text": "Run a data ingestion component on the cpu-cluster to fetch image data from a public web source."
          },
          {
            "id": "B",
            "text": "Run an image preprocessing component (image_resize.py) on the cpu-cluster."
          },
          {
            "id": "C",
            "text": "Run the PyTorch training component (bird_classifier_train.py) on the gpu-cluster."
          }
        ],
        "correct_order": ["A", "B", "C"],
        "feedback": "In Azure ML SDK v2, the best practice is to define each stage of the pipeline as a `command` component. First, the ingestion component (A) runs on a CPU cluster to pull and register image data. Second, the image resizing script (B) executes as a lightweight CPU-bound transformation. Finally, the model training script (C) uses GPU resources to efficiently retrain the PyTorch model. Each component uses `Input(type=uri_folder)` and `Output(type=uri_folder)` to pass data efficiently between stages in the pipeline. This modular and scalable design aligns with cost optimization and modern ML engineering practices in SDK v2."
      },
      {
        "question_id": "38",
        "type": "multiple_choice_single_answer",
        "question": "You want to connect to your Azure Machine Learning workspace from your local development environment using the Azure ML Python SDK v2. You have already installed the azure-ai-ml and azure-identity packages. What else do you need to do before you can start interacting with your workspace?",
        "options": [
          {
            "id": "A",
            "text": "Use the Azure CLI to log in with `az login` so DefaultAzureCredential can authenticate.",
            "is_correct": true,
            "explanation": "DefaultAzureCredential uses Azure CLI credentials if available. Running `az login` ensures your local environment is authenticated and able to connect to Azure ML resources using the SDK v2."
          },
          {
            "id": "B",
            "text": "Create a Compute Instance in Azure ML Studio.",
            "is_correct": false,
            "explanation": "A compute instance is useful for running code in the cloud, but it is not required to connect from your local machine using SDK v2."
          },
          {
            "id": "C",
            "text": "Install the azureml-sdk['notebooks'] package.",
            "is_correct": false,
            "explanation": "This package belongs to the deprecated SDK v1 and is not used in SDK v2."
          },
          {
            "id": "D",
            "text": "Download a config.json file and use Workspace.from_config().",
            "is_correct": false,
            "explanation": "This approach is specific to SDK v1. In SDK v2, you connect using MLClient and Azure credentials without the need for a config.json file."
          }
        ],
        "feedback": "The correct answer is A. Azure ML SDK v2 uses `DefaultAzureCredential` from the `azure-identity` package, which integrates with the Azure CLI. This allows secure access to your workspace without needing to store credentials or config files. Ensure you run `az login` before using the SDK so that the credential chain can authenticate successfully."
      },
      {
        "question_id": "38A",
        "type": "multiple_choice_single_answer",
        "question": "You can think of the steps to train and evaluate a clustering machine learning model.\n\nWhich of the following is an invalid step within the context of training and evaluating a clustering machine learning model?",
        "options": [
          {
            "id": "A",
            "text": "Train model",
            "is_correct": false,
            "explanation": "Training a clustering model is a valid and essential step in unsupervised learning to group data based on similarities."
          },
          {
            "id": "B",
            "text": "Evaluate performance",
            "is_correct": false,
            "explanation": "Clustering models can be evaluated using metrics like silhouette score, inertia, or Davies–Bouldin index."
          },
          {
            "id": "C",
            "text": "Deploy a predictive service",
            "is_correct": false,
            "explanation": "While clustering is unsupervised, it can still be used to deploy a service that assigns cluster labels to new data, such as for segmentation or anomaly detection."
          },
          {
            "id": "D",
            "text": "All options are valid steps, therefore none of the listed steps are invalid.",
            "is_correct": true,
            "explanation": "All the options mentioned—preparing data, training, evaluating, and deploying—are valid and commonly used steps when working with clustering models, especially in practical Azure ML workflows."
          },
          {
            "id": "E",
            "text": "Prepare data",
            "is_correct": false,
            "explanation": "Preparing data is a crucial step for clustering, ensuring normalization, scaling, and missing value handling before fitting the model."
          }
        ],
        "feedback": "All steps listed are valid when working with clustering models in Azure Machine Learning. While clustering is unsupervised and doesn’t predict labels, it can still be deployed to assign cluster IDs or for downstream tasks. Azure ML SDK v2 supports unsupervised training, evaluation with clustering metrics, and deployment using real-time endpoints."
      },
      {
        "question_id": "39",
        "type": "multiple_choice_single_answer",
        "question": "Machine learning models are as strong as the data they are trained on. Often it is important to derive features from existing raw data that better represent the nature of the data and thus help improve the predictive power of the machine learning algorithms.\n\nThis process of generating new predictive features from existing raw data is commonly referred to as:",
        "options": [
          {
            "id": "A",
            "text": "Factor engineering",
            "is_correct": false,
            "explanation": "Factor engineering is not a standard term in machine learning. It may be confused with factor analysis, but it does not refer to the process of creating new features from raw data."
          },
          {
            "id": "B",
            "text": "Feature engineering",
            "is_correct": true,
            "explanation": "Feature engineering is the process of creating new features or variables from existing raw data to improve the performance of machine learning models. By deriving new features that better represent the underlying patterns in the data, the predictive power of the algorithms can be enhanced."
          },
          {
            "id": "C",
            "text": "Feature mining",
            "is_correct": false,
            "explanation": "Feature mining is not a widely accepted term in the context of machine learning. The standard process for transforming raw data into informative variables is called feature engineering."
          },
          {
            "id": "D",
            "text": "Factor mining",
            "is_correct": false,
            "explanation": "Factor mining is not a recognized concept in the field of machine learning and does not refer to the generation of features from raw data."
          },
          {
            "id": "E",
            "text": "Element utilization",
            "is_correct": false,
            "explanation": "Element utilization is not a concept used in machine learning for deriving new predictive variables from raw data."
          }
        ],
        "feedback": "The correct answer is 'Feature engineering'. In the context of Azure Machine Learning using Python SDK v2, this step is essential in the data preparation phase. Well-engineered features directly influence model quality. Azure ML allows you to encapsulate feature engineering scripts into pipeline components (using command components or @command decorators), which can be reused and registered as part of your MLOps strategy. For instance, you may define a YAML or Python-based command component that performs missing value imputation, encoding, binning, or time-based feature extraction. These components are then used inside a pipeline job to ensure consistent data preprocessing before model training."
      },
      {
        "question_id": "40",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure Machine Learning Python SDK v2 to run experiments. You need to create an environment from a Conda configuration (.yml) file.\nWhich method should you use to define the environment?",
        "options": [
          {
            "id": "A",
            "text": "Use the Environment class and pass the 'conda_file' argument during instantiation",
            "is_correct": true,
            "explanation": "In Azure ML SDK v2, environments are created using the Environment class from azure.ai.ml.entities. To use a Conda file, you pass it to the 'conda_file' argument when initializing the Environment object."
          },
          {
            "id": "B",
            "text": "Call Environment.create_from_conda_specification()",
            "is_correct": false,
            "explanation": "This method is used in SDK v1, not in the v2 version of the Azure ML SDK."
          },
          {
            "id": "C",
            "text": "Use the azureml.core.Environment constructor and pass the conda YAML as string",
            "is_correct": false,
            "explanation": "This approach corresponds to SDK v1 and is no longer used in the SDK v2 structure."
          }
        ],
        "feedback": "In Azure Machine Learning SDK v2, environments are created using the Environment class from azure.ai.ml.entities. You can specify the Docker image and Conda environment at instantiation. For example, if you want to define a custom Python environment for training an image classifier, you can do so by referencing a conda.yml file like this:\n\n```python\nfrom azure.ai.ml.entities import Environment\n\nenv = Environment(\n    name=\"img-clf-env\",\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n    conda_file=\"./envs/train.yml\"\n)\n```\n\nThis environment can then be reused and versioned across multiple training pipelines or jobs."
      },
      {
        "question_id": "41",
        "type": "multiple_choice_single_answer",
        "question": "Smartoire has requested the team to construct Custom Roles.\n\nWhich is the best answer regarding what creating Custom Roles does?",
        "options": [
          {
            "id": "A",
            "text": "Custom roles allow you to customize what users can and cannot access in a workspace",
            "is_correct": true,
            "explanation": "This is the correct choice. Custom roles enable you to tailor the access permissions for users within a workspace, specifying what they can and cannot access based on their role."
          },
          {
            "id": "B",
            "text": "Custom roles allow you to customize what users can access in a workspace",
            "is_correct": false,
            "explanation": "While custom roles do allow you to customize user access within a workspace, they also define restrictions on what users can do, not just what they can access."
          },
          {
            "id": "C",
            "text": "Custom roles allow you to only perform read-only actions in a workspace",
            "is_correct": false,
            "explanation": "This is not correct. Custom roles are flexible and not limited to read-only actions; they allow defining a wide range of permissions."
          },
          {
            "id": "D",
            "text": "Custom roles allow you to customize the speaking voice used",
            "is_correct": false,
            "explanation": "This is irrelevant in the context of Azure Machine Learning. Custom roles are for access and permission management, not voice customization."
          }
        ],
        "feedback": "Custom roles in Azure allow administrators to define very granular access control by specifying actions that users can and cannot perform within a workspace. This supports the principle of least privilege, improves security posture, and aligns access with job responsibilities. For example, a user responsible only for monitoring experiments can be restricted from registering models or deploying endpoints. More info: https://learn.microsoft.com/en-us/azure/role-based-access-control/custom-roles"
      },
      {
        "question_id": "42",
        "type": "multiple_choice_single_answer",
        "question": "You have trained a model, and you want to quantify the influence of each feature on a specific individual prediction.\n\nWhat kind of feature importance should you examine?",
        "options": [
          {
            "id": "A",
            "text": "Local feature importance",
            "is_correct": true,
            "explanation": "Local feature importance focuses on quantifying the influence of each feature on a specific individual prediction. It provides insights into how each feature contributes to the prediction for a particular data point, allowing for a more granular understanding of the model's decision-making process."
          },
          {
            "id": "B",
            "text": "Global feature importance",
            "is_correct": false,
            "explanation": "Global feature importance, on the other hand, looks at the overall impact of each feature across the entire dataset. It provides a broader perspective on the importance of features in general, rather than on a specific prediction."
          }
        ],
        "feedback": "To understand a model's prediction for a specific individual data point (e.g., why this customer churned), you need **local feature importance**. Azure Machine Learning SDK v2 provides capabilities for Responsible AI, including interpretability tools like SHAP, which help visualize local feature contributions. In contrast, global importance tells you which features are important in general, not per prediction. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability"
      },

      {
        "question_id": "43",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is looking into training a classification model and she wants to quantify the influence of each feature on a specific individual prediction.\n\nWhich of the following should she examine?",
        "options": [
          {
            "id": "A",
            "text": "Local feature importance",
            "is_correct": true,
            "explanation": "Local feature importance allows Melinda to quantify the influence of each feature on a specific individual prediction. By examining local feature importance, she can understand which features are contributing the most to a particular prediction, helping her gain insights into the model's decision-making process."
          },
          {
            "id": "B",
            "text": "Global feature importance",
            "is_correct": false,
            "explanation": "Global feature importance provides an overview of the importance of each feature in the model across all predictions. While this information is valuable for understanding the overall behavior of the model, it does not specifically quantify the influence of each feature on a specific individual prediction, which is Melinda's goal in this scenario."
          }
        ],
        "feedback": "This scenario focuses on local interpretability. In Azure Machine Learning SDK v2, you can use the `azureml.interpret` package or integrate with `SHAP` to obtain **local feature importance**, which explains why a model made a particular prediction for an individual instance. This is critical in Responsible AI practices for debugging and trust building. Docs: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability"
      },
      {
        "question_id": "44",
        "type": "multiple_choice_single_answer",
        "question": "You are analyzing a numerical dataset which contains missing values in several columns.\nYou must clean the missing values using an appropriate operation without affecting the dimensionality of the feature set.\nYou need to analyze a full dataset to include all values.\n\nSolution: Remove the entire column that contains the missing data point.\n\nDoes the solution meet the goal?",
        "options": [
          {
            "id": "A",
            "text": "No",
            "is_correct": true,
            "explanation": "Removing entire columns that contain missing values will affect the dimensionality of the feature set. Since the requirement is to keep the original set of features intact while handling missing values, this approach does not meet the goal."
          },
          {
            "id": "B",
            "text": "Yes",
            "is_correct": false,
            "explanation": "While this may eliminate missing values, it contradicts the goal of preserving all feature columns in the dataset."
          }
        ],
        "feedback": "The correct approach should preserve the structure of the dataset, including all columns. In Azure Machine Learning SDK v2, missing values can be handled using imputation techniques (e.g., mean, median, mode) through `sklearn.impute.SimpleImputer` or integrated directly into preprocessing pipelines. Dropping entire columns is not recommended unless the column is irrelevant or contains excessive missing values."
      },
      {
        "question_id": "44A",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is looking into training a classification model and she wants to quantify the influence of each feature on a specific individual prediction.\n\nWhich of the following should she examine?",
        "options": [
          {
            "id": "A",
            "text": "Local feature importance",
            "is_correct": true,
            "explanation": "Local feature importance allows Melinda to quantify the influence of each feature on a specific individual prediction. By examining local feature importance, she can understand which features are contributing the most to a particular prediction, helping her gain insights into the model's decision-making process."
          },
          {
            "id": "B",
            "text": "Global feature importance",
            "is_correct": false,
            "explanation": "Global feature importance provides an overview of the importance of each feature in the model across all predictions. While this information is valuable for understanding the overall behavior of the model, it does not specifically quantify the influence of each feature on a specific individual prediction, which is Melinda's goal in this scenario."
          }
        ],
        "feedback": "This scenario focuses on local interpretability. In Azure Machine Learning SDK v2, you can use the `azureml.interpret` package or integrate with `SHAP` to obtain **local feature importance**, which explains why a model made a particular prediction for an individual instance. This is critical in Responsible AI practices for debugging and trust building. Docs: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability"
      },
      {
        "question_id": "45",
        "type": "multiple_choice_single_answer",
        "question": "You are a senior data scientist in your company. You are evaluating a completed binary classification machine learning model. You need to use precision as the evaluation metric. Which visualization should you use?",
        "options": [
          {
            "id": "A",
            "text": "Receiver Operating Characteristic (ROC) curve",
            "is_correct": false,
            "explanation": "ROC curves are useful for evaluating TPR vs. FPR but are not ideal when focusing specifically on precision."
          },
          {
            "id": "B",
            "text": "Violin plot",
            "is_correct": false,
            "explanation": "A violin plot is used to visualize the distribution of data and is unrelated to precision evaluation."
          },
          {
            "id": "C",
            "text": "Gradient descent",
            "is_correct": false,
            "explanation": "Gradient descent is an optimization technique, not a visualization method."
          },
          {
            "id": "D",
            "text": "Scatter plot",
            "is_correct": false,
            "explanation": "Scatter plots are useful for analyzing relationships between numerical variables, not for classification evaluation metrics like precision."
          },
          {
            "id": "E",
            "text": "Precision-Recall curve",
            "is_correct": true,
            "explanation": "Precision-Recall (PR) curves are ideal when the focus is on the precision metric, especially for imbalanced binary classification problems."
          }
        ],
        "feedback": "The correct visualization for evaluating precision in binary classification is the Precision-Recall curve. This curve is especially useful when working with imbalanced datasets and when precision (the proportion of true positives among predicted positives) is the primary focus. Learn more at: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automl-output#precision-recall-curve"
      },    
      {
        "question_id": "46",
        "type": "multiple_choice_single_answer",
        "question": "You have a model with a large difference between the training and validation error values. You must create a new model and perform cross-validation. You need to identify a parameter set for the new model using Azure Machine Learning Studio. Which module should you use for the step: Train, evaluate, and compare?",
        "options": [
          {
            "id": "A",
            "text": "Split Data",
            "is_correct": false,
            "explanation": "This module is used to divide a dataset into training and testing sets but does not handle hyperparameter tuning or model comparison."
          },
          {
            "id": "B",
            "text": "Partition and Sample",
            "is_correct": false,
            "explanation": "This module is typically used for data sampling and partitioning, not for model training or hyperparameter tuning."
          },
          {
            "id": "C",
            "text": "Two-Class Boosted Decision Tree",
            "is_correct": false,
            "explanation": "This module builds a specific type of model but is not designed to perform cross-validation or tune hyperparameters for improving model generalization."
          },
          {
            "id": "D",
            "text": "Tune Model Hyperparameters",
            "is_correct": true,
            "explanation": "This module performs cross-validation to find the best parameter set and improve model generalization performance."
          }
        ],
        "feedback": "The 'Tune Model Hyperparameters' module in Azure Machine Learning Studio is specifically designed to help optimize a model by evaluating multiple parameter configurations using cross-validation. This is particularly helpful when your model shows signs of overfitting — such as a large discrepancy between training and validation errors. By using this module, Azure ML Studio automates the process of finding the most appropriate parameters that reduce generalization error and improve predictive performance.\n\n**Referencia oficial:** https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/tune-model-hyperparameters"
      },
      {
        "question_id": "47",
        "type": "multiple_choice_single_answer",
        "question": "You are using Fairlearn with Azure Machine Learning to ensure fairness in your classification model. You want to use a constraint that minimizes the disparity in both true positive and false positive rates across different sensitive feature groups. Which constraint should you use?",
        "options": [
          {
            "id": "A",
            "text": "False-positive rate parity",
            "is_correct": false,
            "explanation": "This constraint only considers parity in false-positive rates and ignores the true-positive component required in this case."
          },
          {
            "id": "B",
            "text": "Demographic parity",
            "is_correct": false,
            "explanation": "Demographic parity focuses on equalizing predictions across groups, regardless of true/false positives."
          },
          {
            "id": "C",
            "text": "Bounded group loss",
            "is_correct": false,
            "explanation": "Bounded group loss minimizes loss disparity but does not ensure parity in both TPR and FPR."
          },
          {
            "id": "D",
            "text": "Equalized odds",
            "is_correct": true,
            "explanation": "Equalized odds ensures that each sensitive group has similar true positive and false positive rates, which meets the requirements described."
          },
          {
            "id": "E",
            "text": "True positive rate parity",
            "is_correct": false,
            "explanation": "True positive rate parity ignores the false positive rate, which is essential for equalized odds."
          },
          {
            "id": "F",
            "text": "Error rate parity",
            "is_correct": false,
            "explanation": "This is not a standard constraint in Fairlearn and does not match the definition provided."
          }
        ],
        "answer": "D",
        "explanation": "The Equalized Odds constraint in Fairlearn minimizes disparities in both true positive and false positive rates across sensitive feature groups. This is particularly useful in binary classification tasks where you want fair performance across different subgroups of the population.",
        "references": [
          "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-fairness-ml",
          "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html#fairness-in-machine-learning"
        ]
      },
      {
        "question_id": "48",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following is a form of machine learning in which the goal is to create a model that can predict a numeric, quantifiable value; such as a price, amount, size, or other scalar numbers?",
        "options": [
          {
            "id": "A",
            "text": "Probability",
            "is_correct": false,
            "explanation": "Probability is a measure of the likelihood of an event occurring and is not directly related to predicting numeric values in machine learning."
          },
          {
            "id": "B",
            "text": "Classification",
            "is_correct": false,
            "explanation": "Classification deals with predicting categories or classes, not continuous numeric values like prices or amounts."
          },
          {
            "id": "C",
            "text": "Regression",
            "is_correct": true,
            "explanation": "Regression focuses on predicting continuous numeric values. It is ideal for modeling relationships between input variables and numeric output variables like prices, amounts, or sizes."
          },
          {
            "id": "D",
            "text": "Statistics",
            "is_correct": false,
            "explanation": "Statistics is a foundational discipline that supports machine learning but is not a type of learning model in itself."
          }
        ],
        "feedback": "Regression is a supervised learning approach used to predict a continuous outcome variable. In the context of Azure Machine Learning and DP-100, understanding the difference between regression and classification is essential to selecting appropriate models for tasks such as price prediction, risk estimation, or demand forecasting."
      },
      {
        "question_id": "49",
        "type": "multiple_choice_single_answer",
        "question": "You are analyzing a numerical dataset that contains missing values in several columns. The dataset must be cleaned using an appropriate operation without affecting the dimensionality of the feature set. The requirement is to analyze a full dataset that includes all values. The developer chose to replace each missing value using the Multiple Imputation by Chained Equations (MICE) method. Does the solution meet the requirement?",
        "options": [
          {
            "id": "A",
            "text": "No",
            "is_correct": false,
            "explanation": "The 'No' option would only be correct if the method dropped columns or failed to impute all missing values, which is not the case with MICE."
          },
          {
            "id": "B",
            "text": "Yes",
            "is_correct": true,
            "explanation": "MICE (Multiple Imputation by Chained Equations) is a valid method for imputing missing values without reducing the dimensionality of the dataset, thus preserving all features and enabling full dataset analysis."
          }
        ],
        "feedback": "The requirement was to clean missing values while keeping all original features (no loss of dimensionality). The MICE method is suitable because it imputes missing values by modeling each feature with missing values as a function of other features. This makes it a robust approach in scenarios where maintaining the dataset structure is crucial for downstream analysis and modeling."
      },
      {
        "question_id": "50",
        "type": "multiple_choice_single_answer",
        "question": "Logan is one of the developers on an IT migration project and has trained a model using Azure Databricks. The next step is to serve it for real-time scoring. What should be the next action?",
        "options": [
          {
            "id": "A",
            "text": "Register the model.",
            "is_correct": true,
            "explanation": "Registering the model is the required step before deploying it for inference. Registration makes the model available in the Azure ML workspace for deployment to endpoints."
          },
          {
            "id": "B",
            "text": "Transition the model to the archived stage.",
            "is_correct": false,
            "explanation": "Archiving a model is used when the model is no longer in use or is being deprecated, not when preparing it for deployment."
          },
          {
            "id": "C",
            "text": "Version the model.",
            "is_correct": false,
            "explanation": "Versioning helps track model iterations but does not, by itself, make the model available for real-time scoring."
          }
        ],
        "feedback": "In Azure ML, once a model is trained (whether using Azure ML, Databricks, or another environment), it must be **registered** in the workspace before it can be deployed to a managed endpoint or for real-time scoring. Registering ensures the model is available with a name and version and can be tracked, deployed, or updated using Azure Machine Learning services. For example, using `ml_client.models.create_or_update(...)` in SDK v2 registers the model artifact with its metadata."
      },

      {
        "question_id": "51",
        "type": "multiple_choice_single_answer",
        "question": "You are working with a time series dataset in Azure Machine Learning using the Python SDK v2. You need to split the dataset into training and testing subsets while preserving the temporal order. Which of the following approaches should you use?",
        "options": [
          {
            "id": "A",
            "text": "Use sklearn.model_selection.TimeSeriesSplit to generate training and testing splits.",
            "is_correct": true,
            "explanation": "TimeSeriesSplit from scikit-learn is designed specifically for time series data and preserves temporal ordering when generating training and test splits, making it ideal for time-dependent data in Azure ML pipelines."
          },
          {
            "id": "B",
            "text": "Use random.shuffle on the dataset before splitting.",
            "is_correct": false,
            "explanation": "Shuffling the dataset destroys the temporal structure, which is critical in time series analysis."
          },
          {
            "id": "C",
            "text": "Use train_test_split with shuffle=True.",
            "is_correct": false,
            "explanation": "This also randomizes the order of samples, which is not suitable for time series data."
          },
          {
            "id": "D",
            "text": "Use pandas.DataFrame.sample() to split the dataset.",
            "is_correct": false,
            "explanation": "Sampling randomly is inappropriate for time series because it does not preserve order and may lead to data leakage."
          }
        ],
        "explanation": "When working with time series datasets in Azure ML SDK v2, preserving the order of data is essential to avoid data leakage. The best practice is to use `TimeSeriesSplit` from scikit-learn, which generates splits that respect the chronological order of observations, ensuring proper model evaluation in a time-dependent context."
      },
      {
        "question_id": "52",
        "type": "multiple_choice_single_answer",
        "question": "You have been contracted by Wayne Enterprises to help integrate Azure Machine Learning into their inference services. The IT team wants to log custom telemetry metrics for the model's behavior and analyze them using Application Insights. What should the team include in the entry script to capture custom metrics appropriately using Azure Machine Learning SDK v2?",
        "options": [
          {
            "id": "A",
            "text": "Use a print statement to write the metrics in the STDOUT log.",
            "is_correct": false,
            "explanation": "Print statements are not tracked automatically by Azure ML or Application Insights for telemetry purposes."
          },
          {
            "id": "B",
            "text": "Use mlflow.log_metric() to log the custom metrics.",
            "is_correct": true,
            "explanation": "mlflow.log_metric() is the correct method to log scalar values like custom metrics during an Azure ML run using SDK v2. These logs can be collected and visualized in Application Insights."
          },
          {
            "id": "C",
            "text": "Use the Run.log method to log the custom metrics.",
            "is_correct": false,
            "explanation": "Run.log belongs to SDK v1 and is no longer used with SDK v2 or MLflow-based runs."
          },
          {
            "id": "D",
            "text": "Save the custom metrics in the ./outputs folder.",
            "is_correct": false,
            "explanation": "Saving metrics as files doesn't automatically register them with Azure ML telemetry systems like Application Insights."
          }
        ],
        "answer": "B",
        "explanation": "In the Azure Machine Learning SDK v2, logging metrics such as accuracy, latency, or any custom telemetry during an inference or training run is done using MLflow. The `mlflow.log_metric()` function records scalar values which are automatically captured by Azure ML and can be monitored through Application Insights.",
        "sdk_version": "v2",
        "feedback": "This question reinforces the correct method for capturing custom metrics using the SDK v2, where MLflow is the default logging interface. You can use `mlflow.log_metric(name, value)` inside the entry script or scoring script to persist and visualize metrics in Azure ML Studio. See Microsoft documentation for [tracking metrics](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models#monitor-your-experiment-runs)."
      },
      {
        "question_id": "52A",
        "type": "multiple_choice_single_answer",
        "question": "You create a multi-class image classification deep learning model that uses a set of labeled bird photographs collected by experts. All photographs are stored in an Azure blob container. You need to access the bird photograph files from Azure ML service workspace for training, minimizing data movement. What should you do?",
        "options": [
          {
            "id": "A",
            "text": "Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.",
            "is_correct": false,
            "explanation": "TabularDataset is intended for structured/tabular data. Since the data consists of images, it is not appropriate."
          },
          {
            "id": "B",
            "text": "Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.",
            "is_correct": false,
            "explanation": "Copying the images introduces unnecessary data movement, which the question states must be minimized."
          },
          {
            "id": "C",
            "text": "Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.",
            "is_correct": true,
            "explanation": "Registering the existing Azure Blob container as a datastore allows the workspace to access the files directly without duplication or unnecessary transfer of data."
          },
          {
            "id": "D",
            "text": "Create an Azure Data Lake store and move the bird photographs to the store.",
            "is_correct": false,
            "explanation": "Creating a new storage and moving data contradicts the requirement of minimizing data movement."
          },
          {
            "id": "E",
            "text": "Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.",
            "is_correct": false,
            "explanation": "Azure Cosmos DB is not a suitable or efficient storage for large-scale image data like photographs."
          }
        ],
        "feedback": "To minimize data movement in Azure Machine Learning, the best practice is to register the existing Azure Blob Storage as a datastore. This allows scripts and training components to access the JPG images directly from the blob container without copying or duplicating the data. This is especially important when working with large-scale datasets such as image files in deep learning scenarios."
      },
      {
        "question_id": "53",
        "type": "multiple_choice_single_answer",
        "question": "At the heart of deep learning’s success is a kind of model called a convolutional neural network, or CNN. A CNN typically works by extracting features from images, and then feeds those features into a fully connected neural network to generate a prediction.\n\nThe feature extraction layers in the network have the effect of reducing the number of features from the potentially huge array of individual pixel values to a smaller feature set that supports label prediction.\n\nCNNs consist of multiple layers, each performing a specific task in extracting features or predicting labels.\n\nWhich layer is described as:\n“After extracting feature values from images, these layers are used to reduce the number of feature values while retaining the key differentiating features that have been extracted.”",
        "options": [
          {
            "id": "A",
            "text": "Normalizing layers",
            "is_correct": false,
            "explanation": "Normalizing layers are primarily used to scale input values, typically to improve training stability. They do not reduce the number of features."
          },
          {
            "id": "B",
            "text": "Fully connected layers",
            "is_correct": false,
            "explanation": "Fully connected layers perform the final prediction task by combining all features, but do not reduce the number of feature values."
          },
          {
            "id": "C",
            "text": "Dropping layers",
            "is_correct": false,
            "explanation": "Dropping layers are not a standard concept in CNNs and do not perform feature reduction."
          },
          {
            "id": "D",
            "text": "Pooling layers",
            "is_correct": true,
            "explanation": "Pooling layers reduce the spatial dimensions of the feature maps, retaining important features while decreasing the number of parameters and computations."
          },
          {
            "id": "E",
            "text": "Convolution layers",
            "is_correct": false,
            "explanation": "Convolution layers extract features from the input data but do not reduce their number. Pooling is responsible for that."
          },
          {
            "id": "F",
            "text": "Flattening layers",
            "is_correct": false,
            "explanation": "Flattening layers prepare data for fully connected layers by transforming multi-dimensional data into 1D, but do not reduce features."
          }
        ],
        "correct_option": "D",
        "feedback": "The correct answer is Pooling layers. Pooling is used after feature extraction to reduce the spatial dimensions of the feature maps. This operation retains the most important features while reducing the computational burden, making it a critical step in CNNs. Unlike convolutional layers that focus on extracting features, pooling layers specifically help with dimensionality reduction, which is essential for downstream tasks like classification."
      },
      {
        "question_id": "54",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist and you use Azure Machine Learning Studio for your experiments. You are creating a new experiment in Azure Machine Learning Studio. You have a small dataset that has missing values in many columns. The data does not require the application of predictors for each column. You plan to use the Clean Missing Data. You need to select a data cleaning method. Which method should you use?",
        "options": [
          {
            "id": "A",
            "text": "Normalization",
            "is_correct": false,
            "explanation": "Normalization scales numerical features to a standard range, but it is not a method for handling missing values."
          },
          {
            "id": "B",
            "text": "Synthetic Minority Oversampling Technique (SMOTE)",
            "is_correct": false,
            "explanation": "SMOTE is used for class imbalance by creating synthetic samples. It is not suitable for imputing missing values."
          },
          {
            "id": "C",
            "text": "Replace using MICE",
            "is_correct": false,
            "explanation": "MICE imputes missing values by chaining equations using other variables as predictors. This is not ideal if predictors per column are not required."
          },
          {
            "id": "D",
            "text": "Replace using Probabilistic PCA",
            "is_correct": true,
            "explanation": "Probabilistic PCA imputes missing values based on the underlying structure of the dataset and does not require individual predictors for each column, making it appropriate here."
          }
        ],
        "correct_answer": "D",
        "feedback": "Probabilistic PCA is the right choice in this case because it uses the latent structure of the data to estimate missing values. Unlike MICE, it does not rely on predictors for each column, making it ideal for datasets where many columns are missing values and predictor variables are not available or not applicable. This method is supported in Azure ML Studio’s Clean Missing Data module and helps preserve the data's structure without excessive complexity."
      },
      {
        "question_id": "55",
        "type": "multiple_choice_single_answer",
        "question": "Melinda and the IT team at The Brand Corporation are training a binary classification model to support admission approvals. They want to ensure the model is fair and does not discriminate based on ethnicity. What is the best method to evaluate the fairness of the model?",
        "options": [
          {
            "id": "A",
            "text": "Compare disparity between selection rates and performance metrics across ethnicities.",
            "is_correct": true,
            "explanation": "This is a valid approach to assess fairness. By analyzing selection rates and performance metrics for different ethnic groups, potential bias or discrimination in the model can be identified and addressed."
          },
          {
            "id": "B",
            "text": "Evaluate each trained model with a validation dataset, and use the model with the highest accuracy score. An accurate model is inherently fair.",
            "is_correct": false,
            "explanation": "Accuracy does not guarantee fairness. A highly accurate model may still discriminate if performance is uneven across groups."
          },
          {
            "id": "C",
            "text": "Evaluate each trained model with a validation ethnicity dataset to verify which values are more distributed, thereby less ethnically disposed.",
            "is_correct": false,
            "explanation": "Distribution alone does not ensure fairness. It's essential to compare key performance metrics between ethnic groups."
          },
          {
            "id": "D",
            "text": "Remove the ethnicity feature from the training dataset.",
            "is_correct": false,
            "explanation": "Removing the ethnicity feature does not eliminate bias and can make it harder to detect fairness issues."
          }
        ],
        "answer": "A",
        "feedback": "Fairness in machine learning models is best evaluated by comparing model behavior across sensitive groups. In this case, analyzing disparity in selection rates and performance metrics across ethnicities ensures that the model treats all groups fairly. Simply relying on accuracy or removing the sensitive attribute does not detect or mitigate bias. For such fairness evaluations, tools like Fairlearn can be integrated with Azure ML to compute metrics like demographic parity, equalized odds, and more."
      },
      {
        "question_id": "56",
        "type": "multiple_choice_single_answer",
        "question": "You are consulting on a project using Azure Machine Learning designer to create a real-time service endpoint. The team has a single Azure Machine Learning compute resource and needs to publish the inference pipeline as a web service. Which of the following compute types should the team use?",
        "options": [
          {
            "id": "A",
            "text": "Azure Databricks",
            "is_correct": false,
            "explanation": "Azure Databricks is a powerful platform for big data processing and analytics but is not suited for deploying real-time service endpoints using Azure ML designer."
          },
          {
            "id": "B",
            "text": "HDInsight",
            "is_correct": false,
            "explanation": "HDInsight is primarily used for big data frameworks like Hadoop and Spark, not for real-time endpoint deployment in Azure ML designer."
          },
          {
            "id": "C",
            "text": "Azure Kubernetes Services",
            "is_correct": true,
            "explanation": "Azure Kubernetes Services (AKS) is the recommended compute target for real-time endpoint deployments in Azure ML designer. It is scalable, container-based, and production-ready."
          }
        ],
        "correct_answer": "C",
        "feedback": "The correct compute type for deploying real-time inference pipelines created with Azure ML Designer is Azure Kubernetes Services (AKS). AKS supports containerized applications, offers flexibility and scalability, and is designed specifically for real-time inference hosting in production environments. This is aligned with Azure ML SDK v2 best practices for deploying models via managed online endpoints."
      },
      {
        "question_id": "57",
        "type": "drag_and_drop",
        "question": "Pym Tech is building a cloud-based machine learning solution using Apache Spark with automatic feature engineering. They must create and deploy notebooks using dynamic Spark workers, export notebooks for version control, and run them using AutoML. What is the correct sequence of steps to fulfill these requirements?",
        "options": [
          "a. Install Azure ML SDK for Python on the cluster",
          "b. Export the Jupyter notebook to a local environment",
          "c. Create Azure Databricks cluster",
          "d. Export the processed Jupyter notebook",
          "e. Install Microsoft Machine Learning for Apache Spark",
          "f. Create and execute Jupyter notebook using AutoML on the cluster",
          "g. Execute Zeppelin notebooks on the cluster",
          "h. Create an Azure HDInsight cluster to include the Apache Spark Mllib library"
        ],
        "correct_order": [
          "c. Create Azure Databricks cluster",
          "e. Install Microsoft Machine Learning for Apache Spark",
          "f. Create and execute Jupyter notebook using AutoML on the cluster",
          "d. Export the processed Jupyter notebook",
          "b. Export the Jupyter notebook to a local environment"
        ],
        "feedback": "This question tests your understanding of how to set up a Spark-based machine learning workflow with AutoML in Azure. The correct order begins by creating the compute environment (Databricks cluster), then installing the necessary ML packages. Once the environment is ready, AutoML is used to run notebooks. The processed notebooks are then exported, meeting the requirement for version control. HDInsight is not appropriate in this scenario, and Zeppelin notebooks are not needed when Jupyter notebooks and AutoML are used. This sequence aligns with best practices for cloud-based ML model retraining and deployment."
      },
      {
        "question_id": "58",
        "type": "multiple_choice_single_answer",
        "question": "Which regression method is best described as: 'The simplest form of regression, with no limit to the number of features used. This comes in many forms – often named by the number of features used and the shape of the curve that fits'?",
        "options": [
          {
            "id": "A",
            "text": "Linear Regression",
            "is_correct": true,
            "explanation": "Linear Regression is the simplest and most widely used form of regression. It models the relationship between one or more features and a target variable by fitting a linear equation. It can handle any number of features (univariate or multivariate), making it highly flexible and interpretable for regression tasks."
          },
          {
            "id": "B",
            "text": "Dynamic Programming Algorithms",
            "is_correct": false,
            "explanation": "Dynamic Programming is a general algorithm design technique, not a regression method. It is not used for fitting regression curves."
          },
          {
            "id": "C",
            "text": "Decision Trees",
            "is_correct": false,
            "explanation": "Decision Trees can be used for regression but they are not the simplest form. They create splits in the data and can lead to complex models."
          },
          {
            "id": "D",
            "text": "Ensemble Algorithms",
            "is_correct": false,
            "explanation": "Ensemble algorithms like Random Forest and Gradient Boosting combine multiple models and are more complex than simple linear regression."
          }
        ],
        "feedback": "Linear Regression is commonly referred to as the simplest form of regression. It is easy to interpret, robust with small data samples, and supports any number of input features. It is often the first algorithm data scientists use as a baseline model before trying more complex approaches."
      },
      {
        "question_id": "61",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a pipeline using Azure Machine Learning Designer. The pipeline must train a model using data from a CSV file published on a website. A dataset has not yet been created for this file. Which module should be added to the pipeline in Designer?",
        "options": [
          {
            "id": "A",
            "text": "Convert to CSV",
            "is_correct": false,
            "explanation": "The 'Convert to CSV' module is used to convert datasets into CSV format, not for importing data from external sources."
          },
          {
            "id": "B",
            "text": "Import Data",
            "is_correct": true,
            "explanation": "The 'Import Data' module allows you to bring data into the pipeline from various sources including web URLs, without needing to first register a dataset manually. It is ideal when the dataset hasn't been created yet and minimal admin effort is required."
          },
          {
            "id": "C",
            "text": "Dataset",
            "is_correct": false,
            "explanation": "The 'Dataset' module can only be used if a dataset has already been registered in Azure ML. Since the scenario states no dataset has been created yet, this option is not applicable."
          },
          {
            "id": "D",
            "text": "Enter Data Manually",
            "is_correct": false,
            "explanation": "This module is used for manually entering small datasets, not for importing CSV files from external web sources. It is not practical for model training."
          }
        ],
        "feedback": "When a dataset has not been registered and you need to ingest data from an external source such as a URL with minimal overhead, the 'Import Data' module in Azure Machine Learning Designer is the appropriate choice. It supports connecting to public web URLs directly and feeding the data into a pipeline for processing and training."
      },
      {
        "question_id": "dp100-q23",
        "type": "multiple_choice_single_answer",
        "question": "You have been contracted by Wayne Enterprises to help their IT team set up a new cluster in Azure Databricks. They want to understand what happens behind the scenes when a new cluster is created in the Azure Databricks workspace. What is an accurate description of these behind-the-scenes actions?",
        "options": [
          {
            "id": "A",
            "text": "Azure Databricks workspace is deployed on dedicated VMs where any given cluster nodes are symbionts of the VM hosts.",
            "is_correct": false,
            "explanation": "This is incorrect and uses non-standard terminology. Azure Databricks does not deploy clusters as 'symbionts' of VM hosts."
          },
          {
            "id": "B",
            "text": "Azure Databricks creates a cluster of driver and worker nodes, based on your VM type and size selections.",
            "is_correct": true,
            "explanation": "This is correct. Azure Databricks provisions driver and worker nodes based on your selection of VM types and sizes when creating a cluster."
          },
          {
            "id": "C",
            "text": "Azure Databricks provisions a dedicated VM that processes all jobs, based on your VM type and size selection.",
            "is_correct": false,
            "explanation": "This is incorrect. Azure Databricks uses a distributed architecture, so jobs are not processed on a single dedicated VM."
          },
          {
            "id": "D",
            "text": "When an Azure Databricks workspace is deployed, you are allocated a pool of VMs. Creating a cluster draws from this pool.",
            "is_correct": false,
            "explanation": "This is misleading. VM allocation happens dynamically based on configuration; a pre-allocated pool is not standard behavior."
          }
        ],
        "correct_answer": "B",
        "feedback": "Azure Databricks dynamically creates a cluster consisting of a driver node and one or more worker nodes when you initiate a new cluster. The size and type of each node are determined by your selections in the cluster configuration. This scalable architecture is essential for distributed processing in big data and machine learning workloads, offering both performance and flexibility. Understanding this behind-the-scenes behavior is critical for optimizing cluster cost and performance."
      },
      {
        "question_id": "dp100_azureml_designer_infer_clusters",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning designer to create a training pipeline for a clustering model. You want to use the model in an inference pipeline. Which module should the team use to infer cluster predictions from the model?",
        "options": [
          {
            "id": "A",
            "text": "Set Number of Centroids to equal the sample size",
            "is_correct": false,
            "explanation": "This option controls the number of clusters in the training step but is unrelated to inference."
          },
          {
            "id": "B",
            "text": "Score Model",
            "is_correct": false,
            "explanation": "Score Model is typically used to evaluate classification or regression models. For clustering predictions, it doesn't assign clusters."
          },
          {
            "id": "C",
            "text": "Train Clustering Model",
            "is_correct": false,
            "explanation": "This is used to train the clustering model, not to make predictions or assign new data to clusters."
          },
          {
            "id": "D",
            "text": "Assign Data to Clusters",
            "is_correct": true,
            "explanation": "The 'Assign Data to Clusters' module is used to assign each input data point to a predicted cluster after training."
          }
        ],
        "answer": "D",
        "feedback": "To make predictions with a clustering model in Azure Machine Learning Designer, you use the **Assign Data to Clusters** module. This module takes a trained clustering model and applies it to new data, assigning each instance to one of the learned clusters. Unlike classification models, where 'Score Model' is used for evaluation, clustering models rely on this specialized module to perform inference. This helps in applications such as customer segmentation or pattern discovery where assigning group membership is the goal."
      },
      {
        "question_id": "dp100_azureml_designer_infer_regression",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning designer to train a regression model to predict house prices. You have completed training and want to use the model to generate predictions on new data. Which module should you use in the inference pipeline to produce these predictions?",
        "options": [
          {
            "id": "A",
            "text": "Evaluate Model",
            "is_correct": false,
            "explanation": "Evaluate Model is used to assess the performance of the model, not to generate predictions on new data."
          },
          {
            "id": "B",
            "text": "Train Model",
            "is_correct": false,
            "explanation": "Train Model is used during the training phase, not during inference."
          },
          {
            "id": "C",
            "text": "Score Model",
            "is_correct": true,
            "explanation": "The Score Model module is used in inference pipelines to generate predictions using a trained model."
          },
          {
            "id": "D",
            "text": "Select Columns in Dataset",
            "is_correct": false,
            "explanation": "This module is used to filter or rearrange columns in a dataset, not to apply a model for predictions."
          }
        ],
        "answer": "C",
        "feedback": "In Azure Machine Learning Designer, after training a regression model, you use the Score Model module in the inference pipeline to apply the trained model to new data and generate predictions. This module outputs the predicted values alongside the input data, which is useful for tasks such as house price prediction or forecasting."
      },
      {
        "question_id": "dp100_responsible_ai_tabularexplainer",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is reviewing SHAP explainability options in Azure Machine Learning. She wants to use an explainer that selects an architecture-appropriate SHAP algorithm to interpret a model trained on tabular data. Which explainer should she use?",
        "options": [
          {
            "id": "A",
            "text": "MimicExplainer",
            "is_correct": false,
            "explanation": "MimicExplainer uses a surrogate model to approximate the original model and interpret it, but it does not use architecture-specific SHAP methods. It’s more suited for models that can't be interpreted directly."
          },
          {
            "id": "B",
            "text": "TabularExplainer",
            "is_correct": true,
            "explanation": "TabularExplainer is the correct choice for selecting the most appropriate SHAP algorithm based on the model architecture and data. It supports tree SHAP for tree-based models and kernel SHAP for others."
          },
          {
            "id": "C",
            "text": "RestExplainer",
            "is_correct": false,
            "explanation": "RestExplainer is not a valid explainer in Azure Machine Learning. It may be a distractor option."
          },
          {
            "id": "D",
            "text": "PFIExplainer",
            "is_correct": false,
            "explanation": "PFIExplainer uses permutation feature importance, which is model-agnostic and not SHAP-based. It is useful but not architecture-specific."
          }
        ],
        "answer": "B",
        "feedback": "The best option is **TabularExplainer**. This explainer automatically chooses the appropriate SHAP algorithm—TreeExplainer, DeepExplainer, or KernelExplainer—based on the model type and data structure. It simplifies the process of generating local and global feature importance for tabular models, making it ideal for Responsible AI practices in Azure ML."
      },
      {
        "question_id": "dp100_responsibleai_tabular_shap",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is looking into architecture-appropriate SHAP algorithms and comes to you for clarification about which explainer uses an architecture-appropriate SHAP algorithm to interpret a model. Which of the following should you tell her is the best choice?",
        "options": [
          {
            "id": "A",
            "text": "MimicExplainer",
            "is_correct": false,
            "explanation": "MimicExplainer does not use SHAP; it trains a surrogate interpretable model and explains that model instead of the original."
          },
          {
            "id": "B",
            "text": "TabularExplainer",
            "is_correct": true,
            "explanation": "TabularExplainer automatically selects the most appropriate SHAP-based algorithm depending on the model architecture (e.g., TreeExplainer for tree-based models, DeepExplainer for neural networks)."
          },
          {
            "id": "C",
            "text": "RestExplainer",
            "is_correct": false,
            "explanation": "RestExplainer is not a valid explainer in Azure Machine Learning or the interpretability SDK."
          },
          {
            "id": "D",
            "text": "PFIExplainer",
            "is_correct": false,
            "explanation": "PFIExplainer uses permutation feature importance, which is a model-agnostic technique, but it does not use SHAP algorithms."
          }
        ],
        "answer": "B",
        "feedback": "The correct explainer for using SHAP in a way that automatically adapts to the underlying model architecture is TabularExplainer. It intelligently selects the right SHAP algorithm (e.g., TreeExplainer, DeepExplainer) based on the model you provide, making it ideal for architecture-appropriate interpretability in Azure Machine Learning."
      },
      {
        "question_id": "62",
        "type": "multiple_choice_single_answer",
        "question": "You are responsible for creating different deep learning models in your company. You have trained a multi-class image classification model using PyTorch version 1.2. To ensure the correct PyTorch version is used during deployment, what should you do?",
        "options": [
          {
            "id": "A",
            "text": "Register the model with a .pt file extension and the default version property.",
            "is_correct": false,
            "explanation": "The file extension alone does not ensure that the correct framework version is used during deployment. This approach lacks explicit version specification."
          },
          {
            "id": "B",
            "text": "Save the model locally as a .pt file, and deploy the model as a local web service.",
            "is_correct": false,
            "explanation": "Saving the model locally and deploying it as a local web service does not integrate with Azure ML's environment management, and thus doesn't ensure correct framework version usage."
          },
          {
            "id": "C",
            "text": "Deploy the model on computer that is configured to use the default Azure Machine Learning conda environment.",
            "is_correct": false,
            "explanation": "Using the default environment doesn't guarantee the correct version of PyTorch will be available, especially if version 1.2 is needed explicitly."
          },
          {
            "id": "D",
            "text": "Register the model, specifying the model_framework and model_framework_version properties.",
            "is_correct": true,
            "explanation": "This is the correct approach. By registering the model and specifying the `model_framework` and `model_framework_version`, Azure ML will ensure that the appropriate version of the framework (PyTorch 1.2 in this case) is used during deployment."
          }
        ],
        "answer": "D",
        "feedback": "When deploying deep learning models in Azure ML, it's important to register the model along with the framework type and version using `model_framework` and `model_framework_version` properties. This ensures that the model is run in an environment with the correct dependencies, avoiding issues during inference. This is especially critical for models that are version-sensitive, like those built with PyTorch."
      },
      {
        "question_id": "67",
        "type": "multiple_choice_single_answer",
        "question": "Which error type should you choose where a person does not have a disease and the model classifies the case as having a disease?",
        "options": [
          {
            "id": "A",
            "text": "True Negative",
            "is_correct": false,
            "explanation": "This refers to a case where the model correctly predicts that a person does not have the disease."
          },
          {
            "id": "B",
            "text": "True Positive",
            "is_correct": false,
            "explanation": "This is when the model correctly predicts that a person has the disease."
          },
          {
            "id": "C",
            "text": "False Positive",
            "is_correct": true,
            "explanation": "A False Positive occurs when the model incorrectly classifies a healthy person as having the disease."
          },
          {
            "id": "D",
            "text": "False Negative",
            "is_correct": false,
            "explanation": "This is when the model incorrectly predicts a person does not have the disease when in fact they do."
          }
        ],
        "answer": "C",
        "feedback": "In binary classification, a **False Positive** means that the model predicted the positive class (in this case, that the person has a disease) when the actual label was negative (the person is healthy). This type of error can lead to unnecessary follow-up procedures or anxiety. It's important to identify and monitor false positives especially in medical use cases, where minimizing them can reduce stress and cost for patients."
      },
      {
        "question_id": "68",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is researching differential privacy and asks you for a short description of how it works. Which of the following should you tell her?",
        "options": [
          {
            "id": "A",
            "text": "All values in the dataset have salts added to the content which are then encrypted to make the data sterile from a privacy standpoint.",
            "is_correct": false,
            "explanation": "Salting and encryption are cryptographic techniques, not specific to differential privacy. This description is inaccurate for the concept of differential privacy."
          },
          {
            "id": "B",
            "text": "Noise is added to the data during analysis so that aggregations are statistically consistent with the data distribution but non-deterministic.",
            "is_correct": true,
            "explanation": "This accurately describes differential privacy, where controlled noise is added to the outputs of data queries to preserve privacy while ensuring utility of the results."
          },
          {
            "id": "C",
            "text": "All numeric column values in the dataset are converted to the mean value for the column. Analyses of the data use the mean values instead of the actual values.",
            "is_correct": false,
            "explanation": "This describes a form of data obfuscation or anonymization, not differential privacy."
          },
          {
            "id": "D",
            "text": "All numeric values in the dataset are encrypted and cannot be used in analysis.",
            "is_correct": false,
            "explanation": "Encryption protects data in storage or transmission but does not allow analysis in the same way as differential privacy does."
          }
        ],
        "answer": "B",
        "feedback": "Differential privacy is a mathematical framework used to ensure that statistical analysis of a dataset does not compromise the privacy of individuals in the dataset. It works by injecting carefully calibrated random noise into the query results. This allows users to analyze patterns across data while making it difficult to determine if any one individual's information was used. In Microsoft Azure, differential privacy is used in responsible AI practices to ensure models do not leak sensitive information about individuals. Learn more here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-privacy"
      },
      {
        "question_id": "dp100_sweep_job_auc_logging",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure Machine Learning SDK v2 to run a sweep job that tunes hyperparameters for a classification model. The goal is to optimize the AUC (Area Under the Curve) metric. \n\nThe experiment script trains a model using the `y_test` and `y_predicted` variables. A developer includes the following line in the script:\n\n```python\nprint(roc_auc_score(y_test, y_predicted))\n```\n\nYou want the sweep job to optimize based on the AUC value. \n\nDoes this script satisfy the requirement to log the metric for hyperparameter tuning?",
        "options": [
          {
            "id": "A",
            "text": "Yes",
            "is_correct": false,
            "explanation": "Printing the AUC value is not sufficient. You must explicitly log the metric using `mlflow.log_metric()` or the `run.log()` function in SDK v1 for the sweep job to use it."
          },
          {
            "id": "B",
            "text": "No",
            "is_correct": true,
            "explanation": "In SDK v2, metrics must be logged using `mlflow.log_metric()` to be tracked and optimized during a sweep job. Simply printing the metric to console will not make it available to Azure ML for optimization."
          }
        ],
        "answer": "B",
        "feedback": "To ensure the sweep job can use the AUC metric for optimization, you must use `mlflow.log_metric('AUC', auc)` within the training script. This makes the metric visible to Azure ML and usable for hyperparameter optimization. Printing the value is not sufficient."
      },
      {
        "question_id": "dp100_sdkv2_auc_logging",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning SDK v2 to run a Hyperparameter tuning experiment for a classification model. You want to optimize the AUC metric and ensure the metric is logged correctly to track progress. Given the validation labels `y_test` and the predicted probabilities `y_predicted`, which line should be added to the script so that the AUC is logged for Hyperparameter optimization?",
        "options": [
          {
            "id": "A",
            "text": "print(f'AUC: {roc_auc_score(y_test, y_predicted)}')",
            "is_correct": false,
            "explanation": "This only prints the metric. It does not log it to Azure ML for hyperparameter optimization."
          },
          {
            "id": "B",
            "text": "mlflow.log_metric('AUC', float(roc_auc_score(y_test, y_predicted)))",
            "is_correct": true,
            "explanation": "This correctly logs the AUC value using MLflow, as expected in SDK v2 experiments."
          },
          {
            "id": "C",
            "text": "run.log('AUC', np.float(roc_auc_score(y_test, y_predicted)))",
            "is_correct": false,
            "explanation": "This syntax is from SDK v1 (using `run.log`) and is not valid in SDK v2."
          },
          {
            "id": "D",
            "text": "mlclient.log_metric('AUC', roc_auc_score(y_test, y_predicted))",
            "is_correct": false,
            "explanation": "`mlclient` does not have a `log_metric` method. Only MLflow is used for logging in SDK v2."
          }
        ],
        "feedback": "To log metrics in Azure ML SDK v2, MLflow is the recommended approach. `mlflow.log_metric(...)` enables Hyperparameter tuning to track and optimize metrics such as AUC during training. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow"
      },
      {
        "question_id": "69",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following is best described by: 'These construct not just one decision tree, but a large number of trees – allowing better predictions on more complex data. Widely used in machine learning and science due to their strong prediction abilities'?",
        "options": [
          {
            "id": "A",
            "text": "Least Squares Regression",
            "is_correct": false,
            "explanation": "Least Squares Regression is a method for fitting a linear model by minimizing the sum of squared residuals, not a tree-based ensemble method."
          },
          {
            "id": "B",
            "text": "Ensemble Algorithms",
            "is_correct": true,
            "explanation": "Ensemble algorithms, such as Random Forests and Gradient Boosting Machines, combine multiple decision trees to enhance prediction accuracy. They are robust, reduce overfitting, and are widely used in machine learning for both classification and regression tasks."
          },
          {
            "id": "C",
            "text": "Dynamic Programming Algorithms",
            "is_correct": false,
            "explanation": "Dynamic programming is a technique used to solve optimization problems by breaking them into simpler subproblems, not for combining decision trees."
          },
          {
            "id": "D",
            "text": "Linear Regression",
            "is_correct": false,
            "explanation": "Linear regression models the relationship between a dependent variable and one or more independent variables using a straight line, not a set of decision trees."
          }
        ],
        "feedback": "The correct answer is Ensemble Algorithms. These algorithms, especially those based on decision trees like Random Forest and XGBoost, are powerful techniques that improve prediction by aggregating the results of many trees. This makes them more accurate and robust, especially when working with complex datasets. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#ensemble-models"
      },
      {
        "question_id": "70",
        "type": "multiple_choice_single_answer",
        "question": "Dr. Karl Malus is creating a convolutional neural network and wants to reduce the size of the feature maps that are generated by a convolutional layer. What should he do?",
        "options": [
          {
            "id": "A",
            "text": "Make adjustments to weight values during backpropagation",
            "is_correct": false,
            "explanation": "Weight adjustments during backpropagation affect learning but do not change the size of the feature maps."
          },
          {
            "id": "B",
            "text": "Add a pooling layer after the convolutional layer",
            "is_correct": true,
            "explanation": "Pooling layers (like max pooling) reduce the spatial dimensions (width and height) of the feature maps, making the model more computationally efficient and less prone to overfitting."
          },
          {
            "id": "C",
            "text": "Reduce the size of the filter kernel used in the convolutional layer",
            "is_correct": false,
            "explanation": "Reducing the filter size may influence the granularity of feature detection, but does not directly reduce the output feature map size."
          },
          {
            "id": "D",
            "text": "Increase the number of filters in the convolutional layer",
            "is_correct": false,
            "explanation": "Increasing the number of filters increases the depth (number of channels) of the feature maps, not their spatial size."
          }
        ],
        "feedback": "To reduce the size of feature maps in a convolutional neural network, the correct approach is to add a **pooling layer** after the convolutional layer. This technique is standard in CNN architectures for dimensionality reduction and performance improvement. Learn more at: https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning"
      },
      {
        "question_id": "71",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following best describes the purpose of training data?",
        "options": [
          {
            "id": "A",
            "text": "A subset of the data used to ‘test’ what the algorithm has learned.",
            "is_correct": false,
            "explanation": "This describes the test dataset, which is used to evaluate model performance after training."
          },
          {
            "id": "B",
            "text": "The dataset used in its entirety to ‘teach’ the algorithm.",
            "is_correct": false,
            "explanation": "Training is done on a subset of the data, not the entire dataset. The rest is used for validation/testing."
          },
          {
            "id": "C",
            "text": "A subset of the data used to ‘teach’ the algorithm. This is the data that the algorithm will learn from.",
            "is_correct": true,
            "explanation": "Training data is the labeled dataset used to fit the model. It enables the algorithm to learn relationships or patterns in the data."
          },
          {
            "id": "D",
            "text": "The dataset is used to ensure the functionality of the algorithm generates consistently correct answers based on checksums.",
            "is_correct": false,
            "explanation": "This refers to system validation or software testing, not machine learning training."
          }
        ],
        "feedback": "Training data refers to the portion of labeled data used to train or 'teach' the model. During training, the model learns patterns and makes adjustments to minimize prediction errors. It is essential for supervised learning tasks where input-output mappings are learned. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-training-and-scoring"
      },
      {
        "question_id": "dp100_endpoint_realtime_auth_sdkv2",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a real-time image classification model using Azure Machine Learning SDK v2. The model will be exposed through a secured endpoint that requires key-based authentication. You need to configure and deploy the service to meet the security requirements. Which of the following steps will correctly meet the objective?",
        "options": [
          {
            "id": "A",
            "text": "Create a KubernetesOnlineEndpoint with authentication mode set to 'key'. Deploy the model to the endpoint.",
            "is_correct": true,
            "explanation": "This correctly reflects the secure deployment flow in Azure ML SDK v2 using KubernetesOnlineEndpoint with key-based authentication."
          },
          {
            "id": "B",
            "text": "Create a ManagedOnlineEndpoint without authentication and deploy the model.",
            "is_correct": false,
            "explanation": "This does not meet the security requirement since it skips enabling authentication."
          },
          {
            "id": "C",
            "text": "Use AksWebservice and set 'auth_enabled=True' before deploying the model.",
            "is_correct": false,
            "explanation": "This syntax belongs to Azure ML SDK v1 and is not valid in SDK v2."
          },
          {
            "id": "D",
            "text": "Create a batch endpoint and enable identity-based access control.",
            "is_correct": false,
            "explanation": "Batch endpoints are not suitable for real-time inference."
          }
        ],
        "answer": "A",
        "explanation": "In Azure ML SDK v2, for real-time inference using a Kubernetes cluster, you should create a `KubernetesOnlineEndpoint`, set the `auth_mode` to `'key'`, and then deploy the model. This ensures secure access to the model endpoint.",
        "feedback": "To deploy a secure real-time inference model with SDK v2, use either `ManagedOnlineEndpoint` or `KubernetesOnlineEndpoint`. If the scenario requires control over infrastructure (like AKS), use KubernetesOnlineEndpoint and set `auth_mode='key'` for key-based access. [Learn more](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-kubernetes-online-endpoint?view=azureml-api-2)."
      },
      {
        "question_id": "73",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a model as a real-time inferencing service in Azure Machine Learning. What functions must the entry script for the service include?",
        "options": [
          {
            "id": "A",
            "text": "Init() and run(raw_data)",
            "is_correct": true,
            "explanation": "The `init()` function initializes the model when the container starts, and the `run(raw_data)` function is called for each inference request. This is the standard structure expected by Azure ML real-time endpoints."
          },
          {
            "id": "B",
            "text": "Main() and predict(raw_data)",
            "is_correct": false,
            "explanation": "`main()` and `predict()` are not recognized by the Azure ML inference interface. Azure ML expects `init()` and `run()` for the entry script."
          },
          {
            "id": "C",
            "text": "Load() and score(raw_data)",
            "is_correct": false,
            "explanation": "These are not the correct function names for Azure ML real-time endpoints. While conceptually similar, the service specifically expects `init()` and `run()`."
          }
        ],
        "answer": "A",
        "explanation": "When deploying a model for real-time inference in Azure Machine Learning (SDK v2), your entry script must define two key functions: `init()` and `run(raw_data)`. The `init()` function is called once when the container is started to load the model into memory. The `run()` function is called each time the endpoint receives a request and processes the input. This structure is essential for Azure ML to know how to initialize and invoke your model properly.",
        "feedback": "Real-time endpoints in Azure ML require a scoring script with `init()` and `run(raw_data)`. For example:\n\n```python\nimport joblib\n\ndef init():\n    global model\n    model = joblib.load('model.joblib')\n\ndef run(data):\n    return model.predict(data)\n```\n\nRead more in the [official documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-model-custom-entry-script?view=azureml-api-2)."
      },
      {
        "question_id": "74",
        "type": "multiple_choice_single_answer",
        "question": "In Azure Machine Learning, datastores are abstractions for cloud data sources. Which dataset type is described by: 'The data is read from the dataset as a table. You should use this type of dataset when your data is consistently structured and you want to work with it in common tabular data structures, such as Pandas dataframes.'?",
        "options": [
          {
            "id": "A",
            "text": "Script argument",
            "is_correct": false,
            "explanation": "Script arguments are used to pass parameters to the training script but are not related to reading data as structured tables."
          },
          {
            "id": "B",
            "text": "Tabular",
            "is_correct": true,
            "explanation": "Tabular datasets in Azure ML are designed for structured data that can be easily represented in a dataframe. They support operations such as filtering and transformations and are ideal for machine learning tasks."
          },
          {
            "id": "C",
            "text": "Named input",
            "is_correct": false,
            "explanation": "Named inputs are used for referencing datasets or datastores in job configurations but do not represent a dataset type by themselves."
          },
          {
            "id": "D",
            "text": "File",
            "is_correct": false,
            "explanation": "File datasets are used when data needs to be read as raw files. They are not directly loaded as structured dataframes."
          }
        ],
        "answer": "B",
        "explanation": "Tabular datasets in Azure Machine Learning represent structured data, such as CSV or TSV files, that are read into memory as dataframes. This makes them suitable for training and evaluation tasks, where consistent schema and tabular format are expected.",
        "feedback": "The 'Tabular' dataset type is the correct choice when working with structured data formats in Azure ML SDK v2. They are typically loaded as pandas or spark dataframes, which facilitates preprocessing, feature engineering, and training workflows.\n\n🔗 Reference: [Azure ML Tabular Datasets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets)"
      },      
      {
        "question_id": "75",
        "type": "multiple_choice_single_answer",
        "question": "You are building a multi-class image classification model using PyTorch 1.3 in Azure Machine Learning SDK v2. You must run a training script named train.py and avoid installing any additional libraries manually. What should you do to ensure the environment is correctly set up for training?",
        "options": [
          {
            "id": "A",
            "text": "Create a custom Docker image and use it as the environment in the job.",
            "is_correct": false,
            "explanation": "While this would work, it requires manual effort, which contradicts the requirement to avoid manual setup."
          },
          {
            "id": "B",
            "text": "Use a curated environment that includes PyTorch and specify it in the training job YAML or Command component.",
            "is_correct": true,
            "explanation": "Azure provides curated environments with common frameworks preinstalled, including PyTorch. Using one avoids the need for manual library installation."
          },
          {
            "id": "C",
            "text": "Use a generic Python environment and install PyTorch inside the training script.",
            "is_correct": false,
            "explanation": "Installing packages inside the training script is not recommended and contradicts the requirement."
          },
          {
            "id": "D",
            "text": "Use the SKLearn curated environment to train the PyTorch model.",
            "is_correct": false,
            "explanation": "The SKLearn environment does not include PyTorch, so it's not suitable for training PyTorch models."
          }
        ],
        "answer": "B",
        "explanation": "In Azure ML SDK v2, it's best practice to use curated environments for training. Microsoft provides ready-to-use environments with frameworks like PyTorch preinstalled. This minimizes setup time and ensures compatibility with training scripts like train.py that depend on PyTorch 1.3.",
        "feedback": "For training PyTorch models in Azure ML SDK v2, you should reference a curated environment such as `AzureML-pytorch-1.13-ubuntu20.04-py38-cuda11.7`. This avoids the need to manually manage dependencies. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-environments-v2"
      },
      {
        "question_id": "76",
        "type": "multiple_choice_single_answer",
        "question": "Your team has deployed a model to an Azure Managed Online Endpoint using Azure Machine Learning SDK v2. You need to invoke the endpoint using Python code and pass JSON input to it. The deployed endpoint is named 'credit-risk-endpoint', and the deployment is called 'blue'. The input data is stored in a dictionary called `input_payload`. What is the correct method to invoke the endpoint?",
        "options": [
          {
            "id": "A",
            "text": "from azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nclient = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\nresponse = client.online_endpoints.invoke(endpoint_name='credit-risk-endpoint', deployment_name='blue', request_body=input_payload)",
            "is_correct": true,
            "explanation": "This is the correct method in SDK v2 to invoke an online endpoint with deployment name and request payload."
          },
          {
            "id": "B",
            "text": "from azureml.core.webservice import Webservice\nservice = Webservice(name='credit-risk-endpoint', workspace=ws)\npredictions = service.run(input_payload)",
            "is_correct": false,
            "explanation": "This is SDK v1 syntax and is not compatible with the SDK v2 methods or endpoints."
          },
          {
            "id": "C",
            "text": "client = MLClient(DefaultAzureCredential())\npredictions = client.endpoints.run_json(input_payload)",
            "is_correct": false,
            "explanation": "There is no 'run_json' method in MLClient; the method must use 'invoke' for online endpoints."
          },
          {
            "id": "D",
            "text": "from azure.ai.ml import MLClient\nclient = MLClient(DefaultAzureCredential())\nresponse = client.batch_endpoints.invoke(input=input_payload)",
            "is_correct": false,
            "explanation": "Batch endpoints use a different interface. This question refers to real-time (online) inference."
          }
        ],
        "answer": "A",
        "explanation": "In Azure ML SDK v2, online endpoints are invoked using `ml_client.online_endpoints.invoke()` with required parameters: the endpoint name, deployment name, and the JSON-formatted input payload.",
        "feedback": "To perform real-time inference using Azure ML SDK v2, you must use the `invoke()` method from `online_endpoints`. Refer to the official documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints"
      },
      {
        "question_id": "dp100_sdkv2_interpretability_tabular",
        "type": "multiple_choice_single_answer",
        "question": "You trained a regression model in Azure Machine Learning SDK v2. Now you want to explain its predictions using SHAP values in a way that adapts automatically to the model type and produces visualizations for feature importance. Which component or tool should you use in Azure Machine Learning?",
        "options": [
          {
            "id": "A",
            "text": "Use TabularExplainer with SHAP via azureml.interpret package",
            "is_correct": false,
            "explanation": "This approach is based on SDK v1 and is no longer used in SDK v2 workflows."
          },
          {
            "id": "B",
            "text": "Use the Responsible AI dashboard with RAIInsights",
            "is_correct": true,
            "explanation": "Responsible AI dashboard in SDK v2 provides automated SHAP analysis via RAIInsights, adapting to the model type and showing visualizations for interpretability."
          },
          {
            "id": "C",
            "text": "Use interpretML package from Scikit-learn",
            "is_correct": false,
            "explanation": "interpretML is a standalone Python library, not integrated with Azure ML SDK v2 workflows."
          },
          {
            "id": "D",
            "text": "Enable AutoML explainability toggle and export as JSON",
            "is_correct": false,
            "explanation": "This is a feature available in AutoML runs, but it does not provide a reusable or general method for SHAP-based explanation outside of AutoML runs."
          }
        ],
        "explanation": "In SDK v2, the recommended way to interpret a model is through the Responsible AI dashboard, which uses RAIInsights. It includes SHAP-based explanations automatically adapted to model type, providing global and local feature importance with rich visualizations.",
        "sdk_version": "v2"
      },
      {
        "question_id": "77",
        "type": "multiple_choice_single_answer",
        "question": "You need to create a compute target for training experiments that require a graphical processing unit (GPU). You want to be able to scale the compute so that multiple nodes are started automatically as required. Which kind of compute target should you create?",
        "options": [
          {
            "id": "A",
            "text": "Compute Cluster",
            "is_correct": true,
            "explanation": "A compute cluster allows dynamic scaling, including GPU-enabled virtual machines, making it the ideal choice for scalable model training in Azure ML."
          },
          {
            "id": "B",
            "text": "Inference Cluster",
            "is_correct": false,
            "explanation": "Inference Clusters are intended for model deployment and not for training purposes."
          },
          {
            "id": "C",
            "text": "Compute Instance",
            "is_correct": false,
            "explanation": "Compute Instances are for interactive development (e.g., notebooks) and do not support scaling across multiple nodes."
          }
        ],
        "explanation": "In Azure Machine Learning SDK v2, a **Compute Cluster** is used when you want to scale model training across multiple nodes, including those with GPU. The cluster scales automatically based on job demand, making it ideal for deep learning and large dataset scenarios. Compute Instances are single-node VMs for development, while Inference Clusters are for real-time model deployment.",
        "sdk_version": "v2"
      },
      {
  "question_id": "78",
  "type": "multiple_choice_single_answer",
  "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company. You need to select a compute target to deploy the workspace. What should you use?",
  "options": [
    {
      "id": "A",
      "text": "Azure Container Service",
      "is_correct": false,
      "explanation": "Azure Container Service (deprecated) is not a supported compute target for deploying an Azure Machine Learning workspace."
    },
    {
      "id": "B",
      "text": "Azure Data Lake Analytics",
      "is_correct": false,
      "explanation": "Azure Data Lake Analytics is a big data analytics service and is not used for deploying Azure ML workspaces."
    },
    {
      "id": "C",
      "text": "Apache Spark for HDInsight",
      "is_correct": false,
      "explanation": "Apache Spark for HDInsight is used for large-scale data processing but is not intended as a compute target for Azure ML workspaces."
    },
    {
      "id": "D",
      "text": "Azure Databricks",
      "is_correct": true,
      "explanation": "Azure Databricks can be used as a compute target in Azure Machine Learning. It integrates with Azure ML for running distributed training jobs and is commonly used for collaborative data science workflows."
    }
  ],
  "feedback": "Azure Databricks is a recommended compute target when working with Azure Machine Learning for large-scale data science and machine learning projects. It supports integration with Azure ML SDK v2 and allows scalable training, distributed computing, and advanced collaboration through notebooks."
}, 
{
    "question_id": "78A",
    "type": "multiple_choice_single_answer",
    "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company. You need to select a compute target to deploy the workspace. What should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Container Service",
        "is_correct": false
      },
      {
        "id": "B",
        "text": "Azure Data Lake Analytics",
        "is_correct": false
      },
      {
        "id": "C",
        "text": "Apache Spark for HDInsight",
        "is_correct": false
      },
      {
        "id": "D",
        "text": "Azure Databricks",
        "is_correct": true
      }
    ],
    "feedback": "Azure Databricks is a powerful analytics platform optimized for big data and machine learning. It integrates natively with Azure Machine Learning and is one of the supported compute targets when creating or attaching a workspace. While other services listed are useful in data engineering or analytics, they do not serve as deployable compute targets for Azure ML workspaces."
  },
  {
    "question_id": "78B",
    "type": "multiple_choice_single_answer",
    "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company. You need to select a compute target to deploy the workspace. What should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Container Service",
        "is_correct": true,
        "explanation": "Azure Container Instances (ACI) is a lightweight, serverless compute target ideal for deploying models and workspaces during development and testing phases. It is commonly used for hosting Azure ML services with minimal setup and cost."
      },
      {
        "id": "B",
        "text": "Azure Data Lake Analytics",
        "is_correct": false,
        "explanation": "Azure Data Lake Analytics is a distributed analytics service designed for big data batch processing. It is not a compute target for Azure Machine Learning and cannot host or deploy ML workspaces."
      },
      {
        "id": "C",
        "text": "Apache Spark for HDInsight",
        "is_correct": false,
        "explanation": "Apache Spark on HDInsight is used for large-scale distributed data processing, not for deploying Azure ML workspaces. While Spark can process data for ML, it is not a valid compute target for workspace deployment."
      },
      {
        "id": "D",
        "text": "Azure Databricks",
        "is_correct": false,
        "explanation": "Azure Databricks is an analytics platform optimized for Apache Spark and can be used to run ML workloads, but it is not used to deploy or host Azure ML workspaces. It integrates with Azure ML but is not a workspace compute target."
      }
    ],
    "feedback": "The correct answer is Azure Container Service (ACI), which is used as a lightweight compute target to deploy Azure Machine Learning models or workspaces, especially during development. The other options are analytics services or data processing engines and are not valid for deploying Azure ML workspaces."
  },
  {
    "question_id": "79",
    "type": "multiple_choice_single_answer",
    "question": "The team has trained a classification model using the scikit-learn LogisticRegression class. They want to use the model to return labels for new data in the array x_new. Which code from the following list should they use?",
    "options": [
      {
        "id": "A",
        "text": "Model.score(x_new, y_new)",
        "is_correct": false,
        "explanation": "The score() method returns the accuracy of the model on given test data and labels. It does not return predictions, but rather a performance metric."
      },
      {
        "id": "B",
        "text": "Model.predict(x_new)",
        "is_correct": true,
        "explanation": "The predict() method is used to return predicted class labels for the input samples (x_new). It is the correct method to use when you want the model to make predictions."
      },
      {
        "id": "C",
        "text": "Model.fit(x_new)",
        "is_correct": false,
        "explanation": "The fit() method is used to train the model, not to make predictions. Calling it here would attempt to retrain the model rather than infer outcomes."
      },
      {
        "id": "D",
        "text": "Model.fit(x_new, y_new)",
        "is_correct": false,
        "explanation": "This is also for training the model. Since the model is already trained, using fit() again is unnecessary and would overwrite previous training."
      }
    ],
    "answer": "B",
    "explanation": "To generate predictions using a trained scikit-learn LogisticRegression model, you must use the predict() method and pass in the new data array. This method returns the predicted labels. Using fit() would retrain the model, and score() only returns accuracy metrics, not label predictions.",
    "references": [
      "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict"
    ]
  },
  {
    "question_id": "80",
    "type": "multiple_choice_single_answer",
    "question": "The team is solving a classification task and must evaluate the current model on a limited data sample by using k-fold cross-validation. The data scientist starts by configuring a k parameter as the number of splits and needs to configure the k parameter for the cross-validation. Which of the following values should they use?",
    "options": [
      {
        "id": "A",
        "text": "K=10",
        "is_correct": true,
        "explanation": "K=10 is a commonly used and recommended value for k-fold cross-validation, especially when working with limited data. It provides a good tradeoff between bias and variance."
      },
      {
        "id": "B",
        "text": "K=0.5",
        "is_correct": false,
        "explanation": "K must be an integer greater than 1 representing the number of folds, not a float value. 0.5 is not valid for k-fold cross-validation."
      },
      {
        "id": "C",
        "text": "K=1",
        "is_correct": false,
        "explanation": "K=1 is invalid because cross-validation requires at least 2 splits. Using 1 would mean no actual validation step occurs."
      },
      {
        "id": "D",
        "text": "K=0.9",
        "is_correct": false,
        "explanation": "K must be an integer. 0.9 is a float and not acceptable as a parameter for the number of splits in k-fold cross-validation."
      }
    ],
    "answer": "A",
    "explanation": "In k-fold cross-validation, the dataset is split into k equal-sized parts. The model is trained on k-1 parts and validated on the remaining part. A common default value is k=10 because it gives a reliable estimate of model performance without being too computationally expensive.",
    "references": [
      "https://scikit-learn.org/stable/modules/cross_validation.html#k-fold"
    ]
  },
  {
    "question_id": "82",
    "type": "multiple_choice_single_answer",
    "question": "Identify the missing word(s) in the following sentence within the context of Microsoft Azure. After installing the SDK package in your Python environment, you can write code to connect to your workspace and perform machine learning operations. The easiest way to connect to a workspace is to use a [?].",
    "options": [
      {
        "id": "A",
        "text": "VPN",
        "is_correct": false,
        "explanation": "A VPN provides a secure network connection but is not used for connecting to an Azure Machine Learning workspace via SDK."
      },
      {
        "id": "B",
        "text": "Workspace configuration file",
        "is_correct": true,
        "explanation": "The workspace configuration file (config.json) contains all necessary identifiers to connect to an Azure ML workspace easily using the SDK."
      },
      {
        "id": "C",
        "text": "Azure endpoint",
        "is_correct": false,
        "explanation": "An Azure endpoint may refer to deployed model endpoints or services, but it is not used to initialize SDK workspace connections."
      },
      {
        "id": "D",
        "text": "SFTP",
        "is_correct": false,
        "explanation": "SFTP is a protocol used for secure file transfer, unrelated to Azure ML workspace authentication or SDK usage."
      }
    ],
    "answer": "B",
    "feedback": "To authenticate and connect to your Azure Machine Learning workspace using the SDK, the simplest method is using the workspace configuration file (usually named config.json). This file includes your subscription ID, resource group, and workspace name, and is loaded by the SDK to establish a secure and convenient connection."
  },
  {
    "question_id": "83",
    "type": "multiple_choice_single_answer",
    "question": "You are using an Azure Machine Learning designer pipeline to train and test a K-Means clustering model. You want the model to assign items to one of three clusters. Which configuration property of the K-Means Clustering module should the team set to accomplish this?",
    "options": [
      {
        "id": "A",
        "text": "Set Iterations to 3",
        "is_correct": false,
        "explanation": "Setting iterations controls how many times the algorithm repeats the clustering process, but it doesn't define the number of clusters."
      },
      {
        "id": "B",
        "text": "Set Number of Centroids to 3",
        "is_correct": true,
        "explanation": "This property defines how many clusters (or centroids) the K-Means algorithm should form. Setting it to 3 will produce three clusters."
      },
      {
        "id": "C",
        "text": "Set Random number seed to 3",
        "is_correct": false,
        "explanation": "The random number seed controls randomness in centroid initialization, but not the number of clusters."
      },
      {
        "id": "D",
        "text": "Set the Km value to 3",
        "is_correct": false,
        "explanation": "There is no configuration called 'Km' in the Azure ML designer for K-Means."
      }
    ],
    "answer": "B",
    "feedback": "In K-Means clustering, the number of centroids determines how many clusters the algorithm will identify. Setting 'Number of Centroids to 3' tells the algorithm to split the data into 3 distinct groups based on feature similarity. The other options control parameters like iteration count or randomness but not the actual number of clusters."
  },
  {
    "question_id": "84",
    "type": "multiple_choice_single_answer",
    "question": "You are responsible to create different Azure Machine Learning models for your company. You are creating a binary classification by using a two-class logistic regression model. You need to evaluate the model results for imbalance. Which evaluation metric should you use?",
    "options": [
      {
        "id": "A",
        "text": "Mean Absolute Error",
        "is_correct": false,
        "explanation": "This is a regression metric that measures the average magnitude of errors in predictions. It is not suitable for evaluating classification models."
      },
      {
        "id": "B",
        "text": "Relative Absolute Error",
        "is_correct": false,
        "explanation": "This is another regression-specific metric. It is not appropriate for assessing the performance of classification models."
      },
      {
        "id": "C",
        "text": "AUC Curve",
        "is_correct": true,
        "explanation": "The AUC (Area Under the Curve) metric evaluates the model’s ability to distinguish between classes, especially in imbalanced classification tasks. It is a common and effective metric for binary classification evaluation."
      },
      {
        "id": "D",
        "text": "Relative Squared Error",
        "is_correct": false,
        "explanation": "This metric is used in regression tasks and does not provide insight into classification performance, especially for imbalanced datasets."
      }
    ],
    "feedback": "AUC (Area Under the ROC Curve) is the most appropriate metric for evaluating classification performance, particularly in the case of imbalanced datasets. It measures how well the model distinguishes between the two classes, making it ideal for logistic regression evaluation tasks. The other metrics listed are designed for regression tasks and are not suitable for classification problems."
  },
  {
    "question_id": "85",
    "type": "multiple_choice_single_answer",
    "question": "You are creating a machine learning model. You have a dataset that contains null rows. You need to use the Clean Missing Data module in Azure Machine Learning Studio to identify and resolve the null and missing data in the dataset. Which parameter should you use?",
    "options": [
      {
        "id": "A",
        "text": "Replace with mean",
        "is_correct": false,
        "explanation": "Replacing with the mean is used to fill null values in numeric columns, not entire null rows."
      },
      {
        "id": "B",
        "text": "Replace with mode",
        "is_correct": false,
        "explanation": "Mode substitution is appropriate for categorical variables, not for addressing entire null rows."
      },
      {
        "id": "C",
        "text": "Custom substitution value",
        "is_correct": false,
        "explanation": "Custom values can be used to fill specific missing entries, not for removing null rows entirely."
      },
      {
        "id": "D",
        "text": "Remove entire column",
        "is_correct": false,
        "explanation": "This removes columns, not rows. It’s not applicable when rows are the issue."
      },
      {
        "id": "E",
        "text": "Remove entire row",
        "is_correct": true,
        "explanation": "This is the correct option when rows contain null values. The Clean Missing Data module can remove rows that have missing data."
      },
      {
        "id": "F",
        "text": "Hot Deck",
        "is_correct": false,
        "explanation": "Hot Deck is not a standard option in Azure ML Studio's Clean Missing Data module."
      }
    ],
    "feedback": "When your dataset has entire rows with null values, the appropriate way to clean the data using the Clean Missing Data module is to remove those rows. This avoids introducing bias from imputation and ensures only complete records are used for training."
  },
  {
    "question_id": "87",
    "type": "multiple_choice_single_answer",
    "question": "You are junior data scientist of your company. You are building a machine learning model for translating English language textual content into French language textual content. You need to build and train the machine learning model to learn the sequence of the textual content. Which type of neural network should you use?",
    "options": [
      {
        "id": "A",
        "text": "Multilayer Perceptions (MLPs)",
        "is_correct": false,
        "explanation": "MLPs are feed-forward networks and do not handle sequential data well since they lack memory of previous inputs."
      },
      {
        "id": "B",
        "text": "Convolutional Neural Networks (CNNs)",
        "is_correct": false,
        "explanation": "CNNs are effective for image processing tasks but are not suited for modeling sequential dependencies in text."
      },
      {
        "id": "C",
        "text": "Recurrent Neural Networks (RNNs)",
        "is_correct": true,
        "explanation": "RNNs are designed to handle sequential data like text, making them well-suited for translation tasks where order and context matter."
      },
      {
        "id": "D",
        "text": "Generative Adversarial Networks (GANs)",
        "is_correct": false,
        "explanation": "GANs are typically used for generating realistic synthetic data, like images, and are not ideal for sequence-to-sequence tasks such as translation."
      }
    ],
    "feedback": "RNNs are ideal for natural language processing tasks that require learning from sequential data such as text translation. They maintain a memory of previous tokens while processing a sequence, allowing them to capture linguistic patterns. While newer architectures like Transformers outperform RNNs in modern NLP, the question focuses on identifying traditional approaches, and RNNs remain a correct conceptual choice."
  },
  {
    "question_id": "88",
    "type": "multiple_choice_single_answer",
    "question": "You are performing feature engineering on a dataset. You must add a feature named CityName and populate the column value with the text London. You need to add the new feature to the dataset. Which Azure Machine Learning Studio module should you use?",
    "options": [
      {
        "id": "A",
        "text": "Apply SQL Transformation",
        "is_correct": true,
        "explanation": "SQL Transformation allows you to add custom columns and set static values using SQL syntax. It's the most appropriate module for adding a column with a constant value."
      },
      {
        "id": "B",
        "text": "Extract N-Gram Features from Text",
        "is_correct": false,
        "explanation": "This module is used to extract token combinations (n-grams) from text for feature extraction in NLP tasks, not to add or modify dataset structure."
      },
      {
        "id": "C",
        "text": "Preprocess Text",
        "is_correct": false,
        "explanation": "Preprocess Text is for cleaning and preparing existing text columns (e.g., removing stop words or punctuation), not for adding new features to a dataset."
      },
      {
        "id": "D",
        "text": "Edit Metadata",
        "is_correct": false,
        "explanation": "Edit Metadata is used to change the metadata (e.g., data type or column roles) of existing columns, not to add new ones."
      }
    ],
    "feedback": "To add a new feature column with a specific static value in Azure Machine Learning Studio, the Apply SQL Transformation module is the best choice. It enables SQL-like operations including the creation of new columns and assignment of values based on expressions. The other modules are useful for modifying or interpreting existing data but not for structural changes to the dataset."
  },
  {
    "question_id": "89",
    "type": "multiple_choice_single_answer",
    "question": "Identify the correct sentence completion within the context of Microsoft Azure Machine Learning. Compute Instances in Azure ML provide a fully managed and integrated environment for development. These instances include pre-installed tools for code editing and interactive development. Which combination of tools is available out-of-the-box to support real-time experimentation, visualization, and integration with other Azure ML assets?",
    "options": [
      {
        "id": "A",
        "text": "[A] Excel Online, [B] Visual Studio Code Spaces",
        "is_correct": false,
        "explanation": "Excel Online and Visual Studio Code Spaces are not the standard tools pre-installed on Compute Instances in Azure ML environments."
      },
      {
        "id": "B",
        "text": "[A] Azure Data Explorer, [B] Azure Notebooks",
        "is_correct": false,
        "explanation": "Azure Data Explorer and Azure Notebooks are separate services and are not directly included or pre-installed on Compute Instances."
      },
      {
        "id": "C",
        "text": "[A] Jupyter Notebook, [B] JupyterLab",
        "is_correct": true,
        "explanation": "Jupyter Notebook and JupyterLab are both pre-installed on Azure Machine Learning Compute Instances and are the primary tools for interactive experimentation and development."
      },
      {
        "id": "D",
        "text": "[A] Databricks Workspace, [B] Stream Analytics",
        "is_correct": false,
        "explanation": "Databricks and Stream Analytics are separate Azure services and not part of the default Compute Instance setup."
      }
    ],
    "feedback": "Azure ML Compute Instances provide an integrated development environment with pre-installed tools like Jupyter Notebook and JupyterLab. These tools allow data scientists to quickly start experimenting, visualizing data, and developing machine learning workflows with minimal setup."
  },
  {
    "question_id": "90",
    "type": "multiple_choice_single_answer",
    "question": "You are a data scientist creating a linear regression model. You need to determine how closely the data fits the regression line. Which metric should you review?",
    "options": [
      {
        "id": "A",
        "text": "Recall",
        "is_correct": false,
        "explanation": "Recall is used for classification tasks, not regression. It measures the proportion of actual positives correctly identified."
      },
      {
        "id": "B",
        "text": "Root Mean Square Error",
        "is_correct": false,
        "explanation": "RMSE measures the average magnitude of the error but doesn't directly indicate how well the data fits the regression line."
      },
      {
        "id": "C",
        "text": "Mean absolute error",
        "is_correct": false,
        "explanation": "MAE measures average absolute differences between predictions and actual values, but it doesn’t reflect how much of the variance is explained by the model."
      },
      {
        "id": "D",
        "text": "Coefficient of determination",
        "is_correct": true,
        "explanation": "The coefficient of determination (R²) measures how well the model explains the variability of the target variable. It's the most appropriate metric for assessing the goodness-of-fit in regression."
      }
    ],
    "feedback": "The coefficient of determination (R²) is the standard metric for evaluating how well a regression model fits the data. It represents the proportion of variance in the dependent variable that is predictable from the independent variables. Although metrics like RMSE and MAE assess prediction error, they do not capture how well the model explains the underlying data distribution. R² provides this insight, making it the most appropriate choice in this context."
  },  

  {
    "question_id": "91",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure Machine Learning SDK v2 to monitor data drift. You have registered a baseline dataset (dataset1) and a target dataset (dataset2). You want to schedule the drift detection to run every week and track changes in a specific table. Which configuration object should you use to set up this monitoring schedule?",
    "options": [
      {
        "id": "A",
        "text": "DataDriftDetector.run()",
        "is_correct": false,
        "explanation": "This method belongs to SDK v1 and is not applicable in SDK v2."
      },
      {
        "id": "B",
        "text": "ml_client.data_drift.create_schedule()",
        "is_correct": false,
        "explanation": "This method does not exist in the SDK v2 and would raise an error."
      },
      {
        "id": "C",
        "text": "MonitorSchedule()",
        "is_correct": true,
        "explanation": "Correct. In SDK v2, you use MonitorSchedule to define when and how often to trigger drift monitoring."
      },
      {
        "id": "D",
        "text": "DataQualitySignal()",
        "is_correct": false,
        "explanation": "This is used to monitor data quality, not specifically data drift."
      }
    ],
    "feedback": "To monitor data drift in SDK v2, you define a MonitorSchedule object and associate it with a Monitor and a DataDriftSignal configuration. This enables you to periodically compare a baseline dataset to a target dataset. MonitorSchedule defines the frequency of the monitoring job."
  },
  {
    "question_id": "91A",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure Machine Learning SDK v2 to monitor data drift. You have registered a baseline dataset (dataset1) and a target dataset (dataset2). You want to schedule the drift detection to run every week and track changes in a specific table. Which configuration object should you use to set up this monitoring schedule?",
    "options": [
      {
        "id": "A",
        "text": "DataDriftDetector.run()",
        "is_correct": false,
        "explanation": "This method belongs to SDK v1 and is not applicable in SDK v2."
      },
      {
        "id": "B",
        "text": "ml_client.data_drift.create_schedule()",
        "is_correct": false,
        "explanation": "This method does not exist in the SDK v2 and would raise an error."
      },
      {
        "id": "C",
        "text": "MonitorSchedule()",
        "is_correct": true,
        "explanation": "Correct. In SDK v2, you use MonitorSchedule to define when and how often to trigger drift monitoring."
      },
      {
        "id": "D",
        "text": "DataQualitySignal()",
        "is_correct": false,
        "explanation": "This is used to monitor data quality, not specifically data drift."
      }
    ],
    "feedback": "To monitor data drift in SDK v2, you define a MonitorSchedule object and associate it with a Monitor and a DataDriftSignal configuration. This enables you to periodically compare a baseline dataset to a target dataset. MonitorSchedule defines the frequency of the monitoring job."
  },
  {
    "question_id": "92",
    "type": "true_false",
    "question": "True or False: A major difference between clustering and classification models is that clustering is a ‘supervised’ method, where ‘training’ is done with labels.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": "This is incorrect because clustering is an unsupervised learning method, meaning it does not rely on labeled data during training."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": true,
        "explanation": "Correct. Clustering is an unsupervised learning technique where the algorithm groups data without prior labels, unlike classification, which is supervised and requires labeled data."
      }
    ],
    "feedback": "Clustering is an unsupervised method used to group similar data points based on features without requiring labeled outputs. Classification, on the other hand, is supervised and learns from input-output pairs. Therefore, saying clustering is supervised is false."
  },

  {
    "question_id": "93",
    "type": "multiple_choice_single_answer",
    "question": "You are working with Azure Machine Learning SDK v2 and have a workspace already configured using MLClient as `ml_client`. You want to retrieve the default datastore registered in the workspace. Which line of code should you use?",
    "options": [
      {
        "id": "A",
        "text": "default_ds = ml_client.datastores.get('workspaceblobstore')",
        "is_correct": true,
        "explanation": "This is the correct way to access the default datastore in Azure ML SDK v2, assuming the default is named 'workspaceblobstore'."
      },
      {
        "id": "B",
        "text": "default_ds = ml_client.get_default_datastore()",
        "is_correct": false,
        "explanation": "This is incorrect. There is no `get_default_datastore()` method in SDK v2."
      },
      {
        "id": "C",
        "text": "default_ds = ml_client.workspace.datastore",
        "is_correct": false,
        "explanation": "This syntax is invalid in SDK v2 and will raise an attribute error."
      },
      {
        "id": "D",
        "text": "default_ds = Workspace.get_default_datastore()",
        "is_correct": false,
        "explanation": "This method belongs to SDK v1, not SDK v2."
      }
    ],
    "feedback": "In Azure ML SDK v2, datastores are managed using the `ml_client.datastores` interface. The default datastore is usually named 'workspaceblobstore', and you can retrieve it using `ml_client.datastores.get('workspaceblobstore')`. Unlike SDK v1, there is no method like `get_default_datastore()`."
  },
  {
    "question_id": "94",
    "type": "multiple_choice_single_answer",
    "question": "You are creating a model to predict the price of a student’s artwork depending on the following variables: the student’s length of education, degree type, and art form. You start by creating a linear regression model. You need to evaluate the linear regression model. Solution: Use the following metrics: Relative Squared Error, Coefficient of Determination, Accuracy, Precision, Recall, F1 score, and AUC. Does the solution meet the goal?",
    "options": [
      {
        "id": "A",
        "text": "No",
        "is_correct": true,
        "explanation": "Correct. Accuracy, Precision, Recall, F1 score, and AUC are classification metrics, not appropriate for evaluating a linear regression model."
      },
      {
        "id": "B",
        "text": "Yes",
        "is_correct": false,
        "explanation": "Incorrect. The majority of the metrics listed are for classification tasks, not regression. Only RSE and Coefficient of Determination are suitable for regression."
      }
    ],
    "feedback": "To evaluate a linear regression model, you should use metrics designed for regression such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R²). Accuracy, Precision, Recall, F1 score, and AUC are applicable only for classification tasks, so the solution does not meet the goal."
  },
  {
    "question_id": "95",
    "type": "multiple_choice_single_answer",
    "question": "You are using automated machine learning, and you want to determine the influence of features on the predictions made by the best model produced by the automated machine learning experiment. What must you do when configuring the automated machine learning experiment?",
    "options": [
      {
        "id": "A",
        "text": "Enable featurization.",
        "is_correct": false,
        "explanation": "Enabling featurization allows preprocessing of data but does not provide insights into feature influence."
      },
      {
        "id": "B",
        "text": "Enable model explainability.",
        "is_correct": true,
        "explanation": "Correct. To understand how each feature contributes to the model predictions in AutoML, you must enable model explainability."
      },
      {
        "id": "C",
        "text": "Whitelist only tree-based algorithms.",
        "is_correct": false,
        "explanation": "Whitelisting tree-based algorithms may improve interpretability, but it is not required to determine feature influence. Model explainability should be explicitly enabled."
      }
    ],
    "feedback": "To examine feature influence in an Azure AutoML experiment, you must enable **model explainability** during configuration. This activates SHAP-based interpretations that let you understand the contribution of each feature in the best-performing model. Featurization and algorithm selection are separate concerns and not sufficient for generating explanations."
  },
  {
    "question_id": "96",
    "type": "multiple_choice_single_answer",
    "question": "You have a comma-separated values (CSV) file containing data from which you want to train a classification model. You are using the Automated Machine Learning interface in Azure Machine Learning studio to train the classification model. You set the task type to Classification. You need to ensure that the Automated Machine Learning process evaluates only linear models. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Add all algorithms other than linear ones to the blocked algorithms list.",
        "is_correct": true,
        "explanation": "Correct. Blocking all non-linear algorithms forces AutoML to evaluate only linear models during the experiment."
      },
      {
        "id": "B",
        "text": "Clear the option to enable deep learning.",
        "is_correct": false,
        "explanation": "This disables deep learning models, but does not restrict tree-based or other non-linear algorithms."
      },
      {
        "id": "C",
        "text": "Set the Exit criterion option to a metric score threshold.",
        "is_correct": false,
        "explanation": "This controls when the experiment stops but does not restrict algorithm selection."
      },
      {
        "id": "D",
        "text": "Set the task type to Regression.",
        "is_correct": false,
        "explanation": "This changes the task type, which is not the goal. The task type should remain Classification."
      },
      {
        "id": "E",
        "text": "Clear the option to perform automatic featurization.",
        "is_correct": false,
        "explanation": "Featurization impacts preprocessing but not the choice of algorithms evaluated."
      }
    ],
    "feedback": "To ensure AutoML evaluates only linear models, you must explicitly block all non-linear algorithms in the configuration. Disabling deep learning or featurization does not guarantee the exclusion of non-linear methods like tree-based models."
  },
  {
    "question_id": "97",
    "type": "multiple_choice_single_answer",
    "question": "Which of the following is best described by: 'A step-by-step approach to predicting a variable. For example, this may be first split between Spring/Summer and Autumn/Winter to make a prediction based on the day of the week. Spring/Summer-Monday may have a sales rate of 100 units per day, while Autumn/Winter-Monday may have a sales rate of 20 units per day'?",
    "options": [
      {
        "id": "A",
        "text": "Decision Trees",
        "is_correct": true,
        "explanation": "Decision Trees work by recursively splitting the data based on feature values, making them an intuitive, step-by-step prediction method."
      },
      {
        "id": "B",
        "text": "Linear Regression",
        "is_correct": false,
        "explanation": "Linear regression assumes a linear relationship between variables and does not involve recursive partitioning."
      },
      {
        "id": "C",
        "text": "Ensemble Algorithms",
        "is_correct": false,
        "explanation": "While ensemble methods often use decision trees, the question focuses on a single step-by-step decision-making approach."
      },
      {
        "id": "D",
        "text": "Dynamic Programming Algorithms",
        "is_correct": false,
        "explanation": "Dynamic programming is used in optimization and operations research, not for step-by-step predictive modeling."
      }
    ],
    "feedback": "The scenario describes how a model splits data based on conditions to make predictions, which is the core mechanism behind Decision Trees. These models are ideal for capturing non-linear relationships in a simple, interpretable manner."
  },
  {
    "question_id": "98",
    "type": "multiple_choice_single_answer",
    "question": "You need to run a script that trains a deep neural network (DNN) model and logs the loss and accuracy metrics. You decide to run the training script as an experiment on the aks-cluster compute target. Will the solution work properly?",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": "AKS is designed for model inference, not for training. Training jobs should use a compute cluster created for training purposes (Azure ML Compute)."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": true,
        "explanation": "Training cannot be executed on AKS. Instead, use an Azure ML Compute cluster configured for training workloads, such as NC-series VMs."
      }
    ],
    "feedback": "AKS (Azure Kubernetes Service) is a compute target designed for deploying models and serving real-time predictions. It does not support the execution of training jobs. To train a DNN model, you should use an Azure ML Compute cluster (like NC-series VMs with GPU support) created explicitly for training workloads."
  },
  {
    "question_id": "99",
    "type": "multiple_choice_single_answer",
    "question": "You are using the Azure ML SDK v2 to define a batch inference pipeline. You configure a command job that performs inference over a dataset using a compute cluster. The output is defined as an MLTable using Output(type='uri_folder'). After job execution, where can you find the inference results?",
    "options": [
      {
        "id": "A",
        "text": "In the job's Outputs tab in Azure ML Studio under the named output folder",
        "is_correct": true,
        "feedback": "Correct. In SDK v2, batch inference jobs save their outputs to a specified URI folder. These results are visible in the Azure ML Studio under the Outputs tab of the executed job."
      },
      {
        "id": "B",
        "text": "In the Logs tab of the compute target",
        "is_correct": false,
        "feedback": "Incorrect. Logs are stored separately from outputs and are used primarily for debugging purposes."
      },
      {
        "id": "C",
        "text": "In the Environment configuration JSON file",
        "is_correct": false,
        "feedback": "Incorrect. The environment definition does not contain job output results."
      },
      {
        "id": "D",
        "text": "In the Compute clusters dashboard under 'Data outputs'",
        "is_correct": false,
        "feedback": "Incorrect. Outputs are associated with the job, not directly with the compute cluster dashboard."
      }
    ]
  },
  {
    "question_id": "100",
    "type": "multiple_choice_single_answer",
    "question": "Your team is building a data engineering and data science development environment. The environment must support the following requirements:\n- support Python and Scala\n- compose data storage, movement, and processing services into automated data pipelines\n- the same tool should be used for the orchestration of both data engineering and data science\n- support workload isolation and interactive workloads\n- enable scaling across a cluster of machines\nYou need to create the environment. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Build the environment in Azure Databricks and use Azure Data Factory for orchestration.",
        "is_correct": true,
        "feedback": "Correct. Azure Databricks is ideal for scalable data engineering and data science workloads with Python and Scala. Azure Data Factory supports orchestration of data workflows, enabling the creation of automated pipelines that integrate with Databricks clusters."
      },
      {
        "id": "B",
        "text": "Build the environment in Apache Hive for HDInsight and use Azure Data Factory for orchestration.",
        "is_correct": false,
        "feedback": "Incorrect. Apache Hive is focused on SQL-based querying of large datasets and is not ideal for the interactive and programming-intensive workflows typical in data science."
      },
      {
        "id": "C",
        "text": "Build the environment in Apache Spark for HDInsight and use Azure Container Instances for orchestration.",
        "is_correct": false,
        "feedback": "Incorrect. While Apache Spark supports big data processing, Azure Container Instances are not a suitable tool for orchestrating complex data pipelines compared to Azure Data Factory."
      },
      {
        "id": "D",
        "text": "Build the environment in Azure Databricks and use Azure Container Instances for orchestration.",
        "is_correct": false,
        "feedback": "Incorrect. Azure Databricks is appropriate, but Azure Container Instances do not offer orchestration capabilities like scheduling, dependency management, or integration with multiple data services."
      }
    ]
  },
  {
    "question_id": "101",
    "type": "multiple_choice_single_answer",
    "question": "The Daily Bugle is a news organization led by J. Jonah Jameson. One of the current projects is creating a new experiment in Azure Machine Learning Studio. Given: One class has a much smaller number of observations than the other classes in the training set. Required: Select an appropriate data sampling strategy to compensate for the class imbalance. The lead developer, Peter Parker, has used the Stratified split for the sampling mode. Will Peter’s solution meet the goal?",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "feedback": "Incorrect. Stratified split ensures that each split has the same distribution of target classes as the original dataset, but it does not address class imbalance directly. It is not a data sampling strategy such as oversampling or undersampling, which are better suited to tackle class imbalance."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": true,
        "feedback": "Correct. While stratified sampling preserves the class distribution, it does not mitigate class imbalance. To meet the goal, Peter should consider using oversampling (e.g., SMOTE) or undersampling techniques specifically designed to balance class representation in the training data."
      }
    ]
  },
  {
    "question_id": "102",
    "type": "multiple_choice_single_answer",
    "question": "Roxxon Energy Corporation is deploying a model as a real-time inferencing service. What functions must the entry script for the service include?",
    "options": [
      {
        "id": "A",
        "text": "Init() and run()",
        "is_correct": true,
        "feedback": "Correct. In Azure Machine Learning, the entry script for a real-time inference service must implement two specific functions: `init()` to initialize the model and `run()` to handle incoming prediction requests. This structure ensures the model is properly loaded and used during inference."
      },
      {
        "id": "B",
        "text": "Base() and train()",
        "is_correct": false,
        "feedback": "Incorrect. These functions are not part of the standard interface for inference entry scripts in Azure ML. `train()` is used during training, not inference."
      },
      {
        "id": "C",
        "text": "Run() and score()",
        "is_correct": false,
        "feedback": "Incorrect. Only `init()` and `run()` are required for inference in Azure ML. `score()` is not used in this context."
      },
      {
        "id": "D",
        "text": "Main() and score()",
        "is_correct": false,
        "feedback": "Incorrect. `main()` is not used for inference entry scripts in Azure ML. The correct functions are `init()` and `run()`."
      }
    ]
  },
  {
    "question_id": "103",
    "type": "multiple_choice_single_answer",
    "question": "When hyperparameter tuning, what is true about Bayesian sampling?",
    "options": [
      {
        "id": "A",
        "text": "You can only use it with uniform, choice, and quniform parameter expressions",
        "is_correct": true,
        "explanation": "Bayesian sampling in Azure ML SDK v2 supports parameter expressions like Uniform, Choice, and QUniform, as it builds a probabilistic model to guide the search. These types of parameter spaces are required for the algorithm to infer and propose new configurations effectively."
      },
      {
        "id": "B",
        "text": "It's the slowest hyperparameter tuning process",
        "is_correct": false,
        "explanation": "This is not necessarily true. Bayesian sampling can converge faster than random or grid search because it uses previous trials to inform the next suggestions."
      },
      {
        "id": "C",
        "text": "It always finds the best tuned model, albeit being the slowest",
        "is_correct": false,
        "explanation": "While Bayesian methods are effective, there is no guarantee they will always find the best model. Also, it's not always the slowest."
      },
      {
        "id": "D",
        "text": "You can combine it with an early termination policy",
        "is_correct": false,
        "explanation": "Bayesian sampling is typically not used with early termination policies in Azure ML SDK v2, as it requires full evaluations of trials to update the model for proposing new configurations."
      }
    ],
    "feedback": "Bayesian sampling is a popular method for hyperparameter optimization that builds a surrogate model to select promising configurations. In Azure Machine Learning SDK v2, it supports only specific types of parameter expressions such as Uniform, Choice, and QUniform. This ensures the sampling algorithm can work with a meaningful probability distribution over the search space.",
    "correct_answer": "A"
  },
  {
    "question_id": "104",
    "type": "multiple_choice_single_answer",
    "question": "What is not a valid setting you can toggle when creating a model using Automated ML in Azure Machine Learning?",
    "options": [
      {
        "id": "A",
        "text": "Restrict to only using a portion of the data",
        "is_correct": true,
        "explanation": "While you can select a training-validation split, there is no explicit toggle in AutoML to restrict training to only a subset of the data arbitrarily. Data sampling must be handled before launching AutoML."
      },
      {
        "id": "B",
        "text": "Setting the exit criterion at 1 hour time elapsed",
        "is_correct": false,
        "explanation": "Azure AutoML allows you to set the exit criteria based on time elapsed, such as stopping after 1 hour."
      },
      {
        "id": "C",
        "text": "Setting the primary metric to R2",
        "is_correct": false,
        "explanation": "You can specify R2 as the primary metric when the task is regression."
      },
      {
        "id": "D",
        "text": "Restricting all algorithms except for Gradient Boost",
        "is_correct": false,
        "explanation": "Azure AutoML lets you whitelist or blacklist specific algorithms, including using only Gradient Boost if desired."
      }
    ],
    "correct_answer": "A",
    "feedback": "AutoML in Azure Machine Learning supports many configuration options, including setting primary metrics, defining allowed algorithms, and setting exit criteria. However, restricting to an arbitrary portion of the data isn't a supported toggle; data limitations must be pre-processed manually."
  },
  {
    "question_id": "105",
    "type": "multiple_choice_single_answer",
    "question": "What is true about Compute clusters?",
    "options": [
      {
        "id": "A",
        "text": "Compute clusters can only have CPUs",
        "is_correct": false,
        "explanation": "Compute clusters can be configured with both CPUs and GPUs, depending on your workload requirements."
      },
      {
        "id": "B",
        "text": "Compute clusters scale the number of nodes automatically",
        "is_correct": true,
        "explanation": "Azure Machine Learning compute clusters can automatically scale up or down depending on the workload demand, which optimizes cost and performance."
      },
      {
        "id": "C",
        "text": "Compute clusters have more downtime than Compute instances",
        "is_correct": false,
        "explanation": "Compute clusters do not inherently have more downtime; their uptime depends on configuration and Azure availability."
      },
      {
        "id": "D",
        "text": "Compute clusters can only have 1 node",
        "is_correct": false,
        "explanation": "One of the key benefits of compute clusters is the ability to scale out to multiple nodes, not being limited to a single node."
      }
    ],
    "correct_answer": "B",
    "feedback": "Compute clusters in Azure ML are designed to support scalability by automatically adjusting the number of nodes based on current job requirements. This makes them ideal for running scalable training jobs efficiently."
  },
  {
    "question_id": "106",
    "type": "multiple_choice_single_answer",
    "question": "In an SKlearn LinearRegression model, what is the difference between 'predict' and 'score' methods?",
    "options": [
      {
        "id": "A",
        "text": ".predict only takes the X frame, whereas .score takes an X and Y frame",
        "is_correct": true,
        "explanation": ".predict() is used to generate predictions from the input features (X) only, while .score() requires both the features X and true labels Y to compute the R² score."
      },
      {
        "id": "B",
        "text": ".predict takes an X and Y frame, and .score only takes the X frame",
        "is_correct": false,
        "explanation": ".predict() does not use the labels (Y); it only requires input features (X). This option reverses the correct usage."
      },
      {
        "id": "C",
        "text": ".predict predicts the accuracy of the validation set, where .score provides a model scoring metric (R2)",
        "is_correct": false,
        "explanation": ".predict() generates predicted values; it does not measure accuracy or R². That is the job of .score()."
      },
      {
        "id": "D",
        "text": ".predict predicts the label whereas .score calculates the MSE",
        "is_correct": false,
        "explanation": ".score() for LinearRegression in scikit-learn returns the R² score, not the mean squared error."
      }
    ],
    "answer": "A",
    "feedback": "The correct distinction is that `.predict(X)` is used only for generating predictions from input features, while `.score(X, y)` evaluates the model’s performance using both inputs and ground truth labels by computing the R² metric. Many candidates confuse `.score()` with metrics like MSE, but in LinearRegression it returns R² by default."
  },
  {
    "question_id": "107",
    "type": "multiple_choice_single_answer",
    "question": "You have a Data Asset in Python SDK v2 called 'pricehistory'. You load the data from the asset, modify it with Pandas, and then register the modified version using the MLClient instance:\n\n```python\nml_client.data.create_or_update(\n    Data(name='pricehistory', version='2', path='./updated_data', type=AssetTypes.URI_FOLDER)\n)\n```\nWhat will be the result of running this code?",
    "options": [
      {
        "id": "A",
        "text": "The Data Asset 'pricehistory' is updated with the new contents, and version 2 is created",
        "is_correct": true,
        "explanation": "When a Data Asset with a new version is registered using `create_or_update`, Azure ML registers it as a new version under the same name."
      },
      {
        "id": "B",
        "text": "The original 'pricehistory' asset is overwritten with the new content, keeping the same version",
        "is_correct": false,
        "explanation": "Existing versions cannot be overwritten in SDK v2; a new version must be specified."
      },
      {
        "id": "C",
        "text": "An error will occur because the asset name 'pricehistory' already exists",
        "is_correct": false,
        "explanation": "The asset name can be reused as long as the version is unique. No error is raised."
      },
      {
        "id": "D",
        "text": "A new Data Asset is created with a different name since versioning is not supported in SDK v2",
        "is_correct": false,
        "explanation": "Versioning is supported in SDK v2 using the `version` field."
      }
    ],
    "answer": "A",
    "feedback": "In Azure Machine Learning SDK v2, you can manage Data Assets with explicit versioning. Using `ml_client.data.create_or_update()` with the same asset name but a new version creates a new version without affecting the previous ones. This supports reproducibility and traceability of data versions.\n\nReference: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-data-assets?view=azureml-api-2"
  },
  {
    "question_id": "108",
    "type": "multiple_choice_single_answer",
    "question": "Which type of deployment enables a user to submit multiple observations (X rows) and then receive multiple predictions (Y values)?",
    "options": [
      {
        "id": "A",
        "text": "Batch only",
        "is_correct": false,
        "explanation": "Batch endpoints support processing multiple inputs at once, but this capability is not exclusive to batch deployments."
      },
      {
        "id": "B",
        "text": "Real-time only",
        "is_correct": false,
        "explanation": "Real-time endpoints also support multiple inputs, especially when using MLflow or custom deployments, but this is not the only option."
      },
      {
        "id": "C",
        "text": "Neither",
        "is_correct": false,
        "explanation": "Both batch and real-time deployments can handle multiple rows of input; this option is incorrect."
      },
      {
        "id": "D",
        "text": "Real-time and Batch",
        "is_correct": true,
        "explanation": "Both real-time endpoints and batch endpoints in Azure Machine Learning can process multiple observations and return multiple outputs. Real-time endpoints are optimized for low-latency scoring of online requests (e.g., REST), while batch endpoints are used for asynchronous, high-volume scoring tasks across large datasets."
      }
    ],
    "answer": "D",
    "feedback": "In Azure ML, real-time endpoints can be configured to handle multiple records in a single request, particularly if the deployed model supports batch-style inputs (like arrays or DataFrames). Batch endpoints, on the other hand, are explicitly designed for large-scale inference over datasets stored in blob storage or folders. Both methods support input/output of multiple rows, depending on model and implementation. For more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where"
  },
  {
    "question_id": "109",
    "type": "multiple_choice_single_answer",
    "question": "What is the purpose of a differential privacy solution?",
    "options": [
      {
        "id": "A",
        "text": "Protect individual data by adding non-statistical noise",
        "is_correct": false,
        "explanation": "This is incorrect. Differential privacy works by adding carefully calibrated statistical noise to data."
      },
      {
        "id": "B",
        "text": "Protect individual data by adding data access controls",
        "is_correct": false,
        "explanation": "Access controls restrict who can view data, but this is not the core mechanism of differential privacy."
      },
      {
        "id": "C",
        "text": "Protect individual data by adding statistical noise",
        "is_correct": true,
        "explanation": "Correct. Differential privacy protects individual data by injecting controlled, statistical noise into the dataset or the query results to prevent the identification of individuals while preserving overall patterns."
      },
      {
        "id": "D",
        "text": "Pritect individual data by adding privacy access regulations",
        "is_correct": false,
        "explanation": "This option is unrelated to how differential privacy works. Regulations are external legal frameworks, not algorithmic techniques."
      }
    ],
    "answer": "C",
    "feedback": "Differential privacy is a mathematical framework for quantifying and limiting the privacy risks associated with the release of statistical data. It works by adding statistical noise—usually Laplacian or Gaussian—to query results, so that the presence or absence of any single individual in the data has a minimal impact on the output. This approach is essential for scenarios like federated learning or public data release in Azure's Responsible AI ecosystem. More info: https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai"
  },
  {
    "question_id": "110",
    "type": "multiple_choice_single_answer",
    "question": "You are working locally and need to connect to an Azure Machine Learning workspace using the Azure ML Python SDK v2. What is required to establish the connection using DefaultAzureCredential?",
    "options": [
      {
        "id": "A",
        "text": "Create a config.json file with subscription, resource group, and workspace name, and load it using MLClient.",
        "is_correct": true,
        "explanation": "The SDK v2 uses `MLClient` from `azure.ai.ml` to connect to a workspace, typically by loading details from a config.json file in combination with `DefaultAzureCredential` for authentication."
      },
      {
        "id": "B",
        "text": "Use the Workspace.from_config() method to initialize the workspace from the config file.",
        "is_correct": false,
        "explanation": "This method is specific to SDK v1 and not available in SDK v2. SDK v2 requires MLClient and Azure Identity credentials."
      },
      {
        "id": "C",
        "text": "Authenticate using interactive login before calling MLWorkspaceClient.connect().",
        "is_correct": false,
        "explanation": "There is no `MLWorkspaceClient.connect()` method in SDK v2. The correct class is `MLClient` and it uses `DefaultAzureCredential` or similar credential objects."
      },
      {
        "id": "D",
        "text": "Use a managed identity assigned to your compute instance to connect without configuration.",
        "is_correct": false,
        "explanation": "While managed identity is supported in cloud contexts, a local connection still typically requires a `config.json` file unless the full context is injected via environment variables or interactive login."
      }
    ],
    "feedback": "To connect to a workspace using Azure ML SDK v2 locally, you need to create a `config.json` file with workspace metadata (subscription ID, resource group, and workspace name). Then, use the `MLClient` class from `azure.ai.ml` along with `DefaultAzureCredential` from `azure.identity`. Example:\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\ncred = DefaultAzureCredential()\nml_client = MLClient.from_config(credential=cred)\n```\nMore info: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment?tabs=python"
  },
  {
    "question_id": "111",
    "type": "multiple_choice_single_answer",
    "question": "What service is not included in, or deployed with, Azure Machine Learning?",
    "options": [
      {
        "id": "A",
        "text": "Storage Accounts",
        "is_correct": false,
        "explanation": "Storage Accounts are essential for Azure ML to store data, models, logs, and intermediate outputs."
      },
      {
        "id": "B",
        "text": "Key Vault",
        "is_correct": false,
        "explanation": "Azure ML uses Key Vault to securely store secrets such as connection strings and credentials."
      },
      {
        "id": "C",
        "text": "App Service",
        "is_correct": true,
        "explanation": "Azure App Service is not part of Azure Machine Learning. It's designed for hosting web apps, not for managing ML workflows or models."
      },
      {
        "id": "D",
        "text": "Container Registry",
        "is_correct": false,
        "explanation": "Azure Container Registry is used by Azure ML to store Docker images for environments and deployments."
      }
    ],
    "feedback": "Azure Machine Learning integrates and automatically provisions several Azure resources such as Storage Accounts, Azure Key Vault, and Container Registry. However, Azure App Service is not part of the Azure ML ecosystem and is not used in ML workflows or deployments. App Service is intended for web app hosting, not for managing ML assets or compute."
  },
  {
    "question_id": "112",
    "type": "multiple_choice_single_answer",
    "question": "What Compute would be best suited for training a Computer Vision model?",
    "options": [
      {
        "id": "A",
        "text": "Compute instance with high RAM, CPU, 2 cores",
        "is_correct": false,
        "explanation": "This option provides sufficient memory but lacks the GPU needed for efficient image processing tasks in computer vision."
      },
      {
        "id": "B",
        "text": "Compute cluster with CPU, medium RAM, and 16 nodes",
        "is_correct": true,
        "explanation": "Although it allows for parallel processing, using only CPUs makes this setup suboptimal for training deep learning-based computer vision models."
      },
      {
        "id": "C",
        "text": "Compute cluster with GPU, medium RAM, and 16 nodes",
        "is_correct": false,
        "explanation": "Despite being the most appropriate option for computer vision tasks due to GPU availability, it was marked incorrect, likely due to a mistake in the answer key."
      },
      {
        "id": "D",
        "text": "Compute instance with low RAM, CPU, 2 cores",
        "is_correct": false,
        "explanation": "This answer is technically incorrect for training computer vision models, which benefit from GPUs. The answer key appears to be flawed."
      }
    ],
    "feedback": "Computer vision models, especially deep learning ones like CNNs, require significant computational resources. A compute cluster with GPUs is the most appropriate choice for such tasks, as GPUs handle matrix operations and backpropagation much faster than CPUs. The selected correct answer is likely an error in the exam system and should be carefully re-evaluated when studying."
  },
  {
    "question_id": "113",
    "type": "multiple_choice_single_answer",
    "question": "What is the difference between data assets and data stores?",
    "options": [
      {
        "id": "A",
        "text": "Data stores are abstracted locations for data, whereas data assets are data processing power that is used to model",
        "is_correct": false,
        "explanation": "This is incorrect. Data assets represent the actual data, not compute resources."
      },
      {
        "id": "B",
        "text": "Data stores are abstracted locations for data, whereas data assets are abstracted tables / files of data",
        "is_correct": true,
        "explanation": "Correct. A data store refers to the underlying storage system (like Azure Blob Storage or ADLS), while a data asset in Azure ML represents a specific dataset or file within that store, versioned and used for training or inference."
      },
      {
        "id": "C",
        "text": "Data stores are tools used to consume data from the public (using Azure Data Store), whereas data assets are abstracted tables / files of data",
        "is_correct": false,
        "explanation": "Data stores are not limited to public data and are more general in function than stated here."
      },
      {
        "id": "D",
        "text": "Data stores used to consume data from the public (using Azure Data Store), whereas data assets are data processing power that is used to model",
        "is_correct": false,
        "explanation": "Incorrect on both accounts. Data assets are not compute resources, and data stores are not limited to public data."
      }
    ],
    "feedback": "In Azure Machine Learning, data stores are connection configurations to storage locations (e.g., Azure Blob, ADLS). Data assets are references to datasets or files stored in those data stores, and they are versioned entities used in training and pipeline steps. Knowing the distinction is essential for data management and reproducibility in ML workflows."
  },
  {
    "question_id": "114",
    "type": "multiple_choice_single_answer",
    "question": "What language does Azure Data Explorer use?",
    "options": [
      {
        "id": "A",
        "text": "Python (Python SDK Library)",
        "is_correct": false,
        "explanation": "Python is a client language supported via SDKs, but it is not the query language used by Azure Data Explorer."
      },
      {
        "id": "B",
        "text": "Query Language",
        "is_correct": false,
        "explanation": "This is too generic to be correct. Azure Data Explorer uses a specific query language."
      },
      {
        "id": "C",
        "text": "Kusto Query Language",
        "is_correct": true,
        "explanation": "Correct. Azure Data Explorer uses Kusto Query Language (KQL) for querying large volumes of structured, semi-structured, and unstructured data efficiently."
      },
      {
        "id": "D",
        "text": "Structured Query Language",
        "is_correct": false,
        "explanation": "SQL is not the native language used by Azure Data Explorer. KQL is specifically designed for that platform."
      }
    ],
    "feedback": "Azure Data Explorer uses Kusto Query Language (KQL), a read-only request language used to process and analyze large volumes of data. KQL is optimized for speed and scale in telemetry and log analytics scenarios and is distinct from SQL or other traditional query languages."
  },
  {
    "question_id": "115",
    "type": "multiple_choice_single_answer",
    "question": "You have created a real-time inference model and deployed it to an endpoint. You want to test it using the REST API. Which of the following is true?",
    "options": [
      {
        "id": "A",
        "text": "You can only pass multiple records at a time and the data does not need to be in JSON format",
        "is_correct": false,
        "explanation": "Incorrect. Data must be passed in JSON format for REST API inference calls."
      },
      {
        "id": "B",
        "text": "You can only pass one record at a time and the data needs to be in JSON format",
        "is_correct": false,
        "explanation": "This is incorrect. Real-time endpoints can accept multiple records if formatted correctly in a JSON array."
      },
      {
        "id": "C",
        "text": "You can only pass multiple records at a time and the data needs to be in JSON format",
        "is_correct": true,
        "explanation": "Correct. Azure ML real-time endpoints accept JSON-formatted input, and can handle multiple records in a single request using an array format."
      },
      {
        "id": "D",
        "text": "You can only pass one record at a time and the data does not need to be in JSON format",
        "is_correct": false,
        "explanation": "Incorrect. JSON format is required for REST API requests to Azure ML real-time endpoints."
      }
    ],
    "feedback": "When sending data to a deployed Azure Machine Learning real-time endpoint using the REST API, the data must be in JSON format. The endpoint supports batching of requests, meaning that multiple records can be submitted at once as a JSON array. This improves performance and reduces latency for batch inference operations."
  },
  {
    "question_id": "116",
    "type": "multiple_choice_single_answer",
    "question": "You are creating a data pipeline in Azure Machine Learning Studio Designer, and you want to take the Data Asset and remove any outliers for a particular column. What is the best component to do that?",
    "options": [
      {
        "id": "A",
        "text": "Normalize Values",
        "is_correct": false,
        "explanation": "Normalize Values scales the data to a specific range but does not remove or adjust outliers explicitly."
      },
      {
        "id": "B",
        "text": "Clean Missing Data",
        "is_correct": false,
        "explanation": "Clean Missing Data is used for handling nulls or missing entries but is not designed for removing outliers."
      },
      {
        "id": "C",
        "text": "Clip Values",
        "is_correct": true,
        "explanation": "Clip Values is the correct choice for outlier handling. It limits the data to a specified range, effectively removing or adjusting outliers."
      },
      {
        "id": "D",
        "text": "Normalize Data",
        "is_correct": false,
        "explanation": "Normalize Data helps with standardizing data distribution but does not address outliers directly."
      }
    ],
    "feedback": "The 'Clip Values' component in Azure ML Studio Designer is specifically designed to handle outliers by setting upper and lower bounds for a column. This is ideal when you want to remove or adjust extreme values that can skew model performance. Other options like normalization or cleaning missing data do not address outliers directly."
  },
  {
    "question_id": "115",
    "type": "drag_and_drop_ordering",
    "question": "You are planning to host practical training to acquaint staff with Docker for Windows. Staff devices must support the installation of Docker. Which of the following are requirements for this installation? Arrange the correct requirements in order.",
    "options": [
      {
        "id": "1",
        "text": "2 GB of system RAM"
      },
      {
        "id": "2",
        "text": "4 GB of system RAM"
      },
      {
        "id": "3",
        "text": "BIOS-enabled virtualization"
      },
      {
        "id": "4",
        "text": "Microsoft Hardware-Assisted Virtualization Detection Tool"
      },
      {
        "id": "5",
        "text": "Windows 10 64-bit"
      },
      {
        "id": "6",
        "text": "Windows 10 32-bit"
      }
    ],
    "correct_order": ["2", "3", "5"],
    "feedback": "To install Docker Desktop on Windows, the system must meet the following requirements: at least 4 GB of RAM (not 2 GB), virtualization enabled in BIOS, and Windows 10 64-bit. Options like Windows 10 32-bit or diagnostic tools like the Virtualization Detection Tool are not direct installation requirements."
  },
  {
    "question_id": "116",
    "type": "drag_and_drop_ordering",
    "question": "You have been tasked with moving data into Azure Blob Storage for the purpose of supporting Azure Machine Learning. Which of the following can be used to complete your task? Arrange the options that are valid in the correct order.",
    "options": [
      {
        "id": "1",
        "text": "AzCopy"
      },
      {
        "id": "2",
        "text": "Bulk Copy Program (BCP)"
      },
      {
        "id": "3",
        "text": "SSIS"
      },
      {
        "id": "4",
        "text": "Bulk Insert SQL Query"
      },
      {
        "id": "5",
        "text": "Azure Storage Explorer"
      }
    ],
    "correct_order": ["1", "3", "5"],
    "feedback": "To move data into Azure Blob Storage, AzCopy is a command-line tool designed specifically for transferring blobs. SSIS is an ETL tool that supports blob targets, and Azure Storage Explorer provides a GUI to interact with blob containers. BCP and Bulk Insert SQL Query are intended for database-level operations and are not suitable for direct blob storage interaction."
  },  {
    "question_id": "dp100_001",
    "type": "true_false",
    "question": "You need to monitor your training jobs in Azure Machine Learning. You configure 'Diagnostic settings' and select the category 'AmlRunStatusChangedEvent'. Does this solution meet the goal?",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": true,
        "explanation": "This approach enables job status logs to be sent to Log Analytics or other monitoring tools, allowing comprehensive monitoring."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": false,
        "explanation": "Incorrect, because configuring Diagnostic settings with AmlRunStatusChangedEvent is the correct method."
      }
    ],
    "feedback": "The 'AmlRunStatusChangedEvent' enables tracking job execution status via Azure Monitor or Log Analytics.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_002",
    "type": "multiple_choice_single_answer",
    "question": "You want to log metrics, parameters, and tags from an MLflow run. Which MLflow function retrieves this information?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.get_run(run_id)",
        "is_correct": true,
        "explanation": "mlflow.get_run retrieves a Run object that contains metrics, parameters, and tags associated with the run."
      },
      {
        "id": "B",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "log_param logs a single parameter but does not retrieve run metadata."
      },
      {
        "id": "C",
        "text": "mlflow.autolog()",
        "is_correct": false,
        "explanation": "autolog enables automatic logging but does not retrieve existing data."
      },
      {
        "id": "D",
        "text": "mlflow.pyfunc.save_model()",
        "is_correct": false,
        "explanation": "This function saves a model but does not handle metric or parameter logging."
      }
    ],
    "feedback": "mlflow.get_run is used to access metadata including metrics, params, and tags from a run.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_003",
    "type": "multiple_choice_single_answer",
    "question": "You need to log data from your job runs using MLflow. You use the 'Run.log()' Azure ML SDK method. Does this solution meet the goal?",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": "Run.log() is part of the Azure ML SDK, not MLflow. It does not meet the requirement when MLflow is specified."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": true,
        "explanation": "Correct, MLflow has its own API for logging such as mlflow.log_metric, mlflow.log_param, etc."
      }
    ],
    "feedback": "When using MLflow, logging should be done with MLflow API functions like mlflow.log_param, not AzureML SDK's Run.log().",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_004",
    "type": "multiple_choice_single_answer",
    "question": "You need to create a compute instance that supports ML pipeline training in Azure ML Designer v2. You create an Azure Databricks environment. Does this solution meet the goal?",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": "Azure Databricks cannot be used directly as a compute target in Designer v2."
      },
      {
        "id": "B",
        "text": "No",
        "is_correct": true,
        "explanation": "Correct. Designer v2 only supports Azure ML Compute as a compute target for pipeline training."
      }
    ],
    "feedback": "Azure ML Designer v2 only works with AML Compute targets like clusters or instances, not Databricks or HDInsight.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_005",
    "type": "multiple_choice_single_answer",
    "question": "You plan to read Parquet-format data from a folder that is not registered as a dataset using Azure ML SDK v2. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Create a v2 job with Dataset.File.from_files()",
        "is_correct": false,
        "explanation": "Dataset.File.from_files() is from SDK v1 and not compatible with SDK v2 workflows."
      },
      {
        "id": "B",
        "text": "Create a v2 job with AssetTypes.URI_FOLDER and ml_client.jobs.create_or_update()",
        "is_correct": true,
        "explanation": "Correct. URI_FOLDER is the correct type for referencing a folder of data in SDK v2."
      },
      {
        "id": "C",
        "text": "Create a v2 job with AssetTypes.URI_FILE and ml_client.jobs.create_or_update()",
        "is_correct": false,
        "explanation": "URI_FILE is for individual files, not folders. Parquet data often consists of multiple files."
      }
    ],
    "feedback": "Use AssetTypes.URI_FOLDER to work with a directory of Parquet files in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_006",
    "type": "multiple_choice_single_answer",
    "question": "You want to deploy a machine learning model for scalable real-time inference. You need to ensure low latency and autoscaling capabilities. Which deployment target should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Container Instance (ACI)",
        "is_correct": false,
        "explanation": "ACI is lightweight and suitable for testing, but it doesn't offer autoscaling for production workloads."
      },
      {
        "id": "B",
        "text": "Azure Kubernetes Service (AKS)",
        "is_correct": true,
        "explanation": "AKS supports autoscaling and is recommended for real-time inference at scale."
      },
      {
        "id": "C",
        "text": "Azure ML Compute Cluster",
        "is_correct": false,
        "explanation": "Compute clusters are mainly used for training, not for inference endpoints."
      },
      {
        "id": "D",
        "text": "Azure Batch",
        "is_correct": false,
        "explanation": "Azure Batch is better suited for batch inference and parallel processing."
      }
    ],
    "feedback": "AKS is the best choice for deploying scalable, real-time inference endpoints in Azure ML.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_007",
    "type": "multiple_choice_single_answer",
    "question": "You are configuring a pipeline in Azure ML SDK v2. You want to use a component that loads data from a registered MLTable asset. What input type should you define?",
    "options": [
      {
        "id": "A",
        "text": "Input(type='uri_file')",
        "is_correct": false,
        "explanation": "uri_file is for single files, not MLTable datasets."
      },
      {
        "id": "B",
        "text": "Input(type='mltable')",
        "is_correct": true,
        "explanation": "This is the correct type to use for MLTable assets in Azure ML SDK v2."
      },
      {
        "id": "C",
        "text": "Input(type='string')",
        "is_correct": false,
        "explanation": "String inputs are for parameters, not datasets."
      },
      {
        "id": "D",
        "text": "Input(type='uri_folder')",
        "is_correct": false,
        "explanation": "While uri_folder works with folders, it's not used for registered MLTable assets."
      }
    ],
    "feedback": "Use `Input(type='mltable')` when passing a registered MLTable asset into a pipeline component.",
    "sdk_version": "v2",
    "include_in_bank": true
  },{
    "question_id": "dp100_008",
    "type": "multiple_choice_single_answer",
    "question": "You are using MLflow with Azure Machine Learning SDK v2. You want to automatically capture parameters, metrics, and models during training. What should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "This function logs a single parameter, but it doesn't enable automatic logging."
      },
      {
        "id": "B",
        "text": "mlflow.autolog()",
        "is_correct": true,
        "explanation": "mlflow.autolog() enables automatic logging of metrics, parameters, and models during training."
      },
      {
        "id": "C",
        "text": "mlflow.start_run()",
        "is_correct": false,
        "explanation": "This starts a run context but does not perform any logging by itself."
      },
      {
        "id": "D",
        "text": "mlflow.register_model()",
        "is_correct": false,
        "explanation": "This function is used to register models, not for logging during training."
      }
    ],
    "feedback": "Use mlflow.autolog() to simplify tracking during model training in Azure ML with SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_009",
    "type": "multiple_choice_single_answer",
    "question": "You are configuring a job with Azure ML CLI v2 and want to pass a folder of data located in a default datastore. Which input type should you use?",
    "options": [
      {
        "id": "A",
        "text": "uri_file",
        "is_correct": false,
        "explanation": "uri_file is intended for single file inputs, not folders."
      },
      {
        "id": "B",
        "text": "mltable",
        "is_correct": false,
        "explanation": "mltable refers to a structured tabular format, not arbitrary folders."
      },
      {
        "id": "C",
        "text": "uri_folder",
        "is_correct": true,
        "explanation": "uri_folder is the correct type for referencing a folder in a datastore or blob."
      },
      {
        "id": "D",
        "text": "string",
        "is_correct": false,
        "explanation": "string is used for scalar inputs like hyperparameters, not for data assets."
      }
    ],
    "feedback": "Use uri_folder when you want to pass a directory as input to a job in Azure ML SDK v2 or CLI v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },

  {
    "question_id": "dp100_010",
    "type": "multiple_choice_single_answer",
    "question": "You have a component in a pipeline that outputs a folder of images. You want to log this output using MLflow in Azure ML SDK v2. Which function should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_image()",
        "is_correct": false,
        "explanation": "This logs a single image, not an entire folder of images."
      },
      {
        "id": "B",
        "text": "mlflow.log_artifact(path)",
        "is_correct": true,
        "explanation": "mlflow.log_artifact logs a local file or directory as an artifact to the current run."
      },
      {
        "id": "C",
        "text": "mlflow.set_tag()",
        "is_correct": false,
        "explanation": "mlflow.set_tag adds metadata but does not log files or directories."
      },
      {
        "id": "D",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "This function logs a single scalar parameter, not files or folders."
      }
    ],
    "feedback": "To log an entire folder (e.g., image outputs), use mlflow.log_artifact with a directory path.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_011",
    "type": "multiple_choice_single_answer",
    "question": "You want to schedule a model retraining job weekly using Azure Machine Learning. Which tool should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Data Factory",
        "is_correct": true,
        "explanation": "Azure Data Factory can trigger Azure ML pipelines on a schedule or based on events like new data."
      },
      {
        "id": "B",
        "text": "Azure DevOps",
        "is_correct": false,
        "explanation": "Azure DevOps can schedule pipelines but is not the standard approach for ML retraining jobs."
      },
      {
        "id": "C",
        "text": "Azure Blob Storage",
        "is_correct": false,
        "explanation": "Blob Storage is a data source, not a tool for scheduling or orchestrating jobs."
      },
      {
        "id": "D",
        "text": "Azure Batch",
        "is_correct": false,
        "explanation": "Azure Batch is for parallel processing jobs, not scheduling ML pipelines."
      }
    ],
    "feedback": "Azure Data Factory is a recommended service for scheduling and orchestrating ML workflows.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_012",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure ML SDK v2 and want to deploy a model to a local endpoint for testing. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.online_endpoints.create_or_update()",
        "is_correct": false,
        "explanation": "This method is used for cloud-based endpoints, not local testing."
      },
      {
        "id": "B",
        "text": "ml_client.batch_endpoints.create_or_update()",
        "is_correct": false,
        "explanation": "This is used for batch inference, not local or real-time testing."
      },
      {
        "id": "C",
        "text": "ml_client.online_endpoints.invoke_local()",
        "is_correct": true,
        "explanation": "This method allows you to test locally without deploying to the cloud."
      },
      {
        "id": "D",
        "text": "ml_client.models.download()",
        "is_correct": false,
        "explanation": "This is used to download a model, not for deploying or testing locally."
      }
    ],
    "feedback": "To test a model locally, use `invoke_local()` on a local endpoint in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_013",
    "type": "multiple_choice_single_answer",
    "question": "You want to track the RGB values of an image transformation during an experiment using MLflow. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_dict()",
        "is_correct": true,
        "explanation": "mlflow.log_dict allows logging a dictionary, such as RGB values, as a JSON artifact."
      },
      {
        "id": "B",
        "text": "mlflow.log_image()",
        "is_correct": false,
        "explanation": "mlflow.log_image is for visual image files, not structured dictionaries."
      },
      {
        "id": "C",
        "text": "mlflow.log_metric()",
        "is_correct": false,
        "explanation": "mlflow.log_metric is used for scalar values like accuracy, not RGB dictionaries."
      },
      {
        "id": "D",
        "text": "mlflow.register_model()",
        "is_correct": false,
        "explanation": "mlflow.register_model is used for registering models, not logging metadata."
      }
    ],
    "feedback": "Use mlflow.log_dict to capture structured data like dictionaries during experiment runs.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_014",
    "type": "multiple_choice_single_answer",
    "question": "You are deploying a model for testing in Azure ML and want a low-cost option that initializes quickly and does not require managing infrastructure. Which compute target should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Kubernetes Service (AKS)",
        "is_correct": false,
        "explanation": "AKS is more suitable for production deployments with autoscaling, but it's more complex and costly to manage."
      },
      {
        "id": "B",
        "text": "Azure Container Instances (ACI)",
        "is_correct": true,
        "explanation": "ACI is designed for quick, cost-effective deployment of models without infrastructure management."
      },
      {
        "id": "C",
        "text": "Azure Batch",
        "is_correct": false,
        "explanation": "Azure Batch is ideal for batch scoring and not suited for real-time, lightweight inference."
      },
      {
        "id": "D",
        "text": "Compute Cluster",
        "is_correct": false,
        "explanation": "Compute clusters are best for training jobs, not real-time model deployment."
      }
    ],
    "feedback": "ACI is the optimal choice for test deployments with low overhead in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_015",
    "type": "multiple_choice_single_answer",
    "question": "You are registering a model with MLflow after training. Which function should you use to create a named, versioned model in the registry?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.register_model()",
        "is_correct": true,
        "explanation": "This function registers a model with a specified name and creates a version in the model registry."
      },
      {
        "id": "B",
        "text": "mlflow.log_model()",
        "is_correct": false,
        "explanation": "This logs the model to the current run but does not add it to the registry."
      },
      {
        "id": "C",
        "text": "mlflow.save_model()",
        "is_correct": false,
        "explanation": "This saves the model locally but does not track it in the model registry."
      },
      {
        "id": "D",
        "text": "mlflow.get_model_version()",
        "is_correct": false,
        "explanation": "This retrieves model version info but does not register a model."
      }
    ],
    "feedback": "To make a model discoverable and versioned in Azure ML's registry, use mlflow.register_model().",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_016",
    "type": "multiple_choice_single_answer",
    "question": "You are monitoring data drift in Azure Machine Learning using the SDK v2. Which component should you configure to define the logic for drift detection?",
    "options": [
      {
        "id": "A",
        "text": "DataDriftSchedule",
        "is_correct": false,
        "explanation": "There is no component named DataDriftSchedule in SDK v2. The correct object is MonitorSchedule."
      },
      {
        "id": "B",
        "text": "MonitorSchedule",
        "is_correct": true,
        "explanation": "MonitorSchedule defines when and how monitoring jobs like data drift detection should run."
      },
      {
        "id": "C",
        "text": "DriftMetricsLogger",
        "is_correct": false,
        "explanation": "DriftMetricsLogger does not exist in Azure ML SDK v2."
      },
      {
        "id": "D",
        "text": "RunConfig",
        "is_correct": false,
        "explanation": "RunConfig is used in SDK v1 for configuring jobs, not for monitoring in SDK v2."
      }
    ],
    "feedback": "Use MonitorSchedule to set up and automate drift detection jobs in Azure Machine Learning SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_017",
    "type": "multiple_choice_single_answer",
    "question": "You want to restrict training in Azure ML to only a specific algorithm, such as 'XGBoostClassifier'. Which property should you configure in the AutoML job?",
    "options": [
      {
        "id": "A",
        "text": "blocked_training_algorithms",
        "is_correct": false,
        "explanation": "This property excludes algorithms but does not explicitly restrict to a single one."
      },
      {
        "id": "B",
        "text": "allowed_training_algorithms",
        "is_correct": true,
        "explanation": "allowed_training_algorithms specifies exactly which algorithms can be used during training."
      },
      {
        "id": "C",
        "text": "primary_metric",
        "is_correct": false,
        "explanation": "This defines how models are evaluated but not which algorithms are used."
      },
      {
        "id": "D",
        "text": "enable_early_termination",
        "is_correct": false,
        "explanation": "This is used to stop underperforming trials early, not to limit algorithm selection."
      }
    ],
    "feedback": "To restrict training to a specific algorithm in AutoML, use the allowed_training_algorithms setting.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_018",
    "type": "multiple_choice_single_answer",
    "question": "You want to save a trained scikit-learn model using MLflow so it can be registered and deployed in Azure ML. Which function should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.sklearn.log_model()",
        "is_correct": true,
        "explanation": "This function logs a scikit-learn model to the current MLflow run so it can be registered and deployed."
      },
      {
        "id": "B",
        "text": "mlflow.register_model()",
        "is_correct": false,
        "explanation": "This function registers a model already logged to MLflow; it does not save the model initially."
      },
      {
        "id": "C",
        "text": "mlflow.pyfunc.log_model()",
        "is_correct": false,
        "explanation": "pyfunc is for generic Python models; for scikit-learn, use the dedicated mlflow.sklearn module."
      },
      {
        "id": "D",
        "text": "mlflow.set_model()",
        "is_correct": false,
        "explanation": "There is no function called set_model in MLflow."
      }
    ],
    "feedback": "Use mlflow.sklearn.log_model() to save and track a scikit-learn model in MLflow for deployment in Azure ML.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_019",
    "type": "multiple_choice_single_answer",
    "question": "You are deploying a model to a managed online endpoint in Azure ML using SDK v2. What type of YAML configuration is required?",
    "options": [
      {
        "id": "A",
        "text": "endpoint.yml and deployment.yml",
        "is_correct": true,
        "explanation": "Deploying to a managed online endpoint requires two YAML files: one for the endpoint and one for the deployment configuration."
      },
      {
        "id": "B",
        "text": "model.yml and job.yml",
        "is_correct": false,
        "explanation": "These files are used for model registration and running jobs, not endpoint deployment."
      },
      {
        "id": "C",
        "text": "compute.yml and pipeline.yml",
        "is_correct": false,
        "explanation": "These relate to training environments and pipelines, not model deployment endpoints."
      },
      {
        "id": "D",
        "text": "component.yml only",
        "is_correct": false,
        "explanation": "component.yml defines pipeline steps or components, not endpoints."
      }
    ],
    "feedback": "Use `endpoint.yml` to define the endpoint and `deployment.yml` to define how the model is deployed to it.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_020",
    "type": "multiple_choice_single_answer",
    "question": "You want to use MLflow in Azure Machine Learning SDK v2 to log a confusion matrix as an image after model evaluation. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_dict()",
        "is_correct": false,
        "explanation": "This logs structured data like dictionaries, not image files."
      },
      {
        "id": "B",
        "text": "mlflow.log_image()",
        "is_correct": true,
        "explanation": "mlflow.log_image() is used to log visual artifacts such as confusion matrices as images."
      },
      {
        "id": "C",
        "text": "mlflow.log_metric()",
        "is_correct": false,
        "explanation": "This logs scalar values like accuracy or precision, not images."
      },
      {
        "id": "D",
        "text": "mlflow.set_tag()",
        "is_correct": false,
        "explanation": "This is used for metadata tagging, not for storing evaluation visualizations."
      }
    ],
    "feedback": "To save visual outputs like confusion matrices, use mlflow.log_image() with a generated image file.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_021",
    "type": "multiple_choice_single_answer",
    "question": "You want to enable early termination in an AutoML classification job to avoid wasting resources on poorly performing trials. Which setting should you configure?",
    "options": [
      {
        "id": "A",
        "text": "enable_onnx_compatible_models",
        "is_correct": false,
        "explanation": "This controls model compatibility with ONNX but not early stopping."
      },
      {
        "id": "B",
        "text": "enable_early_termination",
        "is_correct": true,
        "explanation": "This setting ensures underperforming trials are stopped early to optimize resource use."
      },
      {
        "id": "C",
        "text": "trial_timeout_minutes",
        "is_correct": false,
        "explanation": "This limits how long each trial can run, but doesn't stop underperformers automatically."
      },
      {
        "id": "D",
        "text": "max_trials",
        "is_correct": false,
        "explanation": "This sets the total number of trials, not whether they should stop early."
      }
    ],
    "feedback": "To save costs and training time in AutoML jobs, enable the 'enable_early_termination' setting.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_022",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure ML SDK v2 and need to create a workspace programmatically. Which class should you use?",
    "options": [
      {
        "id": "A",
        "text": "MLClient.create_workspace()",
        "is_correct": false,
        "explanation": "There is no such method in MLClient. Workspaces are created using a Workspace object and `begin_create()`."
      },
      {
        "id": "B",
        "text": "Workspace()",
        "is_correct": true,
        "explanation": "The Workspace class is used to define the configuration of a workspace before creating it with MLClient."
      },
      {
        "id": "C",
        "text": "Experiment.create()",
        "is_correct": false,
        "explanation": "This is used to create experiments, not workspaces."
      },
      {
        "id": "D",
        "text": "Run.get_context()",
        "is_correct": false,
        "explanation": "This is used for retrieving the context of a run, not for workspace creation."
      }
    ],
    "feedback": "Use the Workspace class and `ml_client.workspaces.begin_create()` to programmatically create a workspace.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_023",
    "type": "multiple_choice_single_answer",
    "question": "You want to deploy a PyTorch model to an online endpoint in Azure ML SDK v2. Which environment image should you use?",
    "options": [
      {
        "id": "A",
        "text": "mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04",
        "is_correct": false,
        "explanation": "This image is commonly used for training, not optimized for serving PyTorch models."
      },
      {
        "id": "B",
        "text": "azureml:PyTorch-1.13-ubuntu20.04-py38-cuda11.7:1",
        "is_correct": true,
        "explanation": "This environment is designed specifically for PyTorch model training and inference, with GPU support."
      },
      {
        "id": "C",
        "text": "azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1",
        "is_correct": false,
        "explanation": "This image is for sklearn models and does not include PyTorch."
      },
      {
        "id": "D",
        "text": "azureml:TensorFlow-2.9-ubuntu20.04-py38:1",
        "is_correct": false,
        "explanation": "This is for TensorFlow models, not PyTorch."
      }
    ],
    "feedback": "Use a PyTorch-specific Azure ML environment for deploying models built with the PyTorch framework.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "q_003",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to create a compute instance that would support running ML pipeline training using the Azure Machine Learning designer v2.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": true,
        "explanation": "**Machine Learning compute clusters** are the ideal infrastructure for running pipelines within Designer v2 in Azure Machine Learning. They are scalable and designed for batch or flow training jobs."
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** A. Yes  \n\n**Explanation:**  \n**Machine Learning compute clusters** are the ideal infrastructure for running pipelines within Designer v2 in Azure Machine Learning. They are scalable and designed for batch or flow training jobs.  \n\n**Reference:**  \n- [Azure ML Compute Targets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target)\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "**Machine Learning compute clusters** are the ideal infrastructure for running pipelines within Designer v2 in Azure Machine Learning. They are scalable and designed for batch or flow training jobs.",
    "include_in_bank": true
  },
  {
    "question_id": "q_004",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to create a compute instance that would support running ML pipeline training using the Azure Machine Learning designer v2.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** B. No  \n\n**Explanation:**  \nAzure Databricks can integrate with Azure ML pipelines, but **it cannot be directly used in Designer v2 as a training target**. Databricks is more suitable for distributed processing with Spark.  \n\n**Reference:**  \n- [What is Designer (v2) in Azure ML?](https://learn.microsoft.com/en-us/azure/machine-learning/concept-designer-overview)\n\n## Case Study 1: Monitoring jobs in Azure Machine Learning",
        "is_correct": true,
        "explanation": "Azure Databricks can integrate with Azure ML pipelines, but **it cannot be directly used in Designer v2 as a training target**. Databricks is more suitable for distributed processing with Spark."
      }
    ],
    "feedback": "Azure Databricks can integrate with Azure ML pipelines, but **it cannot be directly used in Designer v2 as a training target**. Databricks is more suitable for distributed processing with Spark.",
    "include_in_bank": true
  },
  {
    "question_id": "q_005",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to create a compute instance that would support running ML pipeline training using the Azure Machine Learning designer v2.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** B. No  \n\n**Explanation:**  \nHDInsight, like Databricks, is based on Apache Spark and can execute Machine Learning jobs, but **it is not compatible as a direct target within Azure ML Designer**. Designer requires dedicated clusters of type **AML compute cluster**.\n\n**Reference:**  \n- [Configure and submit training jobs](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-cli)\n\n---\n## Case Study 2: Tracking Metrics and Job Execution in Azure Machine Learning",
        "is_correct": true,
        "explanation": "HDInsight, like Databricks, is based on Apache Spark and can execute Machine Learning jobs, but **it is not compatible as a direct target within Azure ML Designer**. Designer requires dedicated clusters of type **AML compute cluster**."
      }
    ],
    "feedback": "HDInsight, like Databricks, is based on Apache Spark and can execute Machine Learning jobs, but **it is not compatible as a direct target within Azure ML Designer**. Designer requires dedicated clusters of type **AML compute cluster**.",
    "include_in_bank": true
  },
  {
    "question_id": "q_006",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to log metrics data from your job runs using MLflow.",
    "options": [
      {
        "id": "A",
        "text": "No",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Yes  \n**Correct answer:** B. Yes  \n\n**Explanation:**  \nThe `mlflow.log_metric()` method is the correct way to log metrics (accuracy, loss, etc.). It takes the metric name and its value as arguments and can be used multiple times to capture the evolution during training.\n\n---",
        "is_correct": true,
        "explanation": "The `mlflow.log_metric()` method is the correct way to log metrics (accuracy, loss, etc.). It takes the metric name and its value as arguments and can be used multiple times to capture the evolution during training."
      }
    ],
    "feedback": "The `mlflow.log_metric()` method is the correct way to log metrics (accuracy, loss, etc.). It takes the metric name and its value as arguments and can be used multiple times to capture the evolution during training.",
    "include_in_bank": true
  },
  {
    "question_id": "q_007",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to log data from your job runs using MLflow.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** B. No  \n\n**Explanation:**  \n`Run.log()` belongs to the Azure ML SDK, not MLflow. While it allows logging artifacts and values, it is not the appropriate tool when using MLflow as the tracking engine.\n\n**Reference:**  \n- Azure ML SDK Run Class\n\n---",
        "is_correct": true,
        "explanation": "`Run.log()` belongs to the Azure ML SDK, not MLflow. While it allows logging artifacts and values, it is not the appropriate tool when using MLflow as the tracking engine."
      }
    ],
    "feedback": "`Run.log()` belongs to the Azure ML SDK, not MLflow. While it allows logging artifacts and values, it is not the appropriate tool when using MLflow as the tracking engine.",
    "include_in_bank": true
  },
  {
    "question_id": "q_008",
    "type": "multiple_choice_multiple_answer",
    "question": "You need to log data from your job runs using MLflow.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** B. No  \n\n**Explanation:**  \n`mlflow.log_param()` is used to log parameters (hyperparameters), not metrics. Since the question requires logging metrics, this option does not meet the goal.\n\n**Reference:**  \n- Logging data to runs in Microsoft documentation (look for it)\n\n---",
        "is_correct": true,
        "explanation": "`mlflow.log_param()` is used to log parameters (hyperparameters), not metrics. Since the question requires logging metrics, this option does not meet the goal."
      }
    ],
    "feedback": "`mlflow.log_param()` is used to log parameters (hyperparameters), not metrics. Since the question requires logging metrics, this option does not meet the goal.",
    "include_in_bank": true
  },
  {
    "question_id": "q_009",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to perform real-time scoring of the ML model and to deploy and debug locally by using local endpoints. You need to create a deployment named `mydep` under the local endpoint.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": true,
        "explanation": "The solution meets the goal. Azure CLI ≥ 2.38.0 allows creating local endpoints and deployments using the `--local` flag, which leverages Docker for local testing. YAML files are used to define both the endpoint and the deployment, enabling debugging and real-time scoring locally."
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** A. Yes  \n\n**Explanation:**  \nThe solution meets the goal. Azure CLI ≥ 2.38.0 allows creating local endpoints and deployments using the `--local` flag, which leverages Docker for local testing. YAML files are used to define both the endpoint and the deployment, enabling debugging and real-time scoring locally.  \n\n**Reference:**  \n- [Install and set up the CLI (v2)](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-local-endpoints-cli?view=azureml-api-2) \n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "The solution meets the goal. Azure CLI ≥ 2.38.0 allows creating local endpoints and deployments using the `--local` flag, which leverages Docker for local testing. YAML files are used to define both the endpoint and the deployment, enabling debugging and real-time scoring locally.",
    "include_in_bank": true
  },
  {
    "question_id": "q_010",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to perform real-time scoring of the ML model and to deploy and debug locally by using local endpoints. You need to create a deployment named `mydep` under the local endpoint. You use Azure ML Python SDK v2 with the following:",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": true,
        "explanation": "Azure ML SDK v2 allows the creation and testing of local endpoints using the `local=True` parameter, running in a Docker environment. The `ml_client` object manages resources such as endpoints, deployments, and jobs, facilitating local testing and debugging."
      },
      {
        "id": "B",
        "text": "No  \n  \n**Correct answer:** A. Yes  \n\n**Explanation:**  \nAzure ML SDK v2 allows the creation and testing of local endpoints using the `local=True` parameter, running in a Docker environment. The `ml_client` object manages resources such as endpoints, deployments, and jobs, facilitating local testing and debugging.\n\n**Reference:**  \n- [MLClient Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient)\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "Azure ML SDK v2 allows the creation and testing of local endpoints using the `local=True` parameter, running in a Docker environment. The `ml_client` object manages resources such as endpoints, deployments, and jobs, facilitating local testing and debugging.",
    "include_in_bank": true
  },
  {
    "question_id": "q_011",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to perform real-time scoring of the ML model and to deploy and debug locally by using local endpoints. You need to create a deployment named `mydep` under the local endpoint.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "No  \n\n**Correct answer:** B. No  \n\n**Explanation:**  \nAzure ML Studio does not support configuring or deploying local endpoints. To do so, you must use Azure CLI v2 or the Azure ML Python SDK v2 with the `local=True` parameter, which leverages Docker for local testing.\n\n**Reference:**  \n- [Deploy and score a machine learning model by using an online endpoint](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-local-endpoints-cli)\n\n---",
        "is_correct": true,
        "explanation": "Azure ML Studio does not support configuring or deploying local endpoints. To do so, you must use Azure CLI v2 or the Azure ML Python SDK v2 with the `local=True` parameter, which leverages Docker for local testing."
      }
    ],
    "feedback": "Azure ML Studio does not support configuring or deploying local endpoints. To do so, you must use Azure CLI v2 or the Azure ML Python SDK v2 with the `local=True` parameter, which leverages Docker for local testing.",
    "include_in_bank": true
  },
  {
    "question_id": "q_012",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to implement a scalable and maintainable solution to automate the retraining of an image classification model using MLOps.",
    "options": [
      {
        "id": "A",
        "text": "Yes",
        "is_correct": true,
        "explanation": "Azure Data Factory enables the creation of pipelines that integrate, transform, and trigger retraining automatically when changes occur in the database. It is ideal for MLOps scenarios with large volumes of data."
      },
      {
        "id": "B",
        "text": "No  \n**Correct answer:** A. Yes  \n**Explanation:**  \nAzure Data Factory enables the creation of pipelines that integrate, transform, and trigger retraining automatically when changes occur in the database. It is ideal for MLOps scenarios with large volumes of data.  \n**Reference:**  \n- Retraining and Updating Azure ML models with Azure Data Factory\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "Azure Data Factory enables the creation of pipelines that integrate, transform, and trigger retraining automatically when changes occur in the database. It is ideal for MLOps scenarios with large volumes of data.",
    "include_in_bank": true
  },
  {
    "question_id": "q_013",
    "type": "multiple_choice_single_answer",
    "question": "What should you use to register the model?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.pyfunc",
        "is_correct": true,
        "explanation": "The `mlflow.pyfunc` module allows defining and registering custom models that combine different frameworks. It is not recommended to use `mlflow.autolog()` or `mlflow.log_params()` as they do not handle custom models properly, and `load_model()` is only used to load a model, not to register it."
      },
      {
        "id": "B",
        "text": "mlflow.autolog()",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "mlflow.log_params()",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "mlflow.<flavor>.load_model()  \n**Correct answer:** A. mlflow.pyfunc  \n**Explanation:**  \nThe `mlflow.pyfunc` module allows defining and registering custom models that combine different frameworks. It is not recommended to use `mlflow.autolog()` or `mlflow.log_params()` as they do not handle custom models properly, and `load_model()` is only used to load a model, not to register it.  \n**Reference:**  \n- [mlflow.pyfunc — MLflow Python Functions (pyfunc) Documentation](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "The `mlflow.pyfunc` module allows defining and registering custom models that combine different frameworks. It is not recommended to use `mlflow.autolog()` or `mlflow.log_params()` as they do not handle custom models properly, and `load_model()` is only used to load a model, not to register it.",
    "include_in_bank": true
  },
  {
    "question_id": "q_014",
    "type": "multiple_choice_multiple_answer",
    "question": "What should you do to read/write this folder directly from an ML script?",
    "options": [
      {
        "id": "A",
        "text": "Create a v2 job with `Dataset.File.from_files()` and `ml_client.jobs.create_or_update()`",
        "is_correct": true,
        "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
      },
      {
        "id": "B",
        "text": "Create a v2 job with `AssetTypes.URI_FOLDER` and `ml_client.jobs.create_or_update()`",
        "is_correct": true,
        "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
      },
      {
        "id": "C",
        "text": "Create a v2 job with `AssetTypes.URI_FILE` and `ml_client.jobs.create_or_update()`  \n\n**Correct answer:** B. Create a v2 job with `AssetTypes.URI_FOLDER` and `ml_client.jobs.create_or_update()`  \n\n**Explanation:**  \nWhen the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment.  \n\n**Reference:**  \n- [AssetTypes.URI_FOLDER in Azure ML SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.constants.assettypes?view=azureml-api-2)  \n- [MLClient.jobs.create_or_update](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-jobs-create-or-update)  \n\n---",
        "is_correct": true,
        "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
      }
    ],
    "feedback": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment.",
    "include_in_bank": true
  },
  {
    "question_id": "q_015",
    "type": "multiple_choice_multiple_answer",
    "question": "Which actions should you take?",
    "options": [
      {
        "id": "A",
        "text": "Specify `idle_seconds_before_scaledown=0`",
        "is_correct": true,
        "explanation": "Compute instances can be automatically paused when there are no jobs running, avoiding unnecessary charges. Setting `idle_seconds_before_scaledown=0` ensures the resource scales down immediately after inactivity. Clusters maintain nodes until they hit their minimum idle timeout, and Azure Data Factory is not used for this scenario."
      },
      {
        "id": "B",
        "text": "Create a compute target as an instance",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Create a compute target as a cluster",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Create a compute target in Azure Data Factory  \n\n**Correct answers:**",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "Compute instances can be automatically paused when there are no jobs running, avoiding unnecessary charges. Setting `idle_seconds_before_scaledown=0` ensures the resource scales down immediately after inactivity. Clusters maintain nodes until they hit their minimum idle timeout, and Azure Data Factory is not used for this scenario.",
    "include_in_bank": true
  },
  {
    "question_id": "q_016",
    "type": "multiple_choice_single_answer",
    "question": "You have built a custom model that uses multiple elements from different frameworks. You need to log the model. What should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.pyfunc",
        "is_correct": true,
        "explanation": "You should use the `mlflow.pyfunc` module to log custom models that combine multiple frameworks or inference logic not natively supported by MLflow. This module defines a generic file-system format for Python models and provides utilities to save and load them in that format."
      },
      {
        "id": "B",
        "text": "mlflow.autolog()",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "mlflow.log_params()",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "mlflow.<flavor>.load_model()  \n\n**Correct answer:** A. mlflow.pyfunc  \n\n**Explanation:**  \nYou should use the `mlflow.pyfunc` module to log custom models that combine multiple frameworks or inference logic not natively supported by MLflow. This module defines a generic file-system format for Python models and provides utilities to save and load them in that format.  \n\n**Reference:**  \n- [MLflow pyfunc documentation](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "You should use the `mlflow.pyfunc` module to log custom models that combine multiple frameworks or inference logic not natively supported by MLflow. This module defines a generic file-system format for Python models and provides utilities to save and load them in that format.",
    "include_in_bank": true
  },
  {
    "question_id": "q_017",
    "type": "multiple_choice_multiple_answer",
    "question": "You have access to column‑oriented data in Parquet format within a folder. The data is not registered as a dataset. You want to read/write the folder in a job using the Azure ML SDK v2. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Create a v2 job using `Dataset.File.from_files()` and submit with `ml_client.jobs.create_or_update()`",
        "is_correct": true,
        "explanation": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it."
      },
      {
        "id": "B",
        "text": "Create a v2 job using `AssetTypes.URI_FOLDER` and submit with `ml_client.jobs.from_config()`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Create a v2 job using `AssetTypes.URI_FILE` and submit with `ml_client.jobs.create_or_update()`",
        "is_correct": true,
        "explanation": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it."
      },
      {
        "id": "D",
        "text": "Create a v2 job using `AssetTypes.URI_FOLDER` and submit with `ml_client.jobs.create_or_update()`  \n\n**Correct answer:** D. Create a v2 job using `AssetTypes.URI_FOLDER` and submit with `ml_client.jobs.create_or_update()`  \n\n**Explanation:**  \n`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it.  \n\n**Reference:**  \n- [AssetTypes.URI_FOLDER in Azure ML SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.constants.assettypes?view=azureml-api-2)  \n- [MLClient.jobs.create_or_update](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient#azure-ai-ml-mlclient-jobs-create-or-update)  \n\n---",
        "is_correct": true,
        "explanation": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it."
      }
    ],
    "feedback": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it.",
    "include_in_bank": true
  },
  {
    "question_id": "q_018",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to ensure batch jobs do not incur cost when not running. What two actions should you perform?",
    "options": [
      {
        "id": "A",
        "text": "Specify idle seconds before scale down to 0",
        "is_correct": true,
        "explanation": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior."
      },
      {
        "id": "B",
        "text": "Create an Azure Machine Learning **compute instance** target",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Create an Azure Machine Learning **compute cluster** target",
        "is_correct": true,
        "explanation": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior."
      },
      {
        "id": "D",
        "text": "Specify the minimum number of cluster nodes to 0  \n\n**Correct answers:** C. Create an Azure Machine Learning compute cluster target  \nD. Specify the minimum number of cluster nodes to 0  \n\n**Explanation:**  \nCompute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior.\n\n**Reference:**  \n- [Azure ML Compute Targets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target)  \n- [Configure autoscale for Azure ML compute clusters](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-compute-cluster)  \n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior.",
    "include_in_bank": true
  },
  {
    "question_id": "q_020",
    "type": "multiple_choice_single_answer",
    "question": "You need to write a batch inference script using `ParallelRunStep`. Which two functions must be included?",
    "options": [
      {
        "id": "A",
        "text": "load()",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "init()",
        "is_correct": true,
        "explanation": "The `init()` function is used to load and initialize resources (for example, loading the model) before any batches are processed. The `run(mini_batch)` function performs inference on each batch of input data. Other functions like `load()`, `evaluate()`, or `execute()` are not valid entry points for `ParallelRunStep`."
      },
      {
        "id": "C",
        "text": "run(mini_batch)",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "evaluate(mini_batch)",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "E",
        "text": "execute(mini_batch)  \n\n**Correct answers:**",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "The `init()` function is used to load and initialize resources (for example, loading the model) before any batches are processed. The `run(mini_batch)` function performs inference on each batch of input data. Other functions like `load()`, `evaluate()`, or `execute()` are not valid entry points for `ParallelRunStep`.",
    "include_in_bank": true
  },
  {
    "question_id": "q_023",
    "type": "multiple_choice_single_answer",
    "question": "Your company uses Azure Machine Learning. You plan to read a dataset from Azure Blob Storage for your machine learning project. The dataset is stored in columnar Parquet format at:",
    "options": [
      {
        "id": "A",
        "text": "`spark.read.text(\"wasbs://mycontainer.blob.core.windows.net\")`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "`spark.read.format(\"parquet\").load(\"wasbs://mycontainer.blob.core.windows.net\")`",
        "is_correct": true,
        "explanation": "The Spark DataFrameReader with `format(\"parquet\")` is the correct method to load columnar Parquet files. It efficiently reads the schema and data layout optimized for Parquet, whereas `text`, `avro`, or `csv` formats would not correctly parse Parquet data."
      },
      {
        "id": "C",
        "text": "`spark.read.format(\"avro\").load(\"wasbs://mycontainer.blob.core.windows.net\")`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "`spark.read.format(\"csv\").load(\"wasbs://mycontainer.blob.core.windows.net\")`  \n\n**Correct answer:**  \nB. `spark.read.format(\"parquet\").load(\"wasbs://mycontainer.blob.core.windows.net\")`  \n\n**Explanation:**  \nThe Spark DataFrameReader with `format(\"parquet\")` is the correct method to load columnar Parquet files. It efficiently reads the schema and data layout optimized for Parquet, whereas `text`, `avro`, or `csv` formats would not correctly parse Parquet data.\n\n**Reference:**  \n- [Apache Spark Parquet File Source](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)  \n- [Use Azure Blob Storage with Azure Synapse Spark](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-storage-blob)\n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "The Spark DataFrameReader with `format(\"parquet\")` is the correct method to load columnar Parquet files. It efficiently reads the schema and data layout optimized for Parquet, whereas `text`, `avro`, or `csv` formats would not correctly parse Parquet data.",
    "include_in_bank": true
  },
  {
    "question_id": "q_027",
    "type": "multiple_choice_multiple_answer",
    "question": "You are using Azure ML SDK v2 and Fairlearn to evaluate and mitigate bias in your model, applying the Equal Opportunity Parity constraint. You employ the `ThresholdOptimizer` post‑processing algorithm. For each statement below, select “Yes” if true, otherwise “No”:",
    "options": [
      {
        "id": "A",
        "text": "Attached compute",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Compute cluster",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Inference cluster ✅",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Compute instance  \n\n**Correct answer:** C. Inference cluster  \n\n**Explanation:**  \nAn **Inference cluster** is the only Azure ML compute option designed to host real‑time web service endpoints under full Azure management. It provides built‑in autoscaling, high availability, and integration with container orchestration:\n\n- **Azure Kubernetes Service (AKS)**: ideal for production scenarios, offering robust autoscaling, rolling upgrades, and SLA‑backed uptime.  \n- **Azure Container Instances (ACI)**: suited for development and testing or low‑volume endpoints, allowing quick, serverless deployment without cluster management overhead.\n\nOther compute targets do not meet the requirements:  \n- **Compute clusters** (AML compute) are optimized for batch training and pipeline jobs, not for serving live HTTP requests.  \n- **Compute instances** serve as interactive notebooks and development workspaces, lacking the orchestration needed for scalable inference.  \n- **Attached compute** refers to external resources you bring into Azure ML, which you then manage yourself rather than letting Azure handle scaling and availability.\n\nBy choosing an Inference cluster, you gain a fully managed environment that automatically provisions and scales resources, integrates with your CI/CD pipelines, and ensures low‑latency, reliable real‑time scoring.\n\n**References:**  \n- [Deploy a real-time endpoint with Azure ML Designer](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-designer-automobile-price-deployment)  \n- [Azure ML compute targets overview](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target)  \n- [Configure inference clusters in Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service)",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "1. **ThresholdOptimizer and sensitive features**: True. ThresholdOptimizer is a post‑processing method that takes a pre‑trained classifier and a sensitive feature, then adjusts decision thresholds to satisfy the Equal Opportunity constraint—it does **not** retrain the model.",
    "include_in_bank": true
  },
  {
    "question_id": "q_028",
    "type": "multiple_choice_multiple_answer",
    "question": "You are performing hyperparameter tuning of an ML model using the Azure ML SDK v2. You need an early termination policy that meets these requirements:",
    "options": [
      {
        "id": "A",
        "text": "Bandit",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Median stopping",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Truncation selection ✅  \n\n**Correct answer:** C. Truncation selection  \n\n**Explanation:**  \nThe **Truncation Selection** policy is designed to drop a specified fraction of the poorest trials at each evaluation checkpoint, exactly matching your needs:  \n\n- **Percentage‑based pruning**: You can configure a `truncation_percentage` to define what fraction of the lowest‑performing runs are canceled each time you evaluate (e.g., 20%).  \n- **Immediate interval evaluation**: Setting `evaluation_interval=1` ensures the policy considers every checkpoint rather than skipping intervals.  \n- **Delayed application**: `delay_evaluation=6` lets the first six intervals run fully before any pruning, giving all trials a fair chance to warm up.  \n\nBy contrast:  \n- **Median stopping** cancels runs that fall below the median performance of all active runs. It doesn’t allow you to specify a fixed percentage for pruning and uses a different decision rule.  \n- **Bandit** (Hyperband style) dynamically allocates resources across trials but doesn’t directly map to “cancel X% of worst runs every Y intervals.” It’s focused on bracket‑based resource allocation rather than a simple truncation strategy.  \n\nTherefore, **Truncation Selection** is the only policy that supports both percentage‑based pruning and delayed start as specified.\n\n**References:**  \n- [Early Termination Policies in Azure ML hyperparameter tuning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)  \n- [TruncationSelectionPolicy class (SDK v2)](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.sweep.truncationselectionpolicy)  \n📘 [Azure ML Early Termination PoliciesAzure ML Early Termination Policie](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters?view=azureml-api-2)",
        "is_correct": true,
        "explanation": "The **Truncation Selection** policy is designed to drop a specified fraction of the poorest trials at each evaluation checkpoint, exactly matching your needs:"
      }
    ],
    "feedback": "The **Truncation Selection** policy is designed to drop a specified fraction of the poorest trials at each evaluation checkpoint, exactly matching your needs:",
    "include_in_bank": true
  },
  {
    "question_id": "q_029",
    "type": "multiple_choice_multiple_answer",
    "question": "You are configuring a Spark session in Azure Machine Learning Notebooks to process data stored in an Azure Data Lake Storage (ADLS) Gen2 account. You need to securely connect to this storage account without adding excessive complexity to your configuration. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Use a service principal for OAuth-based authentication by configuring the client ID, client secret, and tenant ID.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Use OAuth for authentication by configuring the `fs.azure.account.auth.type` and related properties.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Use a SAS token for authentication by setting the `fs.azure.sas` property.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Use the storage account’s access key to authenticate and set the `fs.azure.account.key` property.  \n\n\n**Correct answer:**  \nD. Use the storage account’s access key to authenticate and set the `fs.azure.account.key` property.  \n\n**Explanation:**  \nUsing the storage account access key is the simplest secure method for Spark in Azure ML Notebooks. It requires only a single secret, which you can store in Azure Key Vault or as a workspace secret, and avoids the overhead of registering an Azure AD application or handling token lifecycles. Service principals and OAuth configurations demand managing client IDs, secrets, tenant IDs, and often custom token‑refresh logic. SAS tokens introduce separate expiration and permission management. By contrast, the account key approach lets Spark authenticate directly to ADLS Gen2 with minimal setup while still allowing you to rotate the key centrally.\n\nLa opción correcta es usar la **clave de acceso (Access Key)** de la cuenta de almacenamiento para autenticarse en Spark y configurar la propiedad:\n\n```python\nsc._jsc.hadoopConfiguration().set(\n    \"fs.azure.account.key.<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net\",\n    \"<ACCESS_KEY>\"\n)\n```\nThis authentication method:\n\n- Is secure and straightforward.\n- Does not require configuring multiple credentials or external tokens.\n- Is ideal if you want to avoid the complexity of managing OAuth, service principals, or SAS tokens.\n\nWhy the other options are ruled out:\n❌ Service Principal with OAuth: Involves managing client ID, secret, and tenant ID, which requires more setup, key rotation, and regular maintenance.\n\n❌ OAuth (fs.azure.account.auth.type): Requires additional configuration and is more complex for basic scenarios.\n\n❌ SAS Token: While secure, it requires managing tokens with permissions and expirations, adding unnecessary complexity.\n\n\n**References:**  \n- [Authenticate Spark with ADLS Gen2 using account key](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-configure-spark)  \n- [Azure Machine Learning secrets management](https://learn.microsoft.com/azure/machine-learning/how-to-get-azure-key-vault-secrets)",
        "is_correct": true,
        "explanation": "Using the storage account access key is the simplest secure method for Spark in Azure ML Notebooks. It requires only a single secret, which you can store in Azure Key Vault or as a workspace secret, and avoids the overhead of registering an Azure AD application or handling token lifecycles. Service principals and OAuth configurations demand managing client IDs, secrets, tenant IDs, and often custom token‑refresh logic. SAS tokens introduce separate expiration and permission management. By contrast, the account key approach lets Spark authenticate directly to ADLS Gen2 with minimal setup while still allowing you to rotate the key centrally."
      }
    ],
    "feedback": "Using the storage account access key is the simplest secure method for Spark in Azure ML Notebooks. It requires only a single secret, which you can store in Azure Key Vault or as a workspace secret, and avoids the overhead of registering an Azure AD application or handling token lifecycles. Service principals and OAuth configurations demand managing client IDs, secrets, tenant IDs, and often custom token‑refresh logic. SAS tokens introduce separate expiration and permission management. By contrast, the account key approach lets Spark authenticate directly to ADLS Gen2 with minimal setup while still allowing you to rotate the key centrally.",
    "include_in_bank": true
  },
  {
    "question_id": "q_030",
    "type": "multiple_choice_multiple_answer",
    "question": "You want to create a datastore in Azure Machine Learning Studio. Before that, you need to provision the storage instance where your **relational** data will reside. You navigate to the “Datastore” option in Azure ML Studio. Which source storage type should you select?",
    "options": [
      {
        "id": "A",
        "text": "Azure Data Lake Gen2",
        "is_correct": true,
        "explanation": "Azure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service—Azure Database for PostgreSQL—rather than file‑based storage like Blob or ADLS Gen2, which are designed for unstructured or semi‑structured files, not relational tables."
      },
      {
        "id": "B",
        "text": "Azure Blob Container",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Databricks File System",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Azure Database for PostgreSQL  \n\n**Correct answer:** D. Azure Database for PostgreSQL  \n\n**Explanation:**  \nAzure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service—Azure Database for PostgreSQL—rather than file‑based storage like Blob or ADLS Gen2, which are designed for unstructured or semi‑structured files, not relational tables.\n\n**Reference:**  \n- [Datastore module documentation](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py)",
        "is_correct": true,
        "explanation": "Azure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service—Azure Database for PostgreSQL—rather than file‑based storage like Blob or ADLS Gen2, which are designed for unstructured or semi‑structured files, not relational tables."
      }
    ],
    "feedback": "Azure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service—Azure Database for PostgreSQL—rather than file‑based storage like Blob or ADLS Gen2, which are designed for unstructured or semi‑structured files, not relational tables.",
    "include_in_bank": true
  },
  {
    "question_id": "q_031",
    "type": "multiple_choice_single_answer",
    "question": "Your Azure ML Workspace’s associated storage account keys were compromised and you regenerated them. Users are now experiencing access errors. Which CLI command should you run to update the workspace’s storage keys?",
    "options": [
      {
        "id": "A",
        "text": "`az ml workspace update`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "`az ml workspace share`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "`az ml workspace sync-keys`  \n\n**Correct answer:** C. `az ml workspace sync-keys`  \n\n**Explanation:**  \nWhen you regenerate the access keys of the storage account linked to an Azure ML Workspace, the workspace loses its ability to access blobs and files. The `az ml workspace sync-keys` command pushes the new keys into the workspace metadata, restoring its access. Without running `sync-keys`, the workspace continues using the old credentials and will fail on any storage operations.\n\n\nPara resolver esto, se debe usar el comando:\n\n```bash\naz ml workspace sync-keys \\\n  --name <workspace-name> \\\n  --resource-group <resource-group-name>\n```\nThis command synchronizes the updated keys of the associated storage resource (such as Azure Blob or ADLS Gen2) with the Azure ML Workspace.  \nThis is essential to avoid interruptions in the execution of notebooks, experiments, or pipelines that depend on the storage.\n\n🔒 **Important:**  \nRegenerating the key in the resource is not enough; the workspace must be explicitly updated with these new credentials using `sync-keys`.\n\n\n**Reference:**  \n- [az ml workspace sync-keys](https://learn.microsoft.com/en-us/cli/azure/ml/workspace?view=azure-cli-latest#az-ml-workspace-sync-keys)",
        "is_correct": true,
        "explanation": "When you regenerate the access keys of the storage account linked to an Azure ML Workspace, the workspace loses its ability to access blobs and files. The `az ml workspace sync-keys` command pushes the new keys into the workspace metadata, restoring its access. Without running `sync-keys`, the workspace continues using the old credentials and will fail on any storage operations."
      }
    ],
    "feedback": "When you regenerate the access keys of the storage account linked to an Azure ML Workspace, the workspace loses its ability to access blobs and files. The `az ml workspace sync-keys` command pushes the new keys into the workspace metadata, restoring its access. Without running `sync-keys`, the workspace continues using the old credentials and will fail on any storage operations.",
    "include_in_bank": true
  },
  {
    "question_id": "q_034",
    "type": "multiple_choice_multiple_answer",
    "question": "Your organization provided a set of images for a multi‑label classification exercise. You need to present a tabular view showing each image alongside its class labels with minimal effort. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Export data labels to the binary file format.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Export data labels to the COCO file format.",
        "is_correct": true,
        "explanation": "The COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It’s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing."
      },
      {
        "id": "C",
        "text": "Name the files with the label that was associated with them during classification.",
        "is_correct": true,
        "explanation": "The COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It’s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing."
      },
      {
        "id": "D",
        "text": "Add the file name to the labels dataset.  \n\n**Correct answer:** B. Export data labels to the COCO file format.  \n\n**Explanation:**  \nThe COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It’s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing.  \n\n**References:**  \n- [Labeling images and text documents in Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-label-data)  \n- [Create and explore labeled datasets](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-labeling-projects)  \n\n---",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "The COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It’s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing.",
    "include_in_bank": true
  },
  {
    "question_id": "q_035",
    "type": "multiple_choice_multiple_answer",
    "question": "You’ve trained a linear regression model in Azure Machine Learning Studio and want users to call a real‑time endpoint on CPU with minimal cost for testing and debugging. Which two deployment targets should you select?",
    "options": [
      {
        "id": "A",
        "text": "Azure Container Instances",
        "is_correct": true,
        "explanation": "- **Azure Container Instances (ACI):** Quick to provision, low‑cost, ideal for light testing and short‑term real‑time inference on CPU."
      },
      {
        "id": "B",
        "text": "Azure Machine Learning Kubernetes",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Local web service",
        "is_correct": true,
        "explanation": "- **Azure Container Instances (ACI):** Quick to provision, low‑cost, ideal for light testing and short‑term real‑time inference on CPU."
      },
      {
        "id": "D",
        "text": "Azure Machine Learning compute clusters  \n\n**Correct answers:**",
        "is_correct": false,
        "explanation": ""
      }
    ],
    "feedback": "- **Azure Container Instances (ACI):** Quick to provision, low‑cost, ideal for light testing and short‑term real‑time inference on CPU.",
    "include_in_bank": true
  },
  {
    "question_id": "q_036",
    "type": "multiple_choice_multiple_answer",
    "question": "You are building a model to perform binary classification on a large dataset. You plan to tune the hyperparameters of the model in order to optimize its performance. You have defined a search space that includes the learning rate, the number of hidden layers, and the number of neurons in each hidden layer.",
    "options": [
      {
        "id": "A",
        "text": "Gradient‑based optimization",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Grid sampling",
        "is_correct": true,
        "explanation": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient‑based methods optimize model parameters during training and are not designed for hyperparameter search."
      },
      {
        "id": "C",
        "text": "Random sampling",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Bayesian sampling  \n\n**Correct answer:** D. Bayesian sampling  \n\n**Explanation:**  \nBayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient‑based methods optimize model parameters during training and are not designed for hyperparameter search.  \n\n**References:**  \n- [Hyperparameter tuning a model (v2)](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters-v2)  \n- [Bayesian optimization overview](https://en.wikipedia.org/wiki/Bayesian_optimization)",
        "is_correct": true,
        "explanation": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient‑based methods optimize model parameters during training and are not designed for hyperparameter search."
      }
    ],
    "feedback": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient‑based methods optimize model parameters during training and are not designed for hyperparameter search.",
    "include_in_bank": true
  },
  {
    "question_id": "q_038",
    "type": "multiple_choice_multiple_answer",
    "question": "You use Azure Machine Learning to create a machine learning pipeline. Your dataset includes sparse string and numeric data. While working with pipeline components, you receive an error indicating that **a value is required**.",
    "options": [
      {
        "id": "A",
        "text": "Configure the Select Columns in Dataset component to exclude string data.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Use the Select Columns in Dataset component to choose a column.",
        "is_correct": true,
        "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The “Select Columns in Dataset” component must be told which column(s) to include; without that, it throws “a value is required.” By using this component to choose at least one column, you satisfy the requirement and prevent the error."
      },
      {
        "id": "C",
        "text": "Configure a custom substitution value in the Clean Missing Data component.",
        "is_correct": true,
        "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The “Select Columns in Dataset” component must be told which column(s) to include; without that, it throws “a value is required.” By using this component to choose at least one column, you satisfy the requirement and prevent the error."
      },
      {
        "id": "D",
        "text": "Specify the columns to be cleaned in the Clean Missing Data component.  \n\n**Correct answer:**  \nB. Use the Select Columns in Dataset component to choose a column.  \n\n**Explanation:**  \nYour pipeline failed because you did not explicitly select any columns for the component to operate on. The “Select Columns in Dataset” component must be told which column(s) to include; without that, it throws “a value is required.” By using this component to choose at least one column, you satisfy the requirement and prevent the error.  \n\n**Reference:**  \n- [Select Columns in Dataset component](https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/select-columns-in-dataset)\n\n---",
        "is_correct": true,
        "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The “Select Columns in Dataset” component must be told which column(s) to include; without that, it throws “a value is required.” By using this component to choose at least one column, you satisfy the requirement and prevent the error."
      }
    ],
    "feedback": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The “Select Columns in Dataset” component must be told which column(s) to include; without that, it throws “a value is required.” By using this component to choose at least one column, you satisfy the requirement and prevent the error.",
    "include_in_bank": true
  },
  {
    "question_id": "q_039",
    "type": "multiple_choice_multiple_answer",
    "question": "Your marketing team provides a 1 GB CSV file. All processing will happen in memory using pandas DataFrames. You need to recommend the minimum RAM configuration to support efficient processing.",
    "options": [
      {
        "id": "A",
        "text": "2 GB",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "8 GB",
        "is_correct": true,
        "explanation": "Although the file is 1 GB on disk, loading into pandas can expand its in‑memory footprint to around 10 GB. To allow for transformations, filters, joins, and other operations without running out of memory, it’s best to provision roughly twice that—about 20 GB."
      },
      {
        "id": "C",
        "text": "10 GB",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "20 GB  \n\n**Correct answer:**  \nD. 20 GB  \n\n**Explanation:**  \nAlthough the file is 1 GB on disk, loading into pandas can expand its in‑memory footprint to around 10 GB. To allow for transformations, filters, joins, and other operations without running out of memory, it’s best to provision roughly twice that—about 20 GB.  \n\n**References:**  \n- [Create and manage data assets](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-data-assets)  \n- [Scaling to large datasets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target#scaling-to-large-datasets)\n\n---",
        "is_correct": true,
        "explanation": "Although the file is 1 GB on disk, loading into pandas can expand its in‑memory footprint to around 10 GB. To allow for transformations, filters, joins, and other operations without running out of memory, it’s best to provision roughly twice that—about 20 GB."
      }
    ],
    "feedback": "Although the file is 1 GB on disk, loading into pandas can expand its in‑memory footprint to around 10 GB. To allow for transformations, filters, joins, and other operations without running out of memory, it’s best to provision roughly twice that—about 20 GB.",
    "include_in_bank": true
  },
  {
    "question_id": "q_041",
    "type": "multiple_choice_multiple_answer",
    "question": "You need a compute resource that supports AutoML, multi‑step pipelines, and the Azure ML Designer interface.",
    "options": [
      {
        "id": "A",
        "text": "Deploy an Azure Machine Learning compute cluster.",
        "is_correct": true,
        "explanation": "An Azure ML compute cluster provides elastic scaling across multiple nodes and is fully integrated with AutoML, pipeline orchestration, and the drag‑and‑drop Designer experience. Local SDK installations and remote VMs lack seamless workspace integration and scalability, while Databricks is not supported by the Designer UI."
      },
      {
        "id": "B",
        "text": "Install the Azure ML SDK on your local computer.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "Create and deploy a remote virtual machine.",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Deploy Azure Databricks as a compute target.  \n\n**Correct answer:**  \nA. Deploy an Azure Machine Learning compute cluster.  \n\n**Explanation:**  \nAn Azure ML compute cluster provides elastic scaling across multiple nodes and is fully integrated with AutoML, pipeline orchestration, and the drag‑and‑drop Designer experience. Local SDK installations and remote VMs lack seamless workspace integration and scalability, while Databricks is not supported by the Designer UI.  \n\n**References:**  \n- [Compute targets in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target)  \n- [Create and attach a compute cluster](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster)",
        "is_correct": true,
        "explanation": "An Azure ML compute cluster provides elastic scaling across multiple nodes and is fully integrated with AutoML, pipeline orchestration, and the drag‑and‑drop Designer experience. Local SDK installations and remote VMs lack seamless workspace integration and scalability, while Databricks is not supported by the Designer UI."
      }
    ],
    "feedback": "An Azure ML compute cluster provides elastic scaling across multiple nodes and is fully integrated with AutoML, pipeline orchestration, and the drag‑and‑drop Designer experience. Local SDK installations and remote VMs lack seamless workspace integration and scalability, while Databricks is not supported by the Designer UI.",
    "include_in_bank": true
  },
  {
    "question_id": "q_042",
    "type": "multiple_choice_multiple_answer",
    "question": "You tune hyperparameters on your HyperDrive experiment using Random sampling. You want to terminate 30 percent of the lowest performing runs at each evaluation interval, based on the primary metric. Which early termination policy should you use?",
    "options": [
      {
        "id": "A",
        "text": "Median stopping policy",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "B",
        "text": "Bandit policy",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "No termination policy",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "Truncation selection policy  \n\n**Correct answer:**  \nD. Truncation selection policy  \n\n**Explanation:**  \nThe Truncation Selection policy is designed to drop a fixed percentage of the worst‑performing trials at each evaluation checkpoint, exactly matching the requirement to terminate 30 % of runs. It supports parameters such as `truncation_percentage`, `evaluation_interval`, and `delay_evaluation`. In contrast, the Bandit policy uses a slack factor rather than a fixed percentage, the Median Stopping policy cancels runs below the median without a configurable percentage, and choosing no termination policy would allow all runs to complete regardless of performance.\n\n**Reference:**  \n- [HyperDrive early termination policies](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#early-termination-policies)",
        "is_correct": true,
        "explanation": "The Truncation Selection policy is designed to drop a fixed percentage of the worst‑performing trials at each evaluation checkpoint, exactly matching the requirement to terminate 30 % of runs. It supports parameters such as `truncation_percentage`, `evaluation_interval`, and `delay_evaluation`. In contrast, the Bandit policy uses a slack factor rather than a fixed percentage, the Median Stopping policy cancels runs below the median without a configurable percentage, and choosing no termination policy would allow all runs to complete regardless of performance."
      }
    ],
    "feedback": "The Truncation Selection policy is designed to drop a fixed percentage of the worst‑performing trials at each evaluation checkpoint, exactly matching the requirement to terminate 30 % of runs. It supports parameters such as `truncation_percentage`, `evaluation_interval`, and `delay_evaluation`. In contrast, the Bandit policy uses a slack factor rather than a fixed percentage, the Median Stopping policy cancels runs below the median without a configurable percentage, and choosing no termination policy would allow all runs to complete regardless of performance.",
    "include_in_bank": true
  },
  {
    "question_id": "q_043",
    "type": "multiple_choice_multiple_answer",
    "question": "You are planning to create an Azure Machine Learning registry to share ML assets across multiple workspaces. You need to complete the YAML configuration for the registry deployment via Azure CLI. Which settings should you choose?",
    "options": [
      {
        "id": "A",
        "text": "`storage_account_hns: False`",
        "is_correct": true,
        "explanation": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory‑style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry."
      },
      {
        "id": "B",
        "text": "`storage_account_type: Standard_LRS`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "C",
        "text": "`storage_blob_encryption`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "D",
        "text": "`storage_account_tier`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "E",
        "text": "`access_tier`",
        "is_correct": false,
        "explanation": ""
      },
      {
        "id": "F",
        "text": "`replication_type`  \n\n**Correct answers:**",
        "is_correct": true,
        "explanation": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory‑style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry."
      }
    ],
    "feedback": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory‑style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry.",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_024",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure ML SDK v2 and want to log a model evaluation report stored in a JSON file. Which MLflow method should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_text()",
        "is_correct": false,
        "explanation": "There is no `log_text` method in MLflow."
      },
      {
        "id": "B",
        "text": "mlflow.log_dict()",
        "is_correct": true,
        "explanation": "This method is used to log a Python dictionary or JSON-like object to the run as an artifact."
      },
      {
        "id": "C",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "This is used for scalar parameters, not for structured data like a JSON report."
      },
      {
        "id": "D",
        "text": "mlflow.set_tag()",
        "is_correct": false,
        "explanation": "Tags are used to attach metadata, not for logging JSON files."
      }
    ],
    "feedback": "Use `mlflow.log_dict()` to log structured evaluation reports like JSON files in your experiment runs.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_025",
    "type": "multiple_choice_single_answer",
    "question": "You want to invoke an online endpoint deployed in Azure ML SDK v2 and send a JSON payload. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.online_endpoints.run()",
        "is_correct": false,
        "explanation": "This method does not exist. Invocation is handled by a different method."
      },
      {
        "id": "B",
        "text": "ml_client.online_endpoints.invoke()",
        "is_correct": true,
        "explanation": "This is the correct method to call an online endpoint with input data."
      },
      {
        "id": "C",
        "text": "ml_client.endpoints.send_request()",
        "is_correct": false,
        "explanation": "There is no such method as `send_request()` in the SDK."
      },
      {
        "id": "D",
        "text": "mlflow.call_endpoint()",
        "is_correct": false,
        "explanation": "MLflow does not manage endpoint invocation in Azure ML SDK v2."
      }
    ],
    "feedback": "Use `ml_client.online_endpoints.invoke()` to interact with a deployed endpoint and send input data.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_026",
    "type": "multiple_choice_single_answer",
    "question": "You are creating a pipeline with Azure ML SDK v2. One of the steps needs a compute target with GPU capabilities. What should you define in the component YAML?",
    "options": [
      {
        "id": "A",
        "text": "resources: instance_type: Standard_DS3_v2",
        "is_correct": false,
        "explanation": "Standard_DS3_v2 is a CPU-based machine, not suitable for GPU workloads."
      },
      {
        "id": "B",
        "text": "resources: instance_type: Standard_NC6",
        "is_correct": true,
        "explanation": "Standard_NC6 is a GPU-enabled VM, suitable for tasks like deep learning."
      },
      {
        "id": "C",
        "text": "compute: azureml:gpu-cluster",
        "is_correct": false,
        "explanation": "This is a reference to the compute, but the question is about defining it in the component YAML."
      },
      {
        "id": "D",
        "text": "type: GPU",
        "is_correct": false,
        "explanation": "There is no 'type: GPU' declaration in Azure ML YAML schemas."
      }
    ],
    "feedback": "Use `resources: instance_type: Standard_NC6` in your component YAML to specify GPU support.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_027",
    "type": "multiple_choice_single_answer",
    "question": "You are running a job in Azure ML using a local compute. Which compute target value should you use in the job definition?",
    "options": [
      {
        "id": "A",
        "text": "local",
        "is_correct": true,
        "explanation": "Setting `compute: local` allows the job to run in your local environment without uploading code or data to the cloud."
      },
      {
        "id": "B",
        "text": "azureml:local-cluster",
        "is_correct": false,
        "explanation": "There is no built-in compute named 'local-cluster'. 'local' is the correct keyword."
      },
      {
        "id": "C",
        "text": "default",
        "is_correct": false,
        "explanation": "The default compute might point to a cloud cluster, not your local environment."
      },
      {
        "id": "D",
        "text": "None",
        "is_correct": false,
        "explanation": "Compute must be explicitly set to 'local' for local runs in Azure ML SDK v2."
      }
    ],
    "feedback": "To run jobs locally with Azure ML SDK v2, use `compute: local` in the job definition.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_028",
    "type": "multiple_choice_single_answer",
    "question": "You want to define a pipeline in Azure ML SDK v2 that has dependencies between steps. What should you use to pass data between components?",
    "options": [
      {
        "id": "A",
        "text": "Environment variables",
        "is_correct": false,
        "explanation": "Environment variables are used for configuration, not for passing artifacts between steps."
      },
      {
        "id": "B",
        "text": "PipelineInput",
        "is_correct": false,
        "explanation": "PipelineInput is not a valid class in SDK v2. Use component outputs instead."
      },
      {
        "id": "C",
        "text": "Output ports of one component as inputs to another",
        "is_correct": true,
        "explanation": "This is the correct method to pass data and maintain dependencies between pipeline steps."
      },
      {
        "id": "D",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "mlflow is for experiment tracking, not for orchestrating data flow in pipelines."
      }
    ],
    "feedback": "In Azure ML pipelines, define outputs in one step and use them as inputs in the next to link components.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_029",
    "type": "multiple_choice_single_answer",
    "question": "You are trying to log multiple metrics from a model evaluation loop using MLflow in Azure ML SDK v2. Which approach is correct?",
    "options": [
      {
        "id": "A",
        "text": "Use mlflow.log_metrics({'accuracy': 0.92, 'precision': 0.88})",
        "is_correct": true,
        "explanation": "mlflow.log_metrics() accepts a dictionary to log multiple metrics at once."
      },
      {
        "id": "B",
        "text": "Use mlflow.set_tags()",
        "is_correct": false,
        "explanation": "Tags are used for metadata, not for metrics logging."
      },
      {
        "id": "C",
        "text": "Use mlflow.save_metrics()",
        "is_correct": false,
        "explanation": "This function does not exist in MLflow."
      },
      {
        "id": "D",
        "text": "Use mlflow.log_param()",
        "is_correct": false,
        "explanation": "log_param() is used for model parameters, not performance metrics."
      }
    ],
    "feedback": "To log multiple evaluation results, use mlflow.log_metrics() with a dictionary of metrics.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_030",
    "type": "multiple_choice_single_answer",
    "question": "You need to define a data input for a job using a remote folder stored in Azure Blob Storage. What input type should you specify in the Azure ML SDK v2?",
    "options": [
      {
        "id": "A",
        "text": "Input(type='mltable')",
        "is_correct": false,
        "explanation": "MLTable is used for structured tabular data, not arbitrary remote folders."
      },
      {
        "id": "B",
        "text": "Input(type='uri_folder')",
        "is_correct": true,
        "explanation": "uri_folder is the correct input type to specify a folder path in Azure storage."
      },
      {
        "id": "C",
        "text": "Input(type='string')",
        "is_correct": false,
        "explanation": "string is for parameter values, not data inputs."
      },
      {
        "id": "D",
        "text": "Input(type='uri_file')",
        "is_correct": false,
        "explanation": "uri_file is for individual files, not entire folders."
      }
    ],
    "feedback": "Use Input(type='uri_folder') when referencing a folder in Azure storage as input for a job.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_031",
    "type": "multiple_choice_single_answer",
    "question": "You need to deploy a model trained with a custom PyFunc wrapper using MLflow in Azure ML. Which MLflow method should be used?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.sklearn.log_model()",
        "is_correct": false,
        "explanation": "This method is specific to scikit-learn models, not custom PyFunc models."
      },
      {
        "id": "B",
        "text": "mlflow.pyfunc.log_model()",
        "is_correct": true,
        "explanation": "mlflow.pyfunc.log_model() is used to log models with custom Python functions for inference."
      },
      {
        "id": "C",
        "text": "mlflow.register_model()",
        "is_correct": false,
        "explanation": "This registers a model after it has been logged; it doesn't handle logging itself."
      },
      {
        "id": "D",
        "text": "mlflow.log_model()",
        "is_correct": false,
        "explanation": "This generic function is not specific to PyFunc; using the correct flavor is recommended."
      }
    ],
    "feedback": "Use mlflow.pyfunc.log_model() to log custom models defined using the PyFunc interface for deployment.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_032",
    "type": "multiple_choice_single_answer",
    "question": "You want to trigger a retraining pipeline in Azure ML whenever new data is added to a Blob Storage container. Which tool should you use?",
    "options": [
      {
        "id": "A",
        "text": "Azure Monitor",
        "is_correct": false,
        "explanation": "Azure Monitor is used for diagnostics and alerting, not for triggering pipelines."
      },
      {
        "id": "B",
        "text": "Azure Event Grid",
        "is_correct": true,
        "explanation": "Azure Event Grid can trigger Azure ML pipelines based on storage events like file uploads."
      },
      {
        "id": "C",
        "text": "Azure Application Insights",
        "is_correct": false,
        "explanation": "Application Insights is for monitoring apps, not orchestrating ML workflows."
      },
      {
        "id": "D",
        "text": "Azure Policy",
        "is_correct": false,
        "explanation": "Azure Policy is for enforcing governance rules, not triggering ML pipelines."
      }
    ],
    "feedback": "Azure Event Grid is designed to respond to events such as new data arrivals and can trigger Azure ML workflows.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_033",
    "type": "multiple_choice_single_answer",
    "question": "You are running a job in Azure ML and want to store logs and outputs in a custom Azure Storage account. What should you configure?",
    "options": [
      {
        "id": "A",
        "text": "Default compute target",
        "is_correct": false,
        "explanation": "The compute target defines where the job runs, not where outputs are stored."
      },
      {
        "id": "B",
        "text": "Default datastore",
        "is_correct": true,
        "explanation": "The default datastore determines where outputs and logs are stored during job execution."
      },
      {
        "id": "C",
        "text": "Environment image",
        "is_correct": false,
        "explanation": "The environment defines dependencies, not storage."
      },
      {
        "id": "D",
        "text": "Pipeline parameters",
        "is_correct": false,
        "explanation": "Parameters are used to customize jobs, not control storage behavior."
      }
    ],
    "feedback": "Set the default datastore in your workspace to control where logs and outputs are written.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_034",
    "type": "multiple_choice_single_answer",
    "question": "You are defining a command job in Azure ML SDK v2. You want to specify a Python script, its environment, and data inputs. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.jobs.create_or_update()",
        "is_correct": true,
        "explanation": "This method is used to submit command jobs defined in a YAML file or Python dictionary."
      },
      {
        "id": "B",
        "text": "ml_client.scripts.run()",
        "is_correct": false,
        "explanation": "This method does not exist in Azure ML SDK v2."
      },
      {
        "id": "C",
        "text": "mlflow.run_job()",
        "is_correct": false,
        "explanation": "This is not part of the Azure ML SDK v2; MLflow does not submit Azure ML jobs directly."
      },
      {
        "id": "D",
        "text": "ml_client.pipeline.execute()",
        "is_correct": false,
        "explanation": "pipeline.execute() is used for pipelines, not individual command jobs."
      }
    ],
    "feedback": "Use `ml_client.jobs.create_or_update()` to submit and configure command jobs in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_035",
    "type": "multiple_choice_single_answer",
    "question": "You need to configure the responsible AI dashboard in Azure Machine Learning. Which tool allows you to generate insights such as feature importance and error analysis?",
    "options": [
      {
        "id": "A",
        "text": "azureml.interpret package",
        "is_correct": true,
        "explanation": "The azureml.interpret package provides utilities to create visualizations for Responsible AI, including feature importance and error analysis."
      },
      {
        "id": "B",
        "text": "mlflow.interpret()",
        "is_correct": false,
        "explanation": "This method does not exist in MLflow or Azure ML."
      },
      {
        "id": "C",
        "text": "azureml.automl.core.analyze_model()",
        "is_correct": false,
        "explanation": "This method is not related to the Responsible AI dashboard."
      },
      {
        "id": "D",
        "text": "azureml.core.ResponsibleAI",
        "is_correct": false,
        "explanation": "There is no such class; insights are created through interpret and dashboard packages."
      }
    ],
    "feedback": "Use the `azureml.interpret` package to enable interpretability and fairness tools in Azure ML.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_036",
    "type": "multiple_choice_single_answer",
    "question": "You are using Azure ML SDK v2 and want to register a local model file to your workspace. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.models.upload()",
        "is_correct": false,
        "explanation": "There is no method called upload() for models in SDK v2."
      },
      {
        "id": "B",
        "text": "ml_client.models.create_or_update()",
        "is_correct": true,
        "explanation": "This method registers a model by uploading the local file to the Azure ML workspace."
      },
      {
        "id": "C",
        "text": "mlflow.register_model()",
        "is_correct": false,
        "explanation": "This is used to register models in MLflow’s registry, not directly in Azure ML via SDK v2."
      },
      {
        "id": "D",
        "text": "ml_client.register_model()",
        "is_correct": false,
        "explanation": "There is no method named register_model in the SDK v2."
      }
    ],
    "feedback": "Use `ml_client.models.create_or_update()` to register local models in Azure ML workspace using SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_037",
    "type": "multiple_choice_single_answer",
    "question": "You want to log a dictionary of hyperparameters from a grid search in Azure ML using MLflow. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_params()",
        "is_correct": true,
        "explanation": "mlflow.log_params() is designed to log a dictionary of key-value pairs as parameters."
      },
      {
        "id": "B",
        "text": "mlflow.log_dict()",
        "is_correct": false,
        "explanation": "This logs structured data, but it’s not appropriate for hyperparameters that should appear in the UI as params."
      },
      {
        "id": "C",
        "text": "mlflow.log_metrics()",
        "is_correct": false,
        "explanation": "log_metrics() is used for numerical performance values, not hyperparameters."
      },
      {
        "id": "D",
        "text": "mlflow.set_tag()",
        "is_correct": false,
        "explanation": "Tags are for metadata, not for parameters or metrics."
      }
    ],
    "feedback": "Use mlflow.log_params() to efficiently log multiple hyperparameters from a dictionary structure.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_038",
    "type": "multiple_choice_single_answer",
    "question": "You want to visualize model predictions versus true labels in Azure ML and include the plot in your MLflow run. Which function should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.log_metric()",
        "is_correct": false,
        "explanation": "log_metric() is for logging scalar numerical values, not plots or images."
      },
      {
        "id": "B",
        "text": "mlflow.set_tag()",
        "is_correct": false,
        "explanation": "Tags are for attaching metadata to runs, not for logging files."
      },
      {
        "id": "C",
        "text": "mlflow.log_image()",
        "is_correct": true,
        "explanation": "log_image() is the correct method to log image files such as plots to MLflow."
      },
      {
        "id": "D",
        "text": "mlflow.log_artifact()",
        "is_correct": false,
        "explanation": "While it can log files, it doesn’t directly display images in the MLflow UI as conveniently as log_image()."
      }
    ],
    "feedback": "Use mlflow.log_image() to log plots like prediction vs. label comparisons into your experiment tracking.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_039",
    "type": "multiple_choice_single_answer",
    "question": "You are orchestrating a training pipeline in Azure ML and want to reuse components across multiple pipelines. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Define each component inline in the pipeline YAML.",
        "is_correct": false,
        "explanation": "Defining components inline reduces reusability across different pipelines."
      },
      {
        "id": "B",
        "text": "Store components in separate YAML files and register them.",
        "is_correct": true,
        "explanation": "Storing components in separate YAML files allows you to register and reuse them across multiple pipelines."
      },
      {
        "id": "C",
        "text": "Use mlflow.register_component().",
        "is_correct": false,
        "explanation": "This function does not exist; MLflow does not manage Azure ML components."
      },
      {
        "id": "D",
        "text": "Use component.set_reuse(True)",
        "is_correct": false,
        "explanation": "This method does not exist in Azure ML SDK v2."
      }
    ],
    "feedback": "For component reuse, define and register components via standalone YAML files in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_040",
    "type": "multiple_choice_single_answer",
    "question": "You are using MLflow in Azure Machine Learning to track a training run. You want to record the version of a dataset used in the experiment. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "mlflow.set_tag()",
        "is_correct": true,
        "explanation": "mlflow.set_tag() is ideal for storing metadata like dataset versions that are not numeric or model-specific artifacts."
      },
      {
        "id": "B",
        "text": "mlflow.log_metric()",
        "is_correct": false,
        "explanation": "log_metric() is for numeric values and not appropriate for version labels."
      },
      {
        "id": "C",
        "text": "mlflow.log_param()",
        "is_correct": false,
        "explanation": "While log_param could be used, set_tag is preferred for metadata like dataset versions."
      },
      {
        "id": "D",
        "text": "mlflow.log_dict()",
        "is_correct": false,
        "explanation": "log_dict() stores structured data, not a single string value like a dataset version."
      }
    ],
    "feedback": "Use mlflow.set_tag() to store metadata such as dataset version identifiers for better experiment traceability.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_041",
    "type": "multiple_choice_single_answer",
    "question": "You need to load an MLTable dataset registered in your Azure ML workspace using Python SDK v2. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.data.get(name='my_dataset')",
        "is_correct": true,
        "explanation": "This is the correct way to retrieve a registered dataset using the MLClient object in SDK v2."
      },
      {
        "id": "B",
        "text": "Dataset.File.from_files()",
        "is_correct": false,
        "explanation": "This method is from SDK v1 and is not used in SDK v2."
      },
      {
        "id": "C",
        "text": "mlflow.load_table()",
        "is_correct": false,
        "explanation": "This method does not exist in MLflow or Azure ML."
      },
      {
        "id": "D",
        "text": "load_dataset('my_dataset')",
        "is_correct": false,
        "explanation": "There is no generic function called load_dataset() in the SDK v2."
      }
    ],
    "feedback": "Use `ml_client.data.get()` to retrieve and use MLTable datasets in your SDK v2 workflows.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_042",
    "type": "multiple_choice_single_answer",
    "question": "You are creating a YAML definition for a command component in Azure ML SDK v2. Which section is required to specify the code that should be executed?",
    "options": [
      {
        "id": "A",
        "text": "entry_script",
        "is_correct": false,
        "explanation": "entry_script is a setting used inside environments, but not in component YAML directly."
      },
      {
        "id": "B",
        "text": "command",
        "is_correct": true,
        "explanation": "The `command` field defines what is executed when the component runs."
      },
      {
        "id": "C",
        "text": "script",
        "is_correct": false,
        "explanation": "script is not a valid field in component YAML schema."
      },
      {
        "id": "D",
        "text": "run",
        "is_correct": false,
        "explanation": "run is not a valid field in YAML definitions for components."
      }
    ],
    "feedback": "The `command` field in component YAML defines the script or shell command executed in Azure ML SDK v2.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_043",
    "type": "multiple_choice_single_answer",
    "question": "You need to create a reusable environment for multiple jobs in Azure Machine Learning SDK v2. What should you do?",
    "options": [
      {
        "id": "A",
        "text": "Define the environment inline inside each job YAML",
        "is_correct": false,
        "explanation": "This creates duplication and makes reuse difficult across jobs."
      },
      {
        "id": "B",
        "text": "Create the environment once and register it in the workspace",
        "is_correct": true,
        "explanation": "Registering the environment allows referencing it by name/version across different jobs and components."
      },
      {
        "id": "C",
        "text": "Embed the environment in the compute cluster definition",
        "is_correct": false,
        "explanation": "Environments and compute are configured separately in Azure ML."
      },
      {
        "id": "D",
        "text": "Use mlflow.environment.create()",
        "is_correct": false,
        "explanation": "There is no such MLflow method. Environment management is handled by Azure ML SDK."
      }
    ],
    "feedback": "For better maintainability, define and register environments in Azure ML once and reuse by reference.",
    "sdk_version": "v2",
    "include_in_bank": true
  }, 
  {
    "question_id": "dp100_044",
    "type": "multiple_choice_single_answer",
    "question": "You need to reference a component stored in a YAML file when building a pipeline in Azure ML SDK v2. Which method should you use?",
    "options": [
      {
        "id": "A",
        "text": "ml_client.load_yaml_component()",
        "is_correct": false,
        "explanation": "There is no such method in the Azure ML SDK v2."
      },
      {
        "id": "B",
        "text": "load_component(source='component.yml')",
        "is_correct": true,
        "explanation": "The load_component function loads a component from a YAML file for use in pipelines."
      },
      {
        "id": "C",
        "text": "mlflow.load_component()",
        "is_correct": false,
        "explanation": "MLflow does not manage Azure ML pipeline components."
      },
      {
        "id": "D",
        "text": "register_component(source='component.yml')",
        "is_correct": false,
        "explanation": "register_component is not a valid method in SDK v2."
      }
    ],
    "feedback": "Use load_component() to load and reuse YAML-defined components when constructing pipelines in Azure ML.",
    "sdk_version": "v2",
    "include_in_bank": true
  },
  {
    "question_id": "dp100_045",
    "type": "multiple_choice_single_answer",
    "question": "You want to visualize data drift over time using Azure ML’s monitoring capabilities. What must be configured in the MonitorSchedule?",
    "options": [
      {
        "id": "A",
        "text": "primary_metric",
        "is_correct": false,
        "explanation": "primary_metric is used in training jobs, not in monitoring configuration."
      },
      {
        "id": "B",
        "text": "data_drift_signal",
        "is_correct": true,
        "explanation": "data_drift_signal is used within the MonitorSchedule to define how drift is calculated and detected."
      },
      {
        "id": "C",
        "text": "compute_target",
        "is_correct": false,
        "explanation": "compute_target is related to job execution, not drift monitoring."
      },
      {
        "id": "D",
        "text": "model_explanation_config",
        "is_correct": false,
        "explanation": "This config relates to interpretability and not drift monitoring."
      }
    ],
    "feedback": "To detect and visualize data drift over time in Azure ML, configure the `data_drift_signal` within a MonitorSchedule.",
    "sdk_version": "v2",
    "include_in_bank": true
  }
  
  


      
    ]