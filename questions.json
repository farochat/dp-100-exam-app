[
    {
        "question_id": "1",
        "type": "multiple_choice_single_answer",
        "question": "You trained a machine learning model\n\n* You must deploy the model as a real-time inference service for testing\n\n* The service requires low CPU utilization and less than 48 MB of RAM\n\n* The compute target for the deployed service must initialize automatically while minimizing cost and administrative overhead\n\n**Which compute target should you use?**",
        "options": [
            {
                "id": "A",
                "text": "Azure Machine Learning compute cluster",
                "is_correct": false,
                "explanation": "Azure Machine Learning compute cluster is more suited for training machine learning models and running batch inference jobs rather than real-time inference services. It may not be the most cost-effective or efficient option for a service that requires low CPU utilization and minimal memory usage."
            },
            {
                "id": "B",
                "text": "Azure Container Instance (ACI)",
                "is_correct": true,
                "explanation": "Azure Container Instance (ACI) is the correct choice for this scenario as it allows you to deploy containers quickly without managing the underlying infrastructure. ACI is lightweight, cost-effective, and can be automatically scaled based on demand, making it ideal for real-time inference services with low CPU and memory requirements."
            },
            {
                "id": "C",
                "text": "Azure Kubernetes Service (AKS) inference cluster",
                "is_correct": false,
                "explanation": "Azure Kubernetes Service (AKS) inference cluster is designed for managing containerized applications and services at scale. While it provides scalability and flexibility, it may introduce unnecessary complexity and administrative overhead for a simple real-time inference service with low resource requirements."
            },
            {
                "id": "D",
                "text": "Attached Azure Databricks cluster",
                "is_correct": false,
                "explanation": "Using an Attached Azure Databricks cluster is not the best option for this scenario as it is more suitable for distributed data engineering and machine learning workloads. It may not be cost-effective or efficient for deploying a real-time inference service with low CPU and memory requirements."
            }
        ],
        "feedback": "ACI is the best option for this scenario due to its simplicity and cost-effectiveness."
    },
    {
        "question_id": "2",
        "type": "multiple_choice_single_answer",
        "question": "You are planning to make use of Azure Machine Learning designer to train models\n\nYou need choose a suitable compute type\n\nYou recommend Compute cluster\n\n**Will the requirements be satisfied?**",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": "Azure Machine Learning Designer requires a compute cluster to execute training pipelines. Compute clusters are scalable and can be automatically provisioned, making them the correct choice for this scenario."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": "A compute cluster is actually the recommended and supported compute type for training models in Azure ML Designer. Selecting 'No' would be incorrect."
            }
        ],
        "feedback": "Azure ML Designer runs training jobs using compute clusters, which are ideal for parallelized and scalable training tasks."
    },
    {
        "question_id": "3",
        "type": "multiple_choice_single_answer",
        "question": "After training a model, it is important to evaluate its performance\n\nThere are many performance metrics and methodologies for evaluating how well a model makes predictions\n\nWhich evaluation technique is best described by:\n\n>A metric between 0 and 1 based on the square of the differences between predicted and true values\n\nThe closer to 0 this metric is, the better the model is performing\n\nBecause this metric is relative, it can be used to compare models where the labels are in different units",
        "options": [
            {
                "id": "A",
                "text": "Coefficient of Determination (R2)",
                "is_correct": false,
                "explanation": "R2 measures the proportion of variance explained by the model, but does not provide a relative error metric in the 0-1 range as described."
            },
            {
                "id": "B",
                "text": "Root Mean Squared Error (RMSE)",
                "is_correct": false,
                "explanation": "RMSE gives absolute error in the same units as the target variable, but is not scaled between 0 and 1 or considered relative."
            },
            {
                "id": "C",
                "text": "Mean Absolute Error (MAE)",
                "is_correct": false,
                "explanation": "MAE measures average absolute difference between prediction and truth, but is not a relative metric or scaled between 0 and 1."
            },
            {
                "id": "D",
                "text": "Relative Squared Error (RSE)",
                "is_correct": true,
                "explanation": "Relative Squared Error (RSE) is a metric that calculates the relative performance of a model based on the square of the differences between predicted and true values. It provides a metric between 0 and 1, where lower values indicate better model performance, and it can be used to compare models with labels in different units."
            }
        ],
        "feedback": "RSE is useful when comparing model performance across datasets with different scales or units."
    },
    {
        "question_id": "4",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following represent specific data files or tables that you plan to work with within Azure ML?",
        "options": [
            {
                "id": "A",
                "text": "Datasets",
                "is_correct": true,
                "explanation": "Datasets are specific data files or tables that you work with in Azure ML. They contain the structured or unstructured data that you will use to train machine learning models."
            },
            {
                "id": "B",
                "text": "Rows",
                "is_correct": false,
                "explanation": "Rows are the horizontal structures within a dataset that represent individual data points or observations. While crucial for data analysis, they are not standalone data files or tables in Azure ML."
            },
            {
                "id": "C",
                "text": "Headers",
                "is_correct": false,
                "explanation": "Headers represent the column names or feature identifiers in a dataset. They are part of a dataset structure but not standalone files or tables in Azure ML."
            },
            {
                "id": "D",
                "text": "Columns",
                "is_correct": false,
                "explanation": "Columns refer to the vertical structures within a dataset that represent specific variables or features. While important for data analysis, they are not standalone data files or tables in Azure ML."
            },
            {
                "id": "E",
                "text": "Environments",
                "is_correct": false,
                "explanation": "Environments in Azure ML refer to the configurations and dependencies needed to run machine learning experiments. They are not specific data files or tables that you work with in Azure ML."
            },
            {
                "id": "F",
                "text": "VMs",
                "is_correct": false,
                "explanation": "VMs (Virtual Machines) provide compute power but are not data files or tables. They are part of the infrastructure used to process data."
            }
        ],
        "feedback": "In Azure ML, datasets are the objects that represent the actual data used for training and evaluation."
    },
    {
        "question_id": "5",
        "type": "multiple_choice_single_answer",
        "question": "You are solving a classification task\n\nYou must evaluate your model on a limited data sample by using k-fold cross-validation\n\nYou start by configuring a k parameter as the number of splits\n\nYou need to configure the k parameter for the cross-validation\n\n**Which value should you use?**",
        "options": [
            {
                "id": "A",
                "text": "K=0.5",
                "is_correct": false,
                "explanation": "K must be a positive integer representing the number of splits. A decimal value like 0.5 is invalid for k-fold cross-validation."
            },
            {
                "id": "B",
                "text": "K=0.01",
                "is_correct": false,
                "explanation": "K must be an integer greater than 1. A value of 0.01 is invalid for defining the number of folds in cross-validation."
            },
            {
                "id": "C",
                "text": "K=1",
                "is_correct": false,
                "explanation": "K=1 would not perform any splitting, which makes it unsuitable for cross-validation. Minimum valid value is 2."
            },
            {
                "id": "D",
                "text": "K=5",
                "is_correct": true,
                "explanation": "K=5 is a commonly used and valid setting for k-fold cross-validation. It provides a good balance between bias and variance, especially when working with limited data."
            }
        ],
        "feedback": "In k-fold cross-validation, k must be an integer \u22652. A value of 5 is standard and recommended in many cases."
    },
    {
        "question_id": "6",
        "type": "multiple_choice_single_answer",
        "question": "You plan to use hyperparameter tuning to find optimal discrete values for a set of hyperparameters\n\nYou want to try every possible combination of a set of specified discrete values\n\n**Which kind of sampling should you use?**",
        "options": [
            {
                "id": "A",
                "text": "Bayesian Sampling",
                "is_correct": false,
                "explanation": "Bayesian Sampling uses past evaluation results to decide which hyperparameter values to try next, aiming to find the optimal values more efficiently. However, it does not ensure that every possible combination of discrete values will be explored, making it less suitable for exhaustive grid search."
            },
            {
                "id": "B",
                "text": "Grid Sampling",
                "is_correct": true,
                "explanation": "Grid Sampling is the correct choice for trying every possible combination of specified discrete values for hyperparameters. It systematically explores all combinations in a grid-like fashion, ensuring that no combination is missed during the hyperparameter tuning process."
            },
            {
                "id": "C",
                "text": "Random Sampling",
                "is_correct": false,
                "explanation": "Random Sampling involves randomly selecting combinations of hyperparameter values from the specified discrete values. While it can be effective in some cases, it does not guarantee that every possible combination will be tried, which is essential for grid search hyperparameter tuning."
            }
        ],
        "feedback": "Grid Sampling is used when exhaustive exploration of all discrete combinations is required."
    },
    {
        "question_id": "7",
        "type": "multiple_choice_single_answer",
        "question": "You have trained a model using a dataset containing data that was collected last year\n\n* As this year progresses, you will collect new data\n\n* You want to track any changing data trends that might affect the performance of the model\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Collect the new data in a new version of the existing training dataset, and profile both datasets.",
                "is_correct": false,
                "explanation": "Creating a new version of the training dataset does not support effective tracking of data drift, as this approach does not isolate the datasets for comparison."
            },
            {
                "id": "B",
                "text": "Replace the training dataset with a new dataset that contains both the original training data and the new data.",
                "is_correct": false,
                "explanation": "By combining all data into a single dataset, you lose the ability to isolate the impact of the new data, making it harder to detect data drift accurately."
            },
            {
                "id": "C",
                "text": "Collect the new data in a separate dataset and create a Data Drift Monitor with the training dataset as a baseline and the new dataset as a target.",
                "is_correct": true,
                "explanation": "This is the correct approach. Data Drift Monitor allows you to compare two datasets and detect any significant changes in the distribution of data over time."
            }
        ],
        "feedback": "Collecting the new data in a separate dataset and creating a Data Drift Monitor with the training dataset as a baseline and the new dataset as a target is the correct approach. This method allows you to compare the performance of the model with the original training data against the new data, enabling you to track any changing data trends that might affect the model's performance. This functionality is natively supported in Azure ML SDK v2 using `azure.ai.ml.monitor.DataDriftSignal` and `MonitorSchedule`."
    },
    {
        "question_id": "8",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning designer to create a training pipeline for a binary classification model\n\nYou have added a dataset containing features and labels, a Two-Class Decision Forest module, and a Train Model module\n\nYou plan to use Score Model and Evaluate Model modules to test the trained model with a subset of the dataset that was not used for training\n\nWhich additional kind of module should you add?",
        "options": [
            {
                "id": "A",
                "text": "Join Data",
                "is_correct": false,
                "explanation": "Join Data is used to merge datasets. However, in this case, you need to split data for training/testing, not merge datasets."
            },
            {
                "id": "B",
                "text": "Select Columns in Dataset",
                "is_correct": false,
                "explanation": "While column selection is part of data preprocessing, it is not the most immediate step required to enable evaluation. Splitting the data is more critical at this stage."
            },
            {
                "id": "C",
                "text": "Clustering",
                "is_correct": false,
                "explanation": "Clustering is used for unsupervised learning. This scenario is for a supervised binary classification task."
            },
            {
                "id": "D",
                "text": "Split Data",
                "is_correct": true,
                "explanation": "Split Data is essential for dividing the dataset into training and testing subsets. This is crucial to evaluate the model\u2019s performance on unseen data and prevent overfitting."
            }
        ],
        "sdk_version": "v2",
        "feedback": "Esta pregunta hace parte del ecosistema de Azure Machine Learning SDK v2, espec\u00edficamente usando el entorno visual 'Azure Machine Learning designer'. Aunque no requiere directamente c\u00f3digo en Python, el m\u00f3dulo 'Split Data' es parte fundamental del proceso de entrenamiento y evaluaci\u00f3n. En Azure ML Designer (y en trabajos definidos con SDK v2), dividir los datos permite entrenar con una porci\u00f3n del dataset y evaluar con otra, asegurando una estimaci\u00f3n honesta del rendimiento del modelo. Este paso es obligatorio para evitar overfitting y se integra con m\u00f3dulos como 'Score Model' y 'Evaluate Model'. Referencia oficial: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-designer",
        "correct_option": "D"
    },
    {
        "question_id": "9",
        "type": "multiple_choice_single_answer",
        "question": "Peter plans to have the IT team run the Hyperopt function `fmin()`\n\nWhich arguments are needed to run this function?",
        "options": [
            {
                "id": "A",
                "text": "The evaluation metric, the model, and the data.",
                "is_correct": false,
                "explanation": "These are general ML pipeline components but are not the specific arguments required by the `fmin()` function in Hyperopt."
            },
            {
                "id": "B",
                "text": "The evaluation metric, the model, objective function, and the model.",
                "is_correct": false,
                "explanation": "This option redundantly mentions the model and includes irrelevant elements like the evaluation metric. `fmin()` requires an objective function, search space, and search algorithm."
            },
            {
                "id": "C",
                "text": "The objective function, the search space, and the model.",
                "is_correct": false,
                "explanation": "Although this option includes two of the required arguments, it misses the `algo` parameter, which defines the search algorithm to be used."
            },
            {
                "id": "D",
                "text": "The objective function, the search space, and the search algorithm.",
                "is_correct": true,
                "explanation": "These are the required parameters for the `fmin()` function: an objective function to minimize, a search space defining the hyperparameters, and a search algorithm (e.g., `tpe.suggest`, `random.suggest`)."
            }
        ],
        "explanation": "To use the Hyperopt `fmin()` function effectively, three arguments are mandatory: `fn` (the objective function), `space` (the search space), and `algo` (the search algorithm). This function is part of the Hyperopt library, typically used in hyperparameter tuning workflows where the goal is to minimize a loss function by evaluating different hyperparameter configurations.\n\nFor example:\n```python\nfrom hyperopt import fmin, tpe, hp\n\ndef objective(params):\n    loss = some_model_training_and_evaluation(params)\n    return loss\n\nbest = fmin(fn=objective, space={'lr': hp.uniform('lr', 0.001, 0.1)}, algo=tpe.suggest, max_evals=100)\n```\nThis function is **not** part of the Azure Machine Learning Python SDK v2, but can be used **within** an AzureML pipeline or job by embedding it into a script step. The logic behind this question is important to understand automated hyperparameter tuning when designing custom training logic outside of built-in AutoML.",
        "sdk_version": "not part of Azure SDK v2",
        "exclude_from_exam": true
    },
    {
        "question_id": "11",
        "type": "multiple_choice_single_answer",
        "question": "You have used the Python SDK for Azure Machine Learning to create a pipeline that trains a model\n\nWhat do you need to do so that a client application can invoke the pipeline through an HTTP REST endpoint?",
        "options": [
            {
                "id": "A",
                "text": "Create an inference cluster compute target.",
                "is_correct": false,
                "explanation": "Creating an inference cluster compute target is not directly related to enabling a client application to invoke the pipeline through an HTTP REST endpoint. An inference cluster is typically used for model deployment, not for invoking a pipeline."
            },
            {
                "id": "B",
                "text": "Publish the pipeline.",
                "is_correct": true,
                "explanation": "Publishing the pipeline is required to make it accessible via a REST endpoint. Once published, the pipeline can be invoked programmatically using an HTTP POST request, enabling external applications to run the pipeline remotely."
            },
            {
                "id": "C",
                "text": "Rename the pipeline to pipeline_name-production.",
                "is_correct": false,
                "explanation": "Renaming a pipeline does not affect its accessibility via a REST endpoint. Only publishing it exposes it as an HTTP triggerable service."
            }
        ],
        "feedback": "In Azure ML SDK v2, to make a pipeline available for external invocation, it must be published using `ml_client.jobs.create_or_update()` followed by `ml_client.jobs.publish()` or by specifying `is_published=True` in the job definition YAML. This step registers the pipeline as a REST endpoint, allowing it to be triggered by client apps or services. Without publishing, the pipeline is only available internally and cannot be accessed through an HTTP endpoint. Referencia: [Microsoft Docs - Publish a pipeline](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-machine-learning-pipelines-v2#publish-a-pipeline)."
    },
    {
        "question_id": "12",
        "type": "multiple_choice_single_answer",
        "question": "You are building a recurrent neural network to perform a binary classification\n\nThe training loss, validation loss, training accuracy, and validation accuracy of each training epoch has been provided\n\nYou need to identify whether the classification model is overfitted or not\n\nWhich of the following is would indicate overfitting?",
        "options": [
            {
                "id": "A",
                "text": "The training loss increases while the validation loss decreases when training the model",
                "is_correct": false,
                "explanation": "This would indicate model underfitting, where the model fails to learn from training data."
            },
            {
                "id": "B",
                "text": "The training loss stays constant and the validation loss decreases when training the model",
                "is_correct": false,
                "explanation": "This suggests stable performance and generalization, but does not indicate overfitting."
            },
            {
                "id": "C",
                "text": "The training loss stays constant and the validation loss stays on a constant value and close to the training loss value when training the model",
                "is_correct": false,
                "explanation": "This indicates that the model is not improving over time and might lack complexity, which implies underfitting rather than overfitting."
            },
            {
                "id": "D",
                "text": "The training loss decreases while the validation loss increases when training the model",
                "is_correct": true,
                "explanation": "This pattern clearly shows overfitting: the model fits the training data increasingly well, but performs worse on unseen validation data."
            }
        ],
        "feedback": "Overfitting occurs when a model learns patterns specific to the training data and fails to generalize to unseen data. In this question, the training loss is decreasing (which means the model is learning well on the training data), but the validation loss increases (meaning the model is not performing well on new, unseen data). This divergence between training and validation losses is a key indicator of overfitting. In real-world scenarios using Azure Machine Learning SDK v2, this kind of monitoring is done through metrics logging in training jobs using MLflow or through visualizations in Azure ML Studio. Techniques to mitigate overfitting include early stopping, regularization, and increasing training data or using data augmentation.",
        "correct_option": "D"
    },
    {
        "question_id": "13",
        "type": "multiple_choice_single_answer",
        "question": "The Eat-More Corporation is a US-based fast-food restaurant chain headed by Teresa Payton\n\nThey are building a team data science environment\n\n**Requirements include**: \n* models must be built using Caffe2 or Chainer\n* work in disconnected environments\n* allow pipeline updates when online\n\nWhat environment should they use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": "Azure Databricks offers powerful data processing capabilities, but it does not support training with Caffe2 or Chainer directly, nor is it designed to operate in disconnected environments or on personal devices."
            },
            {
                "id": "B",
                "text": "Azure Kubernetes Service (AKS)",
                "is_correct": false,
                "explanation": "AKS is useful for deployment and scaling containerized ML models, but it lacks the full flexibility needed for offline development or custom training environments on personal devices."
            },
            {
                "id": "C",
                "text": "Azure Machine Learning Designer",
                "is_correct": false,
                "explanation": "Azure ML Designer is a low-code tool and does not support custom frameworks like Caffe2 or Chainer. It also cannot run on disconnected devices or enable offline training."
            },
            {
                "id": "D",
                "text": "Azure Machine Learning Service",
                "is_correct": true,
                "explanation": "Azure Machine Learning Service supports custom training scripts using frameworks like Caffe2 or Chainer, and it enables local experimentation on personal devices with offline support and syncing when reconnected."
            }
        ],
        "feedback": "La opci\u00f3n correcta es **Azure Machine Learning Service** porque permite crear entornos personalizados para entrenamiento de modelos ML utilizando cualquier framework compatible con Python, incluidos Caffe2 o Chainer. A diferencia de otros servicios como Azure ML Designer (limitado a m\u00f3dulos predefinidos) o Azure Databricks (enfocado a Spark y big data), el servicio de Azure ML completo soporta ejecuci\u00f3n local, sincronizaci\u00f3n en la nube y desarrollo offline. Esto lo hace ideal para cient\u00edficos de datos que trabajan en sus propios dispositivos, con necesidad de operar tanto en red como sin conexi\u00f3n, y que luego deben sincronizar sus pipelines o modelos en la nube. Este enfoque es compatible con el SDK v2, donde puedes usar entornos personalizados (`Environment`), pipelines (`PipelineJob`), y programaci\u00f3n distribuida cuando est\u00e9s en l\u00ednea nuevamente."
    },
    {
        "question_id": "14",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a new Azure Machine Learning pipeline using the designer\n\nThe pipeline must train a model using data in a comma-separated values (CSV) file that is published on a website\n\nYou have not created a dataset for this file\n\nYou need to ingest the data from the CSV file into the designer pipeline using the minimal administrative effort\n\nWhich module should you add to the pipeline in Designer?",
        "options": [
            {
                "id": "A",
                "text": "Dataset",
                "is_correct": false,
                "explanation": "The 'Dataset' option requires you to register the dataset manually in Azure ML before using it. Since the question emphasizes minimal administrative effort, this option is not the most efficient for the task."
            },
            {
                "id": "B",
                "text": "Enter Data Manually",
                "is_correct": false,
                "explanation": "Entering data manually is impractical for a CSV file and not suitable for automating data ingestion into a pipeline."
            },
            {
                "id": "C",
                "text": "Import Data",
                "is_correct": true,
                "explanation": "The 'Import Data' module is the correct choice for ingesting data from an external CSV file into the Azure Machine Learning pipeline designer. This module allows you to import data from various sources, including CSV files, with minimal administrative effort, making it the suitable option for this scenario."
            },
            {
                "id": "D",
                "text": "Convert to CSV",
                "is_correct": false,
                "explanation": "The 'Convert to CSV' module is used to convert data into CSV format, not for ingesting an already existing CSV file from an external location."
            }
        ],
        "correct_answer": "C",
        "feedback": "The 'Import Data' module in Azure Machine Learning Designer is designed for bringing external data\u2014such as CSV files hosted on websites\u2014into your pipeline without the need to first register it manually as a dataset. This greatly reduces setup complexity and supports scenarios where agility and minimal overhead are required. This approach is recommended when dealing with simple data ingestion pipelines, especially in early prototyping or when dynamically sourcing data. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data"
    },
    {
        "question_id": "15",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a real-time inference endpoint using the Azure ML SDK v2\n\nYou want to emit custom metrics (e.g latency or prediction count) from your scoring script and analyze them using Application Insights for monitoring purposes\n\nWhat should you do inside the scoring script?",
        "options": [
            {
                "id": "A",
                "text": "Use mlflow.log_metric() to log the custom metrics.",
                "is_correct": false,
                "explanation": "mlflow.log_metric() is used during training jobs to log experiment metrics. It is not used within the scoring script of a deployed endpoint."
            },
            {
                "id": "B",
                "text": "Use print statements or logging to STDOUT in the scoring script.",
                "is_correct": true,
                "explanation": "STDOUT output from the scoring script is captured by Azure ML and automatically routed to Application Insights. This is the recommended way to log telemetry during inference."
            },
            {
                "id": "C",
                "text": "Save metrics in a local file inside the /outputs directory.",
                "is_correct": false,
                "explanation": "Saving to the /outputs directory is used for artifacts in training jobs, not telemetry in inference endpoints."
            },
            {
                "id": "D",
                "text": "Use run.log() from azureml.core.Run inside the script.",
                "is_correct": false,
                "explanation": "run.log() is a v1 SDK method used in training scripts, and is not supported in the SDK v2 or in deployed endpoints."
            }
        ],
        "explanation": "In SDK v2, when deploying a model as a real-time endpoint, the proper way to emit telemetry (such as latency or prediction statistics) is by writing to STDOUT using print() or logging. Azure automatically collects these logs and sends them to Application Insights, allowing you to monitor and analyze them for health checks and diagnostics.",
        "sdk_version": "v2"
    },
    {
        "question_id": "16",
        "type": "multiple_choice_single_answer",
        "question": "You deploy a real-time inference service for a trained model\n\nThe deployed model supports a business-critical application, and it is important to be able to monitor the data submitted to the web service and the predictions the data generates\n\nYou need to implement a monitoring solution for the deployed model using minimal administrative effort\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "View the log files generated by the experiment used to train the model.",
                "is_correct": false,
                "explanation": "Training experiment logs provide insights into the training process but do not allow monitoring of real-time inference requests or outputs once the model is deployed."
            },
            {
                "id": "B",
                "text": "View the explanations for the registered model in Azure ML studio.",
                "is_correct": false,
                "explanation": "Model explanations help you interpret model predictions, but they are not intended for monitoring the web service's request/response flow or telemetry."
            },
            {
                "id": "C",
                "text": "Create an ML Flow tracking URI that references the endpoint, and view the data logged by ML Flow.",
                "is_correct": false,
                "explanation": "MLflow logging is effective during training and experimentation. However, for real-time model deployment monitoring, Application Insights is the preferred minimal-effort solution."
            },
            {
                "id": "D",
                "text": "Enable Azure Application Insights for the service endpoint and view logged data in the Azure portal.",
                "is_correct": true,
                "explanation": "Azure Application Insights is the recommended way to monitor real-time endpoints in Azure Machine Learning. It captures request/response telemetry and logs automatically when enabled, with minimal administrative effort."
            }
        ],
        "feedback": "To monitor real-time inferencing services deployed in Azure Machine Learning, the best practice is to enable Azure Application Insights on the endpoint. This allows telemetry such as request count, latency, and errors to be captured and viewed in the Azure portal. This monitoring helps ensure the health and performance of business-critical ML applications with minimal setup."
    },
    {
        "question_id": "17",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist of your company and are asked implementing a machine learning model to predict stock prices\n\nThe model uses a PostgreSQL database and requires GPU processing\n\nYou need to create a virtual machine that is pre-configured with the required tools\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a Geo AI Data Science Virtual Machine (Geo-DSVM) Windows edition.",
                "is_correct": false,
                "explanation": "Geo-DSVMs are tailored for geospatial analytics and not designed for GPU-intensive deep learning tasks."
            },
            {
                "id": "B",
                "text": "Create a Deep Learning Virtual Machine (DLVM) Linux edition.",
                "is_correct": true,
                "explanation": "The DLVM Linux edition is optimized for deep learning workloads and comes pre-configured with GPU support and popular frameworks. Linux also offers better compatibility and performance for GPU tasks."
            },
            {
                "id": "C",
                "text": "Create a Data Science Virtual Machine (DSVM) Windows edition.",
                "is_correct": false,
                "explanation": "While DSVM is a good general-purpose option, it is not optimized for deep learning tasks that require GPU acceleration."
            },
            {
                "id": "D",
                "text": "Create a Deep Learning Virtual Machine (DLVM) Windows edition.",
                "is_correct": false,
                "explanation": "The Windows edition of DLVM is less commonly used for deep learning due to compatibility and performance issues with GPU frameworks compared to Linux."
            }
        ],
        "feedback": "The DLVM Linux edition is the recommended choice for deep learning workloads involving GPU acceleration. It supports major frameworks like TensorFlow, PyTorch, and others, and is optimized for use in research and production environments. Linux provides superior driver and hardware compatibility, making it ideal for intensive ML tasks such as stock price prediction."
    },
    {
        "question_id": "18",
        "type": "multiple_choice_single_answer",
        "question": "You have been tasked with employing a machine learning model, which makes use of a PostgreSQL database and needs GPU processing, to forecast prices\n\nYou are preparing to create a virtual machine that has the necessary tools built into it\n\nYou need to make use of the correct virtual machine type\n\nRecommendation: You make use of a Geo AI Data Science Virtual Machine (Geo-DSVM) Windows edition\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Geo-DSVMs are designed primarily for geospatial analytics and do not provide the necessary GPU support or tooling for deep learning tasks like time series forecasting using PostgreSQL data."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Geo-DSVM is not suitable for GPU-based deep learning workloads. A better alternative would be the Deep Learning Virtual Machine (DLVM) Linux edition, which includes GPU drivers and frameworks optimized for machine learning tasks."
            }
        ],
        "feedback": "The Geo AI Data Science Virtual Machine is intended for geospatial tasks and lacks optimized support for GPU-accelerated deep learning. For forecasting models requiring PostgreSQL integration and GPU support, a DLVM Linux edition is more appropriate. It includes the necessary drivers, libraries, and frameworks for such ML scenarios."
    },
    {
        "question_id": "19",
        "type": "multiple_choice_single_answer",
        "question": "True or False: Ordinal encoding is often the recommended approach, and it involves transforming each categorical value into n (= number of categories) binary values, with one of them 1, and all others 0\n\n",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "This statement describes One-Hot Encoding, not Ordinal Encoding. Ordinal Encoding assigns a unique integer to each category, preserving some order (if it exists), while One-Hot Encoding creates n binary columns where each column represents one category."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Correct. The description given corresponds to One-Hot Encoding, not Ordinal Encoding. Ordinal Encoding is typically used when the categorical data has an inherent order, such as 'low', 'medium', 'high'."
            }
        ],
        "feedback": "It's important to distinguish between One-Hot Encoding and Ordinal Encoding. While One-Hot Encoding creates binary variables and is recommended when there is no natural order among categories, Ordinal Encoding is better suited for ordered categories. Misapplying these encodings can lead to incorrect model assumptions and degraded performance."
    },
    {
        "question_id": "20",
        "type": "multiple_choice_single_answer",
        "question": "You create a deep learning model for image recognition on Azure Machine Learning service using GPU-based training\n\nYou must deploy the model to a context that allows for real-time GPU-based inferencing\n\nYou need to configure compute resources for model inferencing\n\nWhich compute type should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Kubernetes Service",
                "is_correct": true,
                "explanation": "Azure Kubernetes Service (AKS) supports GPU-based inferencing and is designed for scalable, real-time deployment of containerized models. It is ideal for scenarios that require real-time GPU inference such as deep learning."
            },
            {
                "id": "B",
                "text": "Machine Learning Compute",
                "is_correct": false,
                "explanation": "This resource is optimized for training workloads and not ideal for real-time inferencing, especially when GPU support is required."
            },
            {
                "id": "C",
                "text": "Azure Container Instance",
                "is_correct": false,
                "explanation": "ACI does not support GPU-based inference, making it unsuitable for deploying deep learning models that require such compute capabilities."
            },
            {
                "id": "D",
                "text": "Field Programmable Gate Array",
                "is_correct": false,
                "explanation": "FPGAs are not the standard or most practical option for real-time deployment of deep learning models requiring GPU-based inferencing."
            }
        ],
        "feedback": "AKS is the preferred deployment target for real-time GPU inference in Azure Machine Learning because it provides the scalability, container management, and GPU support necessary for production-level deep learning models. Refer to: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where for details on compute targets."
    },
    {
        "question_id": "21",
        "type": "multiple_choice_single_answer",
        "question": "You train and register a machine learning model\n\nYou plan to deploy the model as a real-time web service\n\nApplications must use key-based authentication to use the model\n\nYou need to deploy the web service\n\nSolution:\n- Create an AksWebservice instance\n\n- Set the value of the auth_enabled property to False\n\n- Set the value of the token_auth_enabled property to True\n\n- Deploy the model to the service\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Enabling token-based authentication while disabling key-based auth does not fulfill the requirement. The correct configuration would be `auth_enabled=True` and `token_auth_enabled=False` to use key-based authentication."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "The requirement is key-based authentication. Setting `auth_enabled=False` disables key-based auth. Setting `token_auth_enabled=True` enables token-based authentication. This setup does not meet the requirement for key-based authentication."
            }
        ],
        "feedback": "For key-based authentication on an AKS deployment using Python SDK v1, you must set `auth_enabled=True`. The proposed configuration enables token-based auth instead. Since the scenario requires key-based auth, the solution does **not** meet the goal. Refer to: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where#authentication"
    },
    {
        "question_id": "23",
        "type": "multiple_choice_single_answer",
        "question": "You are a Data Scientist\n\nIs it common practice to train the model using all available data, and then reuse some of the training data to test the results?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Reusing training data for testing leads to biased evaluation results, as the model has already seen this data."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Best practices recommend separating data into training and testing sets to ensure unbiased evaluation of the model\u2019s performance."
            }
        ],
        "feedback": "Reusing training data to evaluate a model causes data leakage, leading to overly optimistic performance metrics. It is a fundamental principle in machine learning to separate training and test sets so that the evaluation reflects how the model performs on unseen data. This practice ensures generalizability and prevents overfitting."
    },
    {
        "question_id": "24",
        "type": "multiple_choice_single_answer",
        "question": "You plan to run a script as an experiment using a Script Run Configuration\n\nThe script uses modules from the scipy library as well as several Python packages that are not typically installed in a default conda environment\n\nYou plan to run the experiment on your local workstation for small datasets and scale out the experiment by running it on more powerful remote compute clusters for larger datasets\n\nYou need to ensure that the experiment runs successfully on local and remote compute with the least administrative effort\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a virtual machine (VM) with the required Python configuration and attach the VM as a compute target. Use this compute target for all experiment runs.",
                "is_correct": false,
                "explanation": "This approach works but requires significantly more administrative effort than necessary."
            },
            {
                "id": "B",
                "text": "Create and register an Environment that includes the required packages. Use this Environment for all experiment runs.",
                "is_correct": true,
                "explanation": "This is the most efficient approach. It allows consistent experiment runs across compute targets with minimal overhead."
            },
            {
                "id": "C",
                "text": "Create a config.yaml file defining the conda packages that are required and save the file in the experiment folder.",
                "is_correct": false,
                "explanation": "Manually maintaining a config.yaml file can lead to inconsistencies and extra effort when reused across experiments."
            },
            {
                "id": "D",
                "text": "Always run the experiment with an Estimator by using the default packages.",
                "is_correct": false,
                "explanation": "The default packages may not include the required libraries like scipy, leading to failures during execution."
            },
            {
                "id": "E",
                "text": "Do not specify an environment in the run configuration for the experiment. Run the experiment by using the default environment.",
                "is_correct": false,
                "explanation": "The default environment is unlikely to include required libraries such as scipy, so this may result in failed runs."
            }
        ],
        "feedback": "Registering an Azure ML Environment is the recommended approach when working with custom dependencies, particularly when experiments must run consistently across different compute targets (e.g., local and remote). By registering the environment, you can version, reuse, and manage it without manually replicating dependencies in each compute node. This approach is efficient, reduces errors due to inconsistent environments, and integrates seamlessly with both ScriptRunConfig and command jobs in Azure ML SDK v2.\n\nIn SDK v2, you typically define an environment in a YAML file or directly in Python using the `Environment` class, and then register it with the ML client:\n\n```python\nfrom azure.ai.ml.entities import Environment\nfrom azure.ai.ml import MLClient\n\nenv = Environment(\n    name=\"my-env\",\n    image=\"mcr.microsoft.com/azureml/sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    conda_file=\"envs/conda.yaml\"\n)\nml_client.environments.create_or_update(env)\n```\n\nOnce created, you can reference this environment in any job. This aligns with best practices for MLOps and reduces administrative burden.\n\n[Learn more](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-environments-v2?tabs=python)"
    },
    {
        "question_id": "25",
        "type": "multiple_choice_single_answer",
        "question": "You are holding a workgroup session and the topic is how to cache data into the memory of the local executor for instant access\n\nWhich of the following is the correct method?",
        "options": [
            {
                "id": "A",
                "text": ".cache().inMemory()",
                "is_correct": false,
                "explanation": "The method .cache().inMemory() does not conform to the standard practice of caching data into the memory of the local executor for instant access in Azure Machine Learning. It appears to mix up the order of operations and does not clearly indicate the caching process."
            },
            {
                "id": "B",
                "text": ".cache()",
                "is_correct": true,
                "explanation": "The method .cache() is the correct approach for caching data into the memory of the local executor for instant access in Azure Machine Learning. It is the standard method used to store data in memory for quick retrieval and processing."
            },
            {
                "id": "C",
                "text": ".inMemory().save()",
                "is_correct": false,
                "explanation": "The method .inMemory().save() is invalid in PySpark or Spark. It does not represent any recognized caching pattern."
            },
            {
                "id": "D",
                "text": ".inMemory().cache()",
                "is_correct": false,
                "explanation": "This method chaining is incorrect. The function .inMemory() does not exist as a callable method for DataFrames."
            },
            {
                "id": "E",
                "text": ".save().inMemory()",
                "is_correct": false,
                "explanation": "This is not valid Spark syntax. Caching is performed with .cache() before an action, not after a save operation."
            }
        ],
        "feedback": "In Apache Spark, using `.cache()` is the standard way to store a DataFrame or RDD in memory on the local executor for faster subsequent actions. Once `.cache()` is applied and an action like `.count()` or `.show()` is triggered, Spark will store the results in memory. This helps reduce recomputation in iterative processes and is commonly used in Azure Synapse, Databricks, and other Spark-based environments.\n\n```python\ndf = spark.read.csv(\"data.csv\")\ndf.cache()\ndf.show()  # triggers caching\n```\n\nCaching is especially useful when you need low-latency access to reused data during training or data transformation stages."
    },
    {
        "question_id": "26",
        "type": "multiple_choice_single_answer",
        "question": "At its core, Azure Machine Learning is a service for training and managing machine learning models, for which you need compute resources on which to run the training process\n\nWhich of the following are cloud-based resources on which you can run model training and data exploration processes?",
        "options": [
            {
                "id": "A",
                "text": "Notebooks",
                "is_correct": false,
                "explanation": "Notebooks are development tools used to write and run code, but they are not the compute resources where training and data exploration are executed."
            },
            {
                "id": "B",
                "text": "Workspaces",
                "is_correct": false,
                "explanation": "Workspaces are logical containers for Azure ML assets (like models, environments, compute), but they are not compute resources themselves."
            },
            {
                "id": "C",
                "text": "Compute targets",
                "is_correct": true,
                "explanation": "Compute targets are cloud-based resources in Azure Machine Learning used to run model training and data exploration workloads. These can include virtual machines, Azure Databricks, or other compute clusters that provide the necessary processing power."
            },
            {
                "id": "D",
                "text": "Workloads",
                "is_correct": false,
                "explanation": "Workloads refer to the tasks or jobs being executed, not the compute infrastructure used to run them."
            }
        ],
        "feedback": "The correct answer is 'Compute targets'. These are the actual cloud-based resources where Azure ML executes training and data exploration code. They can include Azure Machine Learning Compute clusters, virtual machines, or attached compute like Azure Databricks. Notebooks and workspaces support development and organization, but they do not perform execution. Understanding the role of compute targets is essential for effective resource management in cloud-based ML workflows."
    },
    {
        "question_id": "27",
        "type": "multiple_choice_single_answer",
        "question": "Scenario\n\nPym Tech is a US-based Technology manufacturer headed by Hank Pym\n\nTheir headquarters is located at Treasure Island, San Francisco California and business is booming\n\nThe expansion plans are underway which have presented several IT challenges which Hank has contracted you to advise his IT staff on\n\nAt the moment, the topic is building a data engineering and data science development environment with the following requirements\n\nRequired:\n\u2022 Environment to support Python and Scala\n\u2022 Environment to compose data storage, movement, and processing services into automated data pipelines\n\u2022 The same tool should be used for the orchestration of both data engineering and data science\n\u2022 Environment to support workload isolation and interactive workloads\n\u2022 Environment to enable scaling across a cluster of machines\n\nThe team\u2019s task is to create the environment\n\nWhich of the following should they do?",
        "options": [
            {
                "id": "A",
                "text": "Build the environment in Azure Databricks and use Azure Container Instances for orchestration.",
                "is_correct": false,
                "explanation": "Azure Container Instances (ACI) is suitable for lightweight deployments, but it is not designed for complex orchestration of both data engineering and data science workflows. It lacks the integration and orchestration capabilities required for enterprise-level automation."
            },
            {
                "id": "B",
                "text": "Build the environment in Apache Spark for HDInsight and use Azure Container Instances for orchestration.",
                "is_correct": false,
                "explanation": "Although HDInsight with Apache Spark supports Python, Scala, workload isolation, and scaling, using Azure Container Instances for orchestration fails to meet the requirement of using a single tool for both data engineering and data science."
            },
            {
                "id": "C",
                "text": "Build the environment in Apache Hive for HDInsight and use Azure Data Factory for orchestration.",
                "is_correct": false,
                "explanation": "Apache Hive supports some batch processing workloads but is not well suited for building scalable, interactive data science environments or composing complex pipelines. While Azure Data Factory is a good orchestration tool, Hive doesn't fully meet the interactive and ML-specific needs."
            },
            {
                "id": "D",
                "text": "Build the environment in Azure Databricks and use Azure Data Factory for orchestration.",
                "is_correct": true,
                "explanation": "Azure Databricks is a unified analytics platform built on Apache Spark that supports both Python and Scala. It enables interactive workloads, workload isolation, and scalable processing across a cluster. Combined with Azure Data Factory, which orchestrates pipelines for both data engineering and data science, it fulfills all the scenario's requirements."
            }
        ],
        "feedback": "Azure Databricks is designed for modern big data and machine learning workflows. It supports both Python and Scala, enables rapid scaling, and integrates tightly with Azure Data Factory for orchestrating complex data pipelines. This combination is particularly powerful for hybrid workloads (data engineering + data science) and supports modular, automated workflows. Azure Data Factory provides robust orchestration capabilities that can handle ingestion, transformation, and model training tasks. Therefore, choosing Azure Databricks with Azure Data Factory ensures an enterprise-ready, scalable, and integrated solution that fulfills all outlined needs."
    },
    {
        "question_id": "29",
        "type": "multiple_choice_single_answer",
        "question": "Your manager asked you to solve a classification task\n\nThe dataset is imbalanced\n\nYou need to select an Azure Machine Learning Studio module to improve the classification accuracy\n\nWhich module should you use?",
        "options": [
            {
                "id": "A",
                "text": "Fisher Linear Discriminant Analysis",
                "is_correct": false,
                "explanation": "Fisher Linear Discriminant Analysis is a dimensionality reduction technique that aims to find the linear combination of features that best separates different classes. While it can be useful for feature transformation, it does not directly address the issue of imbalanced datasets or improve classification accuracy in such scenarios."
            },
            {
                "id": "B",
                "text": "Filter Based Feature Selection",
                "is_correct": false,
                "explanation": "Filter Based Feature Selection is not directly related to addressing imbalanced datasets. It is used to select the most relevant features for the model based on statistical measures, but it does not specifically target improving classification accuracy in the presence of imbalanced classes."
            },
            {
                "id": "C",
                "text": "Synthetic Minority Oversampling Technique (SMOTE)",
                "is_correct": true,
                "explanation": "Synthetic Minority Oversampling Technique (SMOTE) is a popular technique used to address imbalanced datasets by generating synthetic samples from the minority class. By oversampling the minority class, SMOTE helps to balance the class distribution and improve the classification accuracy of the model."
            },
            {
                "id": "D",
                "text": "Permutation Feature Importance",
                "is_correct": false,
                "explanation": "Permutation Feature Importance is a method used to evaluate the importance of features in a model by permuting the values of each feature and measuring the impact on the model's performance. While feature importance is important for model interpretability, it does not directly tackle the challenge of imbalanced datasets and improving classification accuracy in such cases."
            }
        ],
        "correct_answer": "C",
        "feedback": "When working with imbalanced classification tasks, the distribution of classes can heavily bias the model toward the majority class, leading to poor predictive performance for the minority class. SMOTE addresses this by synthetically generating new samples for the minority class using a k-nearest neighbors approach. This improves the model's ability to learn decision boundaries that include minority class instances. It's a standard preprocessing step when accuracy, recall, or F1-score for minority classes needs to be improved."
    },
    {
        "question_id": "30",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning Studio to perform feature engineering on a dataset\n\nYou need to normalize values to produce a feature column grouped into bins\n\nSolution: Apply an Entropy Minimum Description Length (MDL) binning mode\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Applying Entropy MDL binning is a discretization technique used to bin continuous numerical values based on information gain, not normalization. Normalization transforms values into a specific scale (such as [0, 1] or standard Gaussian), which is a different preprocessing goal. Therefore, this solution does not meet the goal of normalizing values."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Normalization and binning are different operations. Normalization scales numerical values to a specific range, commonly [0, 1] or standard normal distribution. Entropy Minimum Description Length (MDL) binning, on the other hand, is a discretization technique that groups continuous values into bins using entropy-based criteria. Thus, the solution does not satisfy the goal of normalizing values."
            }
        ],
        "correct_answer": "B",
        "feedback": "The use of MDL binning does not fulfill the requirement of normalization. While MDL is a valid method for discretizing continuous variables into categorical bins, normalization aims to adjust the scale of numerical features without converting them into categories. Azure Machine Learning Studio provides separate modules for normalization, such as MinMax or Z-score normalization, which are more appropriate when the goal is to scale features rather than discretize them."
    },
    {
        "question_id": "31",
        "type": "multiple_choice_single_answer",
        "question": "Scenario: You have been contracted by Wayne Enterprises, a company owned by Bruce Wayne with a market value of over twenty-seven million dollars\n\nBruce founded Wayne Enterprises shortly after he created the Wayne Foundation and he became the president and chairman of the company\n\nBruce has come to you because his IT team plans to use Microsoft Azure Machine Learning and your expertise is required\n\nYou are holding a workgroup session and discussing Spark is a distributed computing environment and work is parallelized across executors\n\nA question was asked by one of the team members as to which two levels does this parallelization occur\n\nWhich of the following is the answer you should provide them?",
        "options": [
            {
                "id": "A",
                "text": "The Slot and the Task",
                "is_correct": false,
                "explanation": "The parallelization in Spark does not occur between the Slot and the Task. A Slot is a resource allocation on an Executor for task execution, while a Task is a unit of work that will be sent to one Executor to be executed."
            },
            {
                "id": "B",
                "text": "The Driver and the Executor",
                "is_correct": false,
                "explanation": "The Driver and the Executor are components of the Spark architecture. The Driver is responsible for managing the job execution, and the Executors perform the tasks. However, parallelization itself occurs within Executors and their resource allocations (Slots)."
            },
            {
                "id": "C",
                "text": "The Executor and the Slot",
                "is_correct": true,
                "explanation": "Parallelization in Apache Spark occurs between the Executor and the Slot. An Executor is a process launched on a worker node that runs tasks and stores data in memory or disk. A Slot is a unit of resource allocation on an Executor where a Task runs. Each Executor can run multiple Tasks in parallel depending on the number of available Slots."
            },
            {
                "id": "D",
                "text": "The Cache and the Slot",
                "is_correct": false,
                "explanation": "The Cache is used for storing data in memory to improve performance, not for parallel task execution. Parallelization does not occur between the Cache and the Slot."
            }
        ],
        "is_from_sdk_v2": true,
        "feedback": "In Apache Spark, parallelization occurs across two levels: Executors and Slots. Executors are distributed worker processes that run on cluster nodes and execute the actual tasks. Within each Executor, resources are divided into Slots, and each Slot can run a task concurrently. This architecture enables efficient distribution and parallel processing of data across the Spark cluster. The Driver coordinates this process but is not directly involved in parallel execution. Understanding this is essential when optimizing workloads in distributed environments, such as those used in Azure Synapse or Azure Databricks clusters integrated with Azure Machine Learning.",
        "correct_option": "C"
    },
    {
        "question_id": "32",
        "type": "multiple_choice_single_answer",
        "question": "You are working with convolutional neural network (CNN)\n\n extracts useful features from input images using several layers, and then passes those features into fully connected layers to make predictions.\n\nThe early layers of a CNN reduce the high-dimensional pixel input into a smaller set of learned features that are more useful for classification.\n\nCNNs are composed of multiple layers, each with a distinct role — such as feature extraction or classification.\n\n**Which layer type is being described below?**\n\n>One of the key challenges in training CNNs is avoiding overfitting, where the model performs well on the training data but poorly on unseen data.\n>To combat this, a technique is used where certain layers randomly modify the feature maps during training.\n>Although this may seem counterintuitive, it helps prevent the model from becoming too reliant on specific features from the training data, encouraging better generalization.",
        "options": [
            {
                "id": "A",
                "text": "Flattening layers",
                "is_correct": false,
                "explanation": "Flattening layers transform multidimensional tensors into flat vectors before they are passed to dense layers. They do not help prevent overfitting."
            },
            {
                "id": "B",
                "text": "Convolution layers",
                "is_correct": false,
                "explanation": "Convolution layers extract spatial features from input data. Although crucial in CNNs, they are not used primarily to prevent overfitting."
            },
            {
                "id": "C",
                "text": "Fully connected layers",
                "is_correct": false,
                "explanation": "Fully connected layers connect all neurons in one layer to every neuron in the next layer and often lead to overfitting if not regulated."
            },
            {
                "id": "D",
                "text": "Dropout layers",
                "is_correct": true,
                "explanation": "Dropout layers randomly deactivate a subset of neurons during training. This prevents the model from becoming overly dependent on specific neurons, improving generalization and reducing overfitting."
            },
            {
                "id": "E",
                "text": "Pooling layers",
                "is_correct": false,
                "explanation": "Pooling layers reduce the spatial size of the representation but do not modify feature maps to prevent overfitting."
            },
            {
                "id": "F",
                "text": "Normalizing layers",
                "is_correct": false,
                "explanation": "Normalizing layers standardize inputs, which helps with training stability, but they are not designed specifically to mitigate overfitting."
            }
        ],
        "feedback": "The correct answer is Dropout layers. These are specifically designed to reduce overfitting in convolutional neural networks (CNNs). During training, dropout randomly deactivates a fraction of the neurons in a layer, which prevents the network from relying too heavily on any single feature. This promotes redundancy and forces the network to learn more robust patterns that generalize better to new, unseen data. This strategy is especially effective in CNNs, where the model might otherwise memorize training data patterns. Dropout is considered a regularization technique and is commonly used in combination with convolutional and fully connected layers to improve model generalization."
    },
    {
        "question_id": "34",
        "type": "multiple_choice_single_answer",
        "question": "You use Azure Machine Learning SDK v2 to define a training job\n\nYou have already registered a tabular dataset in your workspace with the name 'training_data'\n\nYou want to reference this dataset in your training job and make it available inside the script as an input named 'training_data'\n\nWhich code should you use to define the input correctly?",
        "options": [
            {
                "id": "A",
                "text": "inputs={'training_data': Dataset.get_by_name(ws, 'training_data')}",
                "is_correct": false,
                "explanation": "This syntax is used in SDK v1 and is not compatible with SDK v2. SDK v2 uses the Input class for dataset binding."
            },
            {
                "id": "B",
                "text": "inputs={'training_data': Input(type='uri_folder', path='azureml:training_data:1')}",
                "is_correct": true,
                "explanation": "This is the correct syntax for referencing a registered dataset in SDK v2 using the Input class. It binds the dataset as a named input available inside the script."
            },
            {
                "id": "C",
                "text": "training_data = ml_client.datasets.get('training_data')",
                "is_correct": false,
                "explanation": "This is not a valid method in SDK v2. The `ml_client.datasets` interface does not exist in this form."
            },
            {
                "id": "D",
                "text": "inputs=[Dataset.File.from_path('training_data')]",
                "is_correct": false,
                "explanation": "This method resembles v1-style syntax and lacks the proper binding and structure for SDK v2 training jobs."
            }
        ],
        "feedback": "In SDK v2, the correct way to bind a registered dataset to a training job is by using the `Input` class from `azure.ai.ml`. You must specify the input type (e.g., 'uri_file' or 'uri_folder') and use the path to the registered dataset, such as `azureml:training_data:1`. This ensures the dataset is properly mounted or downloaded and accessible inside your training script."
    },
    {
        "question_id": "35",
        "type": "multiple_choice_single_answer",
        "question": "You have been tasked with constructing a machine learning model that translates language text into a different language text\n\nThe machine learning model must be constructed and trained to learn the sequence of the input\n\nSolution: You make use of Convolutional Neural Networks (CNNs)\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "CNNs are powerful for image data and have been used in NLP tasks for feature extraction, but they are not well-suited for sequence modeling tasks such as language translation. Tasks that require understanding of sequential dependencies, like language translation, are better handled by architectures such as RNNs, LSTMs, or Transformers."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "CNNs are not designed to model long-range dependencies in sequences, which are critical for tasks like language translation. Recurrent models or Transformers are more appropriate because they capture the order and context of words in a sequence."
            }
        ],
        "feedback": "The correct answer is 'No'. While CNNs can be adapted for certain NLP tasks, they do not inherently model sequential dependencies over long ranges, which is essential for accurate language translation. Sequence-to-sequence models with attention mechanisms, like the Transformer architecture, are state-of-the-art for translation problems. Azure ML supports training such models using PyTorch or TensorFlow through custom jobs with MLflow tracking for reproducibility."
    },
    {
        "question_id": "36",
        "type": "multiple_choice_single_answer",
        "question": "In the context of Azure Machine Learning and distributed deep learning with Horovod, what is the primary purpose of the following callback in the training script?\n\n`BroadcastGlobalVariablesCallback(0)`",
        "options": [
            {
                "id": "A",
                "text": "To average metrics among workers at the end of every epoch.",
                "is_correct": false,
                "explanation": "Averaging metrics is typically handled by Horovod\u2019s `hvd.allreduce()` function, not by the broadcast callback."
            },
            {
                "id": "B",
                "text": "Broadcast final variable states from rank 0 to all other processes.",
                "is_correct": false,
                "explanation": "While this is related, the focus of the callback is on consistent initialization, not on broadcasting final states."
            },
            {
                "id": "C",
                "text": "Reduce the learning rate if training plateaus.",
                "is_correct": false,
                "explanation": "This functionality is handled by learning rate schedulers like `ReduceLROnPlateau`, not by the Horovod broadcast callback."
            },
            {
                "id": "D",
                "text": "To ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.",
                "is_correct": true,
                "explanation": "This is the main purpose of `BroadcastGlobalVariablesCallback(0)`: to synchronize the initial model weights across all workers to ensure they start training identically."
            }
        ],
        "feedback": "In distributed training using Horovod and Azure Machine Learning SDK v2, each worker runs independently on its own process or GPU. If model initialization is random or restored from a checkpoint, it's critical that all workers begin with the same model weights.\n\nThe callback `BroadcastGlobalVariablesCallback(0)` ensures synchronization by broadcasting the model variables (weights) from the root worker (rank 0) to all other workers at the start of training. This avoids divergence in model updates caused by inconsistent starting points.\n\nThis callback is widely used in distributed training scripts configured via `command()` jobs or custom components with `distribution=HorovodConfiguration()` in SDK v2.\n\n```python\nfrom horovod.tensorflow.keras import BroadcastGlobalVariablesCallback\nmodel.fit(..., callbacks=[BroadcastGlobalVariablesCallback(0)])\n```\n\nThis technique maintains training integrity and is essential when performing large-scale distributed training jobs with Azure ML and Horovod."
    },
    {
        "question_id": "37",
        "type": "ordering",
        "question": "You are building a monthly retraining pipeline for a multi-class image classification model using the PyTorch framework in Azure Machine Learning\n\nThe pipeline should minimize cost and training time by selecting the appropriate compute targets\n\nThe experiment runs on a GPU-enabled compute cluster\n\nUsing the Azure ML Python SDK v2, define a pipeline with three command components that:\n\n1. Ingest new image data from a public web portal\n\n2. Resize the images\n\n3. Train the model using the PyTorch script on a GPU cluster\n\nIn what order should you run these components?",
        "options": [
            {
                "id": "A",
                "text": "Run a data ingestion component on the cpu-cluster to fetch image data from a public web source."
            },
            {
                "id": "B",
                "text": "Run an image preprocessing component (image_resize.py) on the cpu-cluster."
            },
            {
                "id": "C",
                "text": "Run the PyTorch training component (bird_classifier_train.py) on the gpu-cluster."
            }
        ],
        "correct_order": [
            "A",
            "B",
            "C"
        ],
        "feedback": "In Azure ML SDK v2, the best practice is to define each stage of the pipeline as a `command` component. First, the ingestion component (A) runs on a CPU cluster to pull and register image data. Second, the image resizing script (B) executes as a lightweight CPU-bound transformation. Finally, the model training script (C) uses GPU resources to efficiently retrain the PyTorch model. Each component uses `Input(type=uri_folder)` and `Output(type=uri_folder)` to pass data efficiently between stages in the pipeline. This modular and scalable design aligns with cost optimization and modern ML engineering practices in SDK v2."
    },
    {
        "question_id": "38",
        "type": "multiple_choice_single_answer",
        "question": "You want to connect to your Azure Machine Learning workspace from your local development environment using the Azure ML Python SDK v2\n\nYou have already installed the azure-ai-ml and azure-identity packages\n\nWhat else do you need to do before you can start interacting with your workspace?",
        "options": [
            {
                "id": "A",
                "text": "Use the Azure CLI to log in with `az login` so DefaultAzureCredential can authenticate.",
                "is_correct": true,
                "explanation": "DefaultAzureCredential uses Azure CLI credentials if available. Running `az login` ensures your local environment is authenticated and able to connect to Azure ML resources using the SDK v2."
            },
            {
                "id": "B",
                "text": "Create a Compute Instance in Azure ML Studio.",
                "is_correct": false,
                "explanation": "A compute instance is useful for running code in the cloud, but it is not required to connect from your local machine using SDK v2."
            },
            {
                "id": "C",
                "text": "Install the azureml-sdk['notebooks'] package.",
                "is_correct": false,
                "explanation": "This package belongs to the deprecated SDK v1 and is not used in SDK v2."
            },
            {
                "id": "D",
                "text": "Download a config.json file and use Workspace.from_config().",
                "is_correct": false,
                "explanation": "This approach is specific to SDK v1. In SDK v2, you connect using MLClient and Azure credentials without the need for a config.json file."
            }
        ],
        "feedback": "The correct answer is A. Azure ML SDK v2 uses `DefaultAzureCredential` from the `azure-identity` package, which integrates with the Azure CLI. This allows secure access to your workspace without needing to store credentials or config files. Ensure you run `az login` before using the SDK so that the credential chain can authenticate successfully."
    },
    {
        "question_id": "38A",
        "type": "multiple_choice_single_answer",
        "question": "You can think of the steps to train and evaluate a clustering machine learning model\n\nWhich of the following is an invalid step within the context of training and evaluating a clustering machine learning model?",
        "options": [
            {
                "id": "A",
                "text": "Train model",
                "is_correct": false,
                "explanation": "Training a clustering model is a valid and essential step in unsupervised learning to group data based on similarities."
            },
            {
                "id": "B",
                "text": "Evaluate performance",
                "is_correct": false,
                "explanation": "Clustering models can be evaluated using metrics like silhouette score, inertia, or Davies\u2013Bouldin index."
            },
            {
                "id": "C",
                "text": "Deploy a predictive service",
                "is_correct": false,
                "explanation": "While clustering is unsupervised, it can still be used to deploy a service that assigns cluster labels to new data, such as for segmentation or anomaly detection."
            },
            {
                "id": "D",
                "text": "All options are valid steps, therefore none of the listed steps are invalid.",
                "is_correct": true,
                "explanation": "All the options mentioned\u2014preparing data, training, evaluating, and deploying\u2014are valid and commonly used steps when working with clustering models, especially in practical Azure ML workflows."
            },
            {
                "id": "E",
                "text": "Prepare data",
                "is_correct": false,
                "explanation": "Preparing data is a crucial step for clustering, ensuring normalization, scaling, and missing value handling before fitting the model."
            }
        ],
        "feedback": "All steps listed are valid when working with clustering models in Azure Machine Learning. While clustering is unsupervised and doesn\u2019t predict labels, it can still be deployed to assign cluster IDs or for downstream tasks. Azure ML SDK v2 supports unsupervised training, evaluation with clustering metrics, and deployment using real-time endpoints."
    },
    {
        "question_id": "39",
        "type": "multiple_choice_single_answer",
        "question": "Machine learning models are as strong as the data they are trained on\n\nOften it is important to derive features from existing raw data that better represent the nature of the data and thus help improve the predictive power of the machine learning algorithms\n\nThis process of generating new predictive features from existing raw data is commonly referred to as:",
        "options": [
            {
                "id": "A",
                "text": "Factor engineering",
                "is_correct": false,
                "explanation": "Factor engineering is not a standard term in machine learning. It may be confused with factor analysis, but it does not refer to the process of creating new features from raw data."
            },
            {
                "id": "B",
                "text": "Feature engineering",
                "is_correct": true,
                "explanation": "Feature engineering is the process of creating new features or variables from existing raw data to improve the performance of machine learning models. By deriving new features that better represent the underlying patterns in the data, the predictive power of the algorithms can be enhanced."
            },
            {
                "id": "C",
                "text": "Feature mining",
                "is_correct": false,
                "explanation": "Feature mining is not a widely accepted term in the context of machine learning. The standard process for transforming raw data into informative variables is called feature engineering."
            },
            {
                "id": "D",
                "text": "Factor mining",
                "is_correct": false,
                "explanation": "Factor mining is not a recognized concept in the field of machine learning and does not refer to the generation of features from raw data."
            },
            {
                "id": "E",
                "text": "Element utilization",
                "is_correct": false,
                "explanation": "Element utilization is not a concept used in machine learning for deriving new predictive variables from raw data."
            }
        ],
        "feedback": "The correct answer is 'Feature engineering'. In the context of Azure Machine Learning using Python SDK v2, this step is essential in the data preparation phase. Well-engineered features directly influence model quality. Azure ML allows you to encapsulate feature engineering scripts into pipeline components (using command components or @command decorators), which can be reused and registered as part of your MLOps strategy. For instance, you may define a YAML or Python-based command component that performs missing value imputation, encoding, binning, or time-based feature extraction. These components are then used inside a pipeline job to ensure consistent data preprocessing before model training."
    },
    {
        "question_id": "40",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure Machine Learning Python SDK v2 to run experiments\n\nYou need to create an environment from a Conda configuration (.yml) file\n\nWhich method should you use to define the environment?",
        "options": [
            {
                "id": "A",
                "text": "Use the Environment class and pass the 'conda_file' argument during instantiation",
                "is_correct": true,
                "explanation": "In Azure ML SDK v2, environments are created using the Environment class from azure.ai.ml.entities. To use a Conda file, you pass it to the 'conda_file' argument when initializing the Environment object."
            },
            {
                "id": "B",
                "text": "Call Environment.create_from_conda_specification()",
                "is_correct": false,
                "explanation": "This method is used in SDK v1, not in the v2 version of the Azure ML SDK."
            },
            {
                "id": "C",
                "text": "Use the azureml.core.Environment constructor and pass the conda YAML as string",
                "is_correct": false,
                "explanation": "This approach corresponds to SDK v1 and is no longer used in the SDK v2 structure."
            }
        ],
        "feedback": "In Azure Machine Learning SDK v2, environments are created using the Environment class from azure.ai.ml.entities. You can specify the Docker image and Conda environment at instantiation. For example, if you want to define a custom Python environment for training an image classifier, you can do so by referencing a conda.yml file like this:\n\n```python\nfrom azure.ai.ml.entities import Environment\n\nenv = Environment(\n    name=\"img-clf-env\",\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n    conda_file=\"./envs/train.yml\"\n)\n```\n\nThis environment can then be reused and versioned across multiple training pipelines or jobs."
    },
    {
        "question_id": "41",
        "type": "multiple_choice_single_answer",
        "question": "Smartoire has requested the team to construct Custom Roles\n\nWhich is the best answer regarding what creating Custom Roles does?",
        "options": [
            {
                "id": "A",
                "text": "Custom roles allow you to customize what users can and cannot access in a workspace",
                "is_correct": true,
                "explanation": "This is the correct choice. Custom roles enable you to tailor the access permissions for users within a workspace, specifying what they can and cannot access based on their role."
            },
            {
                "id": "B",
                "text": "Custom roles allow you to customize what users can access in a workspace",
                "is_correct": false,
                "explanation": "While custom roles do allow you to customize user access within a workspace, they also define restrictions on what users can do, not just what they can access."
            },
            {
                "id": "C",
                "text": "Custom roles allow you to only perform read-only actions in a workspace",
                "is_correct": false,
                "explanation": "This is not correct. Custom roles are flexible and not limited to read-only actions; they allow defining a wide range of permissions."
            },
            {
                "id": "D",
                "text": "Custom roles allow you to customize the speaking voice used",
                "is_correct": false,
                "explanation": "This is irrelevant in the context of Azure Machine Learning. Custom roles are for access and permission management, not voice customization."
            }
        ],
        "feedback": "Custom roles in Azure allow administrators to define very granular access control by specifying actions that users can and cannot perform within a workspace. This supports the principle of least privilege, improves security posture, and aligns access with job responsibilities. For example, a user responsible only for monitoring experiments can be restricted from registering models or deploying endpoints. More info: https://learn.microsoft.com/en-us/azure/role-based-access-control/custom-roles"
    },
    {
        "question_id": "42",
        "type": "multiple_choice_single_answer",
        "question": "You have trained a model, and you want to quantify the influence of each feature on a specific individual prediction\n\nWhat kind of feature importance should you examine?",
        "options": [
            {
                "id": "A",
                "text": "Local feature importance",
                "is_correct": true,
                "explanation": "Local feature importance focuses on quantifying the influence of each feature on a specific individual prediction. It provides insights into how each feature contributes to the prediction for a particular data point, allowing for a more granular understanding of the model's decision-making process."
            },
            {
                "id": "B",
                "text": "Global feature importance",
                "is_correct": false,
                "explanation": "Global feature importance, on the other hand, looks at the overall impact of each feature across the entire dataset. It provides a broader perspective on the importance of features in general, rather than on a specific prediction."
            }
        ],
        "feedback": "To understand a model's prediction for a specific individual data point (e.g., why this customer churned), you need **local feature importance**. Azure Machine Learning SDK v2 provides capabilities for Responsible AI, including interpretability tools like SHAP, which help visualize local feature contributions. In contrast, global importance tells you which features are important in general, not per prediction. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability"
    },
    {
        "question_id": "44",
        "type": "multiple_choice_single_answer",
        "question": "You are analyzing a numerical dataset which contains missing values in several columns\n\nYou must clean the missing values using an appropriate operation without affecting the dimensionality of the feature set\n\nYou need to analyze a full dataset to include all values\n\nSolution: Remove the entire column that contains the missing data point\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "While this may eliminate missing values, it contradicts the goal of preserving all feature columns in the dataset."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Removing entire columns that contain missing values will affect the dimensionality of the feature set. Since the requirement is to keep the original set of features intact while handling missing values, this approach does not meet the goal."
            }
        ],
        "feedback": "The correct approach should preserve the structure of the dataset, including all columns. In Azure Machine Learning SDK v2, missing values can be handled using imputation techniques (e.g., mean, median, mode) through `sklearn.impute.SimpleImputer` or integrated directly into preprocessing pipelines. Dropping entire columns is not recommended unless the column is irrelevant or contains excessive missing values."
    },
    {
        "question_id": "45",
        "type": "multiple_choice_single_answer",
        "question": "You are a senior data scientist in your company\n\nYou are evaluating a completed binary classification machine learning model\n\nYou need to use precision as the evaluation metric\n\nWhich visualization should you use?",
        "options": [
            {
                "id": "A",
                "text": "Receiver Operating Characteristic (ROC) curve",
                "is_correct": false,
                "explanation": "ROC curves are useful for evaluating TPR vs. FPR but are not ideal when focusing specifically on precision."
            },
            {
                "id": "B",
                "text": "Violin plot",
                "is_correct": false,
                "explanation": "A violin plot is used to visualize the distribution of data and is unrelated to precision evaluation."
            },
            {
                "id": "C",
                "text": "Gradient descent",
                "is_correct": false,
                "explanation": "Gradient descent is an optimization technique, not a visualization method."
            },
            {
                "id": "D",
                "text": "Scatter plot",
                "is_correct": false,
                "explanation": "Scatter plots are useful for analyzing relationships between numerical variables, not for classification evaluation metrics like precision."
            },
            {
                "id": "E",
                "text": "Precision-Recall curve",
                "is_correct": true,
                "explanation": "Precision-Recall (PR) curves are ideal when the focus is on the precision metric, especially for imbalanced binary classification problems."
            }
        ],
        "feedback": "The correct visualization for evaluating precision in binary classification is the Precision-Recall curve. This curve is especially useful when working with imbalanced datasets and when precision (the proportion of true positives among predicted positives) is the primary focus. Learn more at: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automl-output#precision-recall-curve"
    },
    {
        "question_id": "46",
        "type": "multiple_choice_single_answer",
        "question": "You have a model with a large difference between the training and validation error values\n\nYou must create a new model and perform cross-validation\n\nYou need to identify a parameter set for the new model using Azure Machine Learning Studio\n\nWhich module should you use for the step: Train, evaluate, and compare?",
        "options": [
            {
                "id": "A",
                "text": "Split Data",
                "is_correct": false,
                "explanation": "This module is used to divide a dataset into training and testing sets but does not handle hyperparameter tuning or model comparison."
            },
            {
                "id": "B",
                "text": "Partition and Sample",
                "is_correct": false,
                "explanation": "This module is typically used for data sampling and partitioning, not for model training or hyperparameter tuning."
            },
            {
                "id": "C",
                "text": "Two-Class Boosted Decision Tree",
                "is_correct": false,
                "explanation": "This module builds a specific type of model but is not designed to perform cross-validation or tune hyperparameters for improving model generalization."
            },
            {
                "id": "D",
                "text": "Tune Model Hyperparameters",
                "is_correct": true,
                "explanation": "This module performs cross-validation to find the best parameter set and improve model generalization performance."
            }
        ],
        "feedback": "The 'Tune Model Hyperparameters' module in Azure Machine Learning Studio is specifically designed to help optimize a model by evaluating multiple parameter configurations using cross-validation. This is particularly helpful when your model shows signs of overfitting \u2014 such as a large discrepancy between training and validation errors. By using this module, Azure ML Studio automates the process of finding the most appropriate parameters that reduce generalization error and improve predictive performance.\n\n**Referencia oficial:** https://learn.microsoft.com/en-us/azure/machine-learning/component-reference/tune-model-hyperparameters"
    },
    {
        "question_id": "47",
        "type": "multiple_choice_single_answer",
        "question": "You are using Fairlearn with Azure Machine Learning to ensure fairness in your classification model\n\nYou want to use a constraint that minimizes the disparity in both true positive and false positive rates across different sensitive feature groups\n\nWhich constraint should you use?",
        "options": [
            {
                "id": "A",
                "text": "False-positive rate parity",
                "is_correct": false,
                "explanation": "This constraint only considers parity in false-positive rates and ignores the true-positive component required in this case."
            },
            {
                "id": "B",
                "text": "Demographic parity",
                "is_correct": false,
                "explanation": "Demographic parity focuses on equalizing predictions across groups, regardless of true/false positives."
            },
            {
                "id": "C",
                "text": "Bounded group loss",
                "is_correct": false,
                "explanation": "Bounded group loss minimizes loss disparity but does not ensure parity in both TPR and FPR."
            },
            {
                "id": "D",
                "text": "Equalized odds",
                "is_correct": true,
                "explanation": "Equalized odds ensures that each sensitive group has similar true positive and false positive rates, which meets the requirements described."
            },
            {
                "id": "E",
                "text": "True positive rate parity",
                "is_correct": false,
                "explanation": "True positive rate parity ignores the false positive rate, which is essential for equalized odds."
            },
            {
                "id": "F",
                "text": "Error rate parity",
                "is_correct": false,
                "explanation": "This is not a standard constraint in Fairlearn and does not match the definition provided."
            }
        ],
        "answer": "D",
        "explanation": "The Equalized Odds constraint in Fairlearn minimizes disparities in both true positive and false positive rates across sensitive feature groups. This is particularly useful in binary classification tasks where you want fair performance across different subgroups of the population.",
        "references": [
            "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-fairness-ml",
            "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html#fairness-in-machine-learning"
        ]
    },
    {
        "question_id": "48",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following is a form of machine learning in which the goal is to create a model that can predict a numeric, quantifiable value; such as a price, amount, size, or other scalar numbers?",
        "options": [
            {
                "id": "A",
                "text": "Probability",
                "is_correct": false,
                "explanation": "Probability is a measure of the likelihood of an event occurring and is not directly related to predicting numeric values in machine learning."
            },
            {
                "id": "B",
                "text": "Classification",
                "is_correct": false,
                "explanation": "Classification deals with predicting categories or classes, not continuous numeric values like prices or amounts."
            },
            {
                "id": "C",
                "text": "Regression",
                "is_correct": true,
                "explanation": "Regression focuses on predicting continuous numeric values. It is ideal for modeling relationships between input variables and numeric output variables like prices, amounts, or sizes."
            },
            {
                "id": "D",
                "text": "Statistics",
                "is_correct": false,
                "explanation": "Statistics is a foundational discipline that supports machine learning but is not a type of learning model in itself."
            }
        ],
        "feedback": "Regression is a supervised learning approach used to predict a continuous outcome variable. In the context of Azure Machine Learning and DP-100, understanding the difference between regression and classification is essential to selecting appropriate models for tasks such as price prediction, risk estimation, or demand forecasting."
    },
    {
        "question_id": "49",
        "type": "multiple_choice_single_answer",
        "question": "You are analyzing a numerical dataset that contains missing values in several columns\n\nThe dataset must be cleaned using an appropriate operation without affecting the dimensionality of the feature set\n\nThe requirement is to analyze a full dataset that includes all values\n\nThe developer chose to replace each missing value using the Multiple Imputation by Chained Equations (MICE) method\n\nDoes the solution meet the requirement?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": "MICE (Multiple Imputation by Chained Equations) is a valid method for imputing missing values without reducing the dimensionality of the dataset, thus preserving all features and enabling full dataset analysis."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": "The 'No' option would only be correct if the method dropped columns or failed to impute all missing values, which is not the case with MICE."
            }
        ],
        "feedback": "The requirement was to clean missing values while keeping all original features (no loss of dimensionality). The MICE method is suitable because it imputes missing values by modeling each feature with missing values as a function of other features. This makes it a robust approach in scenarios where maintaining the dataset structure is crucial for downstream analysis and modeling."
    },
    {
        "question_id": "50",
        "type": "multiple_choice_single_answer",
        "question": "Logan is one of the developers on an IT migration project and has trained a model using Azure Databricks\n\nThe next step is to serve it for real-time scoring\n\nWhat should be the next action?",
        "options": [
            {
                "id": "A",
                "text": "Register the model.",
                "is_correct": true,
                "explanation": "Registering the model is the required step before deploying it for inference. Registration makes the model available in the Azure ML workspace for deployment to endpoints."
            },
            {
                "id": "B",
                "text": "Transition the model to the archived stage.",
                "is_correct": false,
                "explanation": "Archiving a model is used when the model is no longer in use or is being deprecated, not when preparing it for deployment."
            },
            {
                "id": "C",
                "text": "Version the model.",
                "is_correct": false,
                "explanation": "Versioning helps track model iterations but does not, by itself, make the model available for real-time scoring."
            }
        ],
        "feedback": "In Azure ML, once a model is trained (whether using Azure ML, Databricks, or another environment), it must be **registered** in the workspace before it can be deployed to a managed endpoint or for real-time scoring. Registering ensures the model is available with a name and version and can be tracked, deployed, or updated using Azure Machine Learning services. For example, using `ml_client.models.create_or_update(...)` in SDK v2 registers the model artifact with its metadata."
    },
    {
        "question_id": "51",
        "type": "multiple_choice_single_answer",
        "question": "You are working with a time series dataset in Azure Machine Learning using the Python SDK v2\n\nYou need to split the dataset into training and testing subsets while preserving the temporal order\n\nWhich of the following approaches should you use?",
        "options": [
            {
                "id": "A",
                "text": "Use sklearn.model_selection.TimeSeriesSplit to generate training and testing splits.",
                "is_correct": true,
                "explanation": "TimeSeriesSplit from scikit-learn is designed specifically for time series data and preserves temporal ordering when generating training and test splits, making it ideal for time-dependent data in Azure ML pipelines."
            },
            {
                "id": "B",
                "text": "Use random.shuffle on the dataset before splitting.",
                "is_correct": false,
                "explanation": "Shuffling the dataset destroys the temporal structure, which is critical in time series analysis."
            },
            {
                "id": "C",
                "text": "Use train_test_split with shuffle=True.",
                "is_correct": false,
                "explanation": "This also randomizes the order of samples, which is not suitable for time series data."
            },
            {
                "id": "D",
                "text": "Use pandas.DataFrame.sample() to split the dataset.",
                "is_correct": false,
                "explanation": "Sampling randomly is inappropriate for time series because it does not preserve order and may lead to data leakage."
            }
        ],
        "explanation": "When working with time series datasets in Azure ML SDK v2, preserving the order of data is essential to avoid data leakage. The best practice is to use `TimeSeriesSplit` from scikit-learn, which generates splits that respect the chronological order of observations, ensuring proper model evaluation in a time-dependent context."
    },
    {
        "question_id": "52",
        "type": "multiple_choice_single_answer",
        "question": "You have been contracted by Wayne Enterprises to help integrate Azure Machine Learning into their inference services\n\nThe IT team wants to log custom telemetry metrics for the model's behavior and analyze them using Application Insights\n\nWhat should the team include in the entry script to capture custom metrics appropriately using Azure Machine Learning SDK v2?",
        "options": [
            {
                "id": "A",
                "text": "Use a print statement to write the metrics in the STDOUT log.",
                "is_correct": false,
                "explanation": "Print statements are not tracked automatically by Azure ML or Application Insights for telemetry purposes."
            },
            {
                "id": "B",
                "text": "Use mlflow.log_metric() to log the custom metrics.",
                "is_correct": true,
                "explanation": "mlflow.log_metric() is the correct method to log scalar values like custom metrics during an Azure ML run using SDK v2. These logs can be collected and visualized in Application Insights."
            },
            {
                "id": "C",
                "text": "Use the Run.log method to log the custom metrics.",
                "is_correct": false,
                "explanation": "Run.log belongs to SDK v1 and is no longer used with SDK v2 or MLflow-based runs."
            },
            {
                "id": "D",
                "text": "Save the custom metrics in the ./outputs folder.",
                "is_correct": false,
                "explanation": "Saving metrics as files doesn't automatically register them with Azure ML telemetry systems like Application Insights."
            }
        ],
        "answer": "B",
        "explanation": "In the Azure Machine Learning SDK v2, logging metrics such as accuracy, latency, or any custom telemetry during an inference or training run is done using MLflow. The `mlflow.log_metric()` function records scalar values which are automatically captured by Azure ML and can be monitored through Application Insights.",
        "sdk_version": "v2",
        "feedback": "This question reinforces the correct method for capturing custom metrics using the SDK v2, where MLflow is the default logging interface. You can use `mlflow.log_metric(name, value)` inside the entry script or scoring script to persist and visualize metrics in Azure ML Studio. See Microsoft documentation for [tracking metrics](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models#monitor-your-experiment-runs)."
    },
    {
        "question_id": "52A",
        "type": "multiple_choice_single_answer",
        "question": "You create a multi-class image classification deep learning model that uses a set of labeled bird photographs collected by experts\n\nAll photographs are stored in an Azure blob container\n\nYou need to access the bird photograph files from Azure ML service workspace for training, minimizing data movement\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.",
                "is_correct": false,
                "explanation": "TabularDataset is intended for structured/tabular data. Since the data consists of images, it is not appropriate."
            },
            {
                "id": "B",
                "text": "Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.",
                "is_correct": false,
                "explanation": "Copying the images introduces unnecessary data movement, which the question states must be minimized."
            },
            {
                "id": "C",
                "text": "Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.",
                "is_correct": true,
                "explanation": "Registering the existing Azure Blob container as a datastore allows the workspace to access the files directly without duplication or unnecessary transfer of data."
            },
            {
                "id": "D",
                "text": "Create an Azure Data Lake store and move the bird photographs to the store.",
                "is_correct": false,
                "explanation": "Creating a new storage and moving data contradicts the requirement of minimizing data movement."
            },
            {
                "id": "E",
                "text": "Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.",
                "is_correct": false,
                "explanation": "Azure Cosmos DB is not a suitable or efficient storage for large-scale image data like photographs."
            }
        ],
        "feedback": "To minimize data movement in Azure Machine Learning, the best practice is to register the existing Azure Blob Storage as a datastore. This allows scripts and training components to access the JPG images directly from the blob container without copying or duplicating the data. This is especially important when working with large-scale datasets such as image files in deep learning scenarios."
    },
    {
        "question_id": "53",
        "type": "multiple_choice_single_answer",
        "question": "You are working with convolutional neural network (CNN)\n\n extracts useful features from input images using several layers, and then passes those features into fully connected layers to make predictions.\n\nThe early layers of a CNN reduce the high-dimensional pixel input into a smaller set of learned features that are more useful for classification.\n\nCNNs are composed of multiple layers, each with a distinct role — such as feature extraction or classification.\n\n**Which layer type is being described below?**\n\n>After extracting feature values from images, these layers are used to reduce the number of feature values while retaining the key differentiating features that have been extracted",
        "options": [
            {
                "id": "A",
                "text": "Normalizing layers",
                "is_correct": false,
                "explanation": "Normalizing layers are primarily used to scale input values, typically to improve training stability. They do not reduce the number of features."
            },
            {
                "id": "B",
                "text": "Fully connected layers",
                "is_correct": false,
                "explanation": "Fully connected layers perform the final prediction task by combining all features, but do not reduce the number of feature values."
            },
            {
                "id": "C",
                "text": "Dropping layers",
                "is_correct": false,
                "explanation": "Dropping layers are not a standard concept in CNNs and do not perform feature reduction."
            },
            {
                "id": "D",
                "text": "Pooling layers",
                "is_correct": true,
                "explanation": "Pooling layers reduce the spatial dimensions of the feature maps, retaining important features while decreasing the number of parameters and computations."
            },
            {
                "id": "E",
                "text": "Convolution layers",
                "is_correct": false,
                "explanation": "Convolution layers extract features from the input data but do not reduce their number. Pooling is responsible for that."
            },
            {
                "id": "F",
                "text": "Flattening layers",
                "is_correct": false,
                "explanation": "Flattening layers prepare data for fully connected layers by transforming multi-dimensional data into 1D, but do not reduce features."
            }
        ],
        "correct_option": "D",
        "feedback": "The correct answer is Pooling layers. Pooling is used after feature extraction to reduce the spatial dimensions of the feature maps. This operation retains the most important features while reducing the computational burden, making it a critical step in CNNs. Unlike convolutional layers that focus on extracting features, pooling layers specifically help with dimensionality reduction, which is essential for downstream tasks like classification."
    },
    {
        "question_id": "54",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist and you use Azure Machine Learning Studio for your experiments\n\nYou are creating a new experiment in Azure Machine Learning Studio\n\nYou have a small dataset that has missing values in many columns\n\nThe data does not require the application of predictors for each column\n\nYou plan to use the Clean Missing Data\n\nYou need to select a data cleaning method\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "Normalization",
                "is_correct": false,
                "explanation": "Normalization scales numerical features to a standard range, but it is not a method for handling missing values."
            },
            {
                "id": "B",
                "text": "Synthetic Minority Oversampling Technique (SMOTE)",
                "is_correct": false,
                "explanation": "SMOTE is used for class imbalance by creating synthetic samples. It is not suitable for imputing missing values."
            },
            {
                "id": "C",
                "text": "Replace using MICE",
                "is_correct": false,
                "explanation": "MICE imputes missing values by chaining equations using other variables as predictors. This is not ideal if predictors per column are not required."
            },
            {
                "id": "D",
                "text": "Replace using Probabilistic PCA",
                "is_correct": true,
                "explanation": "Probabilistic PCA imputes missing values based on the underlying structure of the dataset and does not require individual predictors for each column, making it appropriate here."
            }
        ],
        "correct_answer": "D",
        "feedback": "Probabilistic PCA is the right choice in this case because it uses the latent structure of the data to estimate missing values. Unlike MICE, it does not rely on predictors for each column, making it ideal for datasets where many columns are missing values and predictor variables are not available or not applicable. This method is supported in Azure ML Studio\u2019s Clean Missing Data module and helps preserve the data's structure without excessive complexity."
    },
    {
        "question_id": "55",
        "type": "multiple_choice_single_answer",
        "question": "Melinda and the IT team at The Brand Corporation are training a binary classification model to support admission approvals\n\nThey want to ensure the model is fair and does not discriminate based on ethnicity\n\nWhat is the best method to evaluate the fairness of the model?",
        "options": [
            {
                "id": "A",
                "text": "Compare disparity between selection rates and performance metrics across ethnicities.",
                "is_correct": true,
                "explanation": "This is a valid approach to assess fairness. By analyzing selection rates and performance metrics for different ethnic groups, potential bias or discrimination in the model can be identified and addressed."
            },
            {
                "id": "B",
                "text": "Evaluate each trained model with a validation dataset, and use the model with the highest accuracy score. An accurate model is inherently fair.",
                "is_correct": false,
                "explanation": "Accuracy does not guarantee fairness. A highly accurate model may still discriminate if performance is uneven across groups."
            },
            {
                "id": "C",
                "text": "Evaluate each trained model with a validation ethnicity dataset to verify which values are more distributed, thereby less ethnically disposed.",
                "is_correct": false,
                "explanation": "Distribution alone does not ensure fairness. It's essential to compare key performance metrics between ethnic groups."
            },
            {
                "id": "D",
                "text": "Remove the ethnicity feature from the training dataset.",
                "is_correct": false,
                "explanation": "Removing the ethnicity feature does not eliminate bias and can make it harder to detect fairness issues."
            }
        ],
        "answer": "A",
        "feedback": "Fairness in machine learning models is best evaluated by comparing model behavior across sensitive groups. In this case, analyzing disparity in selection rates and performance metrics across ethnicities ensures that the model treats all groups fairly. Simply relying on accuracy or removing the sensitive attribute does not detect or mitigate bias. For such fairness evaluations, tools like Fairlearn can be integrated with Azure ML to compute metrics like demographic parity, equalized odds, and more."
    },
    {
        "question_id": "56",
        "type": "multiple_choice_single_answer",
        "question": "You are consulting on a project using Azure Machine Learning designer to create a real-time service endpoint\n\nThe team has a single Azure Machine Learning compute resource and needs to publish the inference pipeline as a web service\n\nWhich of the following compute types should the team use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": "Azure Databricks is a powerful platform for big data processing and analytics but is not suited for deploying real-time service endpoints using Azure ML designer."
            },
            {
                "id": "B",
                "text": "HDInsight",
                "is_correct": false,
                "explanation": "HDInsight is primarily used for big data frameworks like Hadoop and Spark, not for real-time endpoint deployment in Azure ML designer."
            },
            {
                "id": "C",
                "text": "Azure Kubernetes Services",
                "is_correct": true,
                "explanation": "Azure Kubernetes Services (AKS) is the recommended compute target for real-time endpoint deployments in Azure ML designer. It is scalable, container-based, and production-ready."
            }
        ],
        "correct_answer": "C",
        "feedback": "The correct compute type for deploying real-time inference pipelines created with Azure ML Designer is Azure Kubernetes Services (AKS). AKS supports containerized applications, offers flexibility and scalability, and is designed specifically for real-time inference hosting in production environments. This is aligned with Azure ML SDK v2 best practices for deploying models via managed online endpoints."
    },
    {
        "question_id": "57",
        "type": "ordering",
        "question": "Pym Tech is building a cloud-based machine learning solution using Apache Spark with automatic feature engineering\n\nThey must:\n\n- Create and deploy notebooks using dynamic Spark workers,\n- Export notebooks for version control,\n- Run them using AutoML\n\nWhat is the correct sequence of steps to fulfill these requirements?",
        "options": [
            {
                "id": "A",
                "text": "Create Azure Databricks cluster"
            },
            {
                "id": "B",
                "text": "Install Microsoft Machine Learning for Apache Spark"
            },
            {
                "id": "C",
                "text": "Create and execute Jupyter notebook using AutoML on the cluster"
            },
            {
                "id": "D",
                "text": "Export the processed Jupyter notebook"
            },
            {
                "id": "E",
                "text": "Export the Jupyter notebook to a local environment"
            },
            {
                "id": "F",
                "text": "Install Azure ML SDK for Python on the cluster"
            },
            {
                "id": "G",
                "text": "Execute Zeppelin notebooks on the cluster"
            },
            {
                "id": "H",
                "text": "Create an Azure HDInsight cluster to include the Apache Spark Mllib library"
            }
        ],
        "correct_order": [
            "A",
            "B",
            "C",
            "D",
            "E"
        ],
        "feedback": "The correct order begins by creating the compute environment (Azure Databricks cluster), then installing the Microsoft ML package for Spark. Once the environment is ready, AutoML can run Jupyter notebooks. After execution, the notebooks are exported for version control. HDInsight and Zeppelin are not needed in this scenario."
    },
    {
        "question_id": "58",
        "type": "multiple_choice_single_answer",
        "question": "Which regression method is best described as: 'The simplest form of regression, with no limit to the number of features used\n\nThis comes in many forms \u2013 often named by the number of features used and the shape of the curve that fits'?",
        "options": [
            {
                "id": "A",
                "text": "Linear Regression",
                "is_correct": true,
                "explanation": "Linear Regression is the simplest and most widely used form of regression. It models the relationship between one or more features and a target variable by fitting a linear equation. It can handle any number of features (univariate or multivariate), making it highly flexible and interpretable for regression tasks."
            },
            {
                "id": "B",
                "text": "Dynamic Programming Algorithms",
                "is_correct": false,
                "explanation": "Dynamic Programming is a general algorithm design technique, not a regression method. It is not used for fitting regression curves."
            },
            {
                "id": "C",
                "text": "Decision Trees",
                "is_correct": false,
                "explanation": "Decision Trees can be used for regression but they are not the simplest form. They create splits in the data and can lead to complex models."
            },
            {
                "id": "D",
                "text": "Ensemble Algorithms",
                "is_correct": false,
                "explanation": "Ensemble algorithms like Random Forest and Gradient Boosting combine multiple models and are more complex than simple linear regression."
            }
        ],
        "feedback": "Linear Regression is commonly referred to as the simplest form of regression. It is easy to interpret, robust with small data samples, and supports any number of input features. It is often the first algorithm data scientists use as a baseline model before trying more complex approaches."
    },
    {
        "question_id": "61",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a pipeline using Azure Machine Learning Designer\n\nThe pipeline must train a model using data from a CSV file published on a website\n\nA dataset has not yet been created for this file\n\nWhich module should be added to the pipeline in Designer?",
        "options": [
            {
                "id": "A",
                "text": "Convert to CSV",
                "is_correct": false,
                "explanation": "The 'Convert to CSV' module is used to convert datasets into CSV format, not for importing data from external sources."
            },
            {
                "id": "B",
                "text": "Import Data",
                "is_correct": true,
                "explanation": "The 'Import Data' module allows you to bring data into the pipeline from various sources including web URLs, without needing to first register a dataset manually. It is ideal when the dataset hasn't been created yet and minimal admin effort is required."
            },
            {
                "id": "C",
                "text": "Dataset",
                "is_correct": false,
                "explanation": "The 'Dataset' module can only be used if a dataset has already been registered in Azure ML. Since the scenario states no dataset has been created yet, this option is not applicable."
            },
            {
                "id": "D",
                "text": "Enter Data Manually",
                "is_correct": false,
                "explanation": "This module is used for manually entering small datasets, not for importing CSV files from external web sources. It is not practical for model training."
            }
        ],
        "feedback": "When a dataset has not been registered and you need to ingest data from an external source such as a URL with minimal overhead, the 'Import Data' module in Azure Machine Learning Designer is the appropriate choice. It supports connecting to public web URLs directly and feeding the data into a pipeline for processing and training."
    },
    {
        "question_id": "dp100-q23",
        "type": "multiple_choice_single_answer",
        "question": "You have been contracted by Wayne Enterprises to help their IT team set up a new cluster in Azure Databricks\n\nThey want to understand what happens behind the scenes when a new cluster is created in the Azure Databricks workspace\n\nWhat is an accurate description of these behind-the-scenes actions?",
        "options": [
            {
                "id": "A",
                "text": "Azure Databricks workspace is deployed on dedicated VMs where any given cluster nodes are symbionts of the VM hosts.",
                "is_correct": false,
                "explanation": "This is incorrect and uses non-standard terminology. Azure Databricks does not deploy clusters as 'symbionts' of VM hosts."
            },
            {
                "id": "B",
                "text": "Azure Databricks creates a cluster of driver and worker nodes, based on your VM type and size selections.",
                "is_correct": true,
                "explanation": "This is correct. Azure Databricks provisions driver and worker nodes based on your selection of VM types and sizes when creating a cluster."
            },
            {
                "id": "C",
                "text": "Azure Databricks provisions a dedicated VM that processes all jobs, based on your VM type and size selection.",
                "is_correct": false,
                "explanation": "This is incorrect. Azure Databricks uses a distributed architecture, so jobs are not processed on a single dedicated VM."
            },
            {
                "id": "D",
                "text": "When an Azure Databricks workspace is deployed, you are allocated a pool of VMs. Creating a cluster draws from this pool.",
                "is_correct": false,
                "explanation": "This is misleading. VM allocation happens dynamically based on configuration; a pre-allocated pool is not standard behavior."
            }
        ],
        "correct_answer": "B",
        "feedback": "Azure Databricks dynamically creates a cluster consisting of a driver node and one or more worker nodes when you initiate a new cluster. The size and type of each node are determined by your selections in the cluster configuration. This scalable architecture is essential for distributed processing in big data and machine learning workloads, offering both performance and flexibility. Understanding this behind-the-scenes behavior is critical for optimizing cluster cost and performance."
    },
    {
        "question_id": "dp100_azureml_designer_infer_clusters",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning designer to create a training pipeline for a clustering model\n\nYou want to use the model in an inference pipeline\n\nWhich module should the team use to infer cluster predictions from the model?",
        "options": [
            {
                "id": "A",
                "text": "Set Number of Centroids to equal the sample size",
                "is_correct": false,
                "explanation": "This option controls the number of clusters in the training step but is unrelated to inference."
            },
            {
                "id": "B",
                "text": "Score Model",
                "is_correct": false,
                "explanation": "Score Model is typically used to evaluate classification or regression models. For clustering predictions, it doesn't assign clusters."
            },
            {
                "id": "C",
                "text": "Train Clustering Model",
                "is_correct": false,
                "explanation": "This is used to train the clustering model, not to make predictions or assign new data to clusters."
            },
            {
                "id": "D",
                "text": "Assign Data to Clusters",
                "is_correct": true,
                "explanation": "The 'Assign Data to Clusters' module is used to assign each input data point to a predicted cluster after training."
            }
        ],
        "answer": "D",
        "feedback": "To make predictions with a clustering model in Azure Machine Learning Designer, you use the **Assign Data to Clusters** module. This module takes a trained clustering model and applies it to new data, assigning each instance to one of the learned clusters. Unlike classification models, where 'Score Model' is used for evaluation, clustering models rely on this specialized module to perform inference. This helps in applications such as customer segmentation or pattern discovery where assigning group membership is the goal."
    },
    {
        "question_id": "dp100_azureml_designer_infer_regression",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning designer to train a regression model to predict house prices\n\nYou have completed training and want to use the model to generate predictions on new data\n\nWhich module should you use in the inference pipeline to produce these predictions?",
        "options": [
            {
                "id": "A",
                "text": "Evaluate Model",
                "is_correct": false,
                "explanation": "Evaluate Model is used to assess the performance of the model, not to generate predictions on new data."
            },
            {
                "id": "B",
                "text": "Train Model",
                "is_correct": false,
                "explanation": "Train Model is used during the training phase, not during inference."
            },
            {
                "id": "C",
                "text": "Score Model",
                "is_correct": true,
                "explanation": "The Score Model module is used in inference pipelines to generate predictions using a trained model."
            },
            {
                "id": "D",
                "text": "Select Columns in Dataset",
                "is_correct": false,
                "explanation": "This module is used to filter or rearrange columns in a dataset, not to apply a model for predictions."
            }
        ],
        "answer": "C",
        "feedback": "In Azure Machine Learning Designer, after training a regression model, you use the Score Model module in the inference pipeline to apply the trained model to new data and generate predictions. This module outputs the predicted values alongside the input data, which is useful for tasks such as house price prediction or forecasting."
    },
    {
        "question_id": "dp100_responsible_ai_tabularexplainer",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is reviewing SHAP explainability options in Azure Machine Learning\n\nShe wants to use an explainer that selects an architecture-appropriate SHAP algorithm to interpret a model trained on tabular data\n\nWhich explainer should she use?",
        "options": [
            {
                "id": "A",
                "text": "MimicExplainer",
                "is_correct": false,
                "explanation": "MimicExplainer uses a surrogate model to approximate the original model and interpret it, but it does not use architecture-specific SHAP methods. It\u2019s more suited for models that can't be interpreted directly."
            },
            {
                "id": "B",
                "text": "TabularExplainer",
                "is_correct": true,
                "explanation": "TabularExplainer is the correct choice for selecting the most appropriate SHAP algorithm based on the model architecture and data. It supports tree SHAP for tree-based models and kernel SHAP for others."
            },
            {
                "id": "C",
                "text": "RestExplainer",
                "is_correct": false,
                "explanation": "RestExplainer is not a valid explainer in Azure Machine Learning. It may be a distractor option."
            },
            {
                "id": "D",
                "text": "PFIExplainer",
                "is_correct": false,
                "explanation": "PFIExplainer uses permutation feature importance, which is model-agnostic and not SHAP-based. It is useful but not architecture-specific."
            }
        ],
        "answer": "B",
        "feedback": "The best option is **TabularExplainer**. This explainer automatically chooses the appropriate SHAP algorithm\u2014TreeExplainer, DeepExplainer, or KernelExplainer\u2014based on the model type and data structure. It simplifies the process of generating local and global feature importance for tabular models, making it ideal for Responsible AI practices in Azure ML."
    },
    {
        "question_id": "dp100_responsibleai_tabular_shap",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is looking into architecture-appropriate SHAP algorithms and comes to you for clarification about which explainer uses an architecture-appropriate SHAP algorithm to interpret a model\n\nWhich of the following should you tell her is the best choice?",
        "options": [
            {
                "id": "A",
                "text": "MimicExplainer",
                "is_correct": false,
                "explanation": "MimicExplainer does not use SHAP; it trains a surrogate interpretable model and explains that model instead of the original."
            },
            {
                "id": "B",
                "text": "TabularExplainer",
                "is_correct": true,
                "explanation": "TabularExplainer automatically selects the most appropriate SHAP-based algorithm depending on the model architecture (e.g., TreeExplainer for tree-based models, DeepExplainer for neural networks)."
            },
            {
                "id": "C",
                "text": "RestExplainer",
                "is_correct": false,
                "explanation": "RestExplainer is not a valid explainer in Azure Machine Learning or the interpretability SDK."
            },
            {
                "id": "D",
                "text": "PFIExplainer",
                "is_correct": false,
                "explanation": "PFIExplainer uses permutation feature importance, which is a model-agnostic technique, but it does not use SHAP algorithms."
            }
        ],
        "answer": "B",
        "feedback": "The correct explainer for using SHAP in a way that automatically adapts to the underlying model architecture is TabularExplainer. It intelligently selects the right SHAP algorithm (e.g., TreeExplainer, DeepExplainer) based on the model you provide, making it ideal for architecture-appropriate interpretability in Azure Machine Learning."
    },
    {
        "question_id": "62",
        "type": "multiple_choice_single_answer",
        "question": "You are responsible for creating different deep learning models in your company\n\nYou have trained a multi-class image classification model using PyTorch version 1.2\n\nTo ensure the correct PyTorch version is used during deployment, what should you do?",
        "options": [
            {
                "id": "A",
                "text": "Register the model with a .pt file extension and the default version property.",
                "is_correct": false,
                "explanation": "The file extension alone does not ensure that the correct framework version is used during deployment. This approach lacks explicit version specification."
            },
            {
                "id": "B",
                "text": "Save the model locally as a .pt file, and deploy the model as a local web service.",
                "is_correct": false,
                "explanation": "Saving the model locally and deploying it as a local web service does not integrate with Azure ML's environment management, and thus doesn't ensure correct framework version usage."
            },
            {
                "id": "C",
                "text": "Deploy the model on computer that is configured to use the default Azure Machine Learning conda environment.",
                "is_correct": false,
                "explanation": "Using the default environment doesn't guarantee the correct version of PyTorch will be available, especially if version 1.2 is needed explicitly."
            },
            {
                "id": "D",
                "text": "Register the model, specifying the model_framework and model_framework_version properties.",
                "is_correct": true,
                "explanation": "This is the correct approach. By registering the model and specifying the `model_framework` and `model_framework_version`, Azure ML will ensure that the appropriate version of the framework (PyTorch 1.2 in this case) is used during deployment."
            }
        ],
        "answer": "D",
        "feedback": "When deploying deep learning models in Azure ML, it's important to register the model along with the framework type and version using `model_framework` and `model_framework_version` properties. This ensures that the model is run in an environment with the correct dependencies, avoiding issues during inference. This is especially critical for models that are version-sensitive, like those built with PyTorch."
    },
    {
        "question_id": "67",
        "type": "multiple_choice_single_answer",
        "question": "Which error type should you choose where a person does not have a disease and the model classifies the case as having a disease?",
        "options": [
            {
                "id": "A",
                "text": "True Negative",
                "is_correct": false,
                "explanation": "This refers to a case where the model correctly predicts that a person does not have the disease."
            },
            {
                "id": "B",
                "text": "True Positive",
                "is_correct": false,
                "explanation": "This is when the model correctly predicts that a person has the disease."
            },
            {
                "id": "C",
                "text": "False Positive",
                "is_correct": true,
                "explanation": "A False Positive occurs when the model incorrectly classifies a healthy person as having the disease."
            },
            {
                "id": "D",
                "text": "False Negative",
                "is_correct": false,
                "explanation": "This is when the model incorrectly predicts a person does not have the disease when in fact they do."
            }
        ],
        "answer": "C",
        "feedback": "In binary classification, a **False Positive** means that the model predicted the positive class (in this case, that the person has a disease) when the actual label was negative (the person is healthy). This type of error can lead to unnecessary follow-up procedures or anxiety. It's important to identify and monitor false positives especially in medical use cases, where minimizing them can reduce stress and cost for patients."
    },
    {
        "question_id": "68",
        "type": "multiple_choice_single_answer",
        "question": "Melinda is researching differential privacy and asks you for a short description of how it works\n\nWhich of the following should you tell her?",
        "options": [
            {
                "id": "A",
                "text": "All values in the dataset have salts added to the content which are then encrypted to make the data sterile from a privacy standpoint.",
                "is_correct": false,
                "explanation": "Salting and encryption are cryptographic techniques, not specific to differential privacy. This description is inaccurate for the concept of differential privacy."
            },
            {
                "id": "B",
                "text": "Noise is added to the data during analysis so that aggregations are statistically consistent with the data distribution but non-deterministic.",
                "is_correct": true,
                "explanation": "This accurately describes differential privacy, where controlled noise is added to the outputs of data queries to preserve privacy while ensuring utility of the results."
            },
            {
                "id": "C",
                "text": "All numeric column values in the dataset are converted to the mean value for the column. Analyses of the data use the mean values instead of the actual values.",
                "is_correct": false,
                "explanation": "This describes a form of data obfuscation or anonymization, not differential privacy."
            },
            {
                "id": "D",
                "text": "All numeric values in the dataset are encrypted and cannot be used in analysis.",
                "is_correct": false,
                "explanation": "Encryption protects data in storage or transmission but does not allow analysis in the same way as differential privacy does."
            }
        ],
        "answer": "B",
        "feedback": "Differential privacy is a mathematical framework used to ensure that statistical analysis of a dataset does not compromise the privacy of individuals in the dataset. It works by injecting carefully calibrated random noise into the query results. This allows users to analyze patterns across data while making it difficult to determine if any one individual's information was used. In Microsoft Azure, differential privacy is used in responsible AI practices to ensure models do not leak sensitive information about individuals. Learn more here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-privacy"
    },
    {
        "question_id": "dp100_sweep_job_auc_logging",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure Machine Learning SDK v2 to run a sweep job that tunes hyperparameters for a classification model\n\nThe goal is to optimize the AUC (Area Under the Curve) metric\n\nThe experiment script trains a model using the `y_test` and `y_predicted` variables\n\nA developer includes the following line in the script:\n\n```python\nprint(roc_auc_score(y_test, y_predicted))\n```\n\nYou want the sweep job to optimize based on the AUC value\n\nDoes this script satisfy the requirement to log the metric for hyperparameter tuning?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Printing the AUC value is not sufficient. You must explicitly log the metric using `mlflow.log_metric()` or the `run.log()` function in SDK v1 for the sweep job to use it."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "In SDK v2, metrics must be logged using `mlflow.log_metric()` to be tracked and optimized during a sweep job. Simply printing the metric to console will not make it available to Azure ML for optimization."
            }
        ],
        "answer": "B",
        "feedback": "To ensure the sweep job can use the AUC metric for optimization, you must use `mlflow.log_metric('AUC', auc)` within the training script. This makes the metric visible to Azure ML and usable for hyperparameter optimization. Printing the value is not sufficient."
    },
    {
        "question_id": "dp100_sdkv2_auc_logging",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning SDK v2 to run a Hyperparameter tuning experiment for a classification model\n\nYou want to optimize the AUC metric and ensure the metric is logged correctly to track progress\n\nGiven the validation labels `y_test` and the predicted probabilities `y_predicted`, which line should be added to the script so that the AUC is logged for Hyperparameter optimization?",
        "options": [
            {
                "id": "A",
                "text": "print(f'AUC: {roc_auc_score(y_test, y_predicted)}')",
                "is_correct": false,
                "explanation": "This only prints the metric. It does not log it to Azure ML for hyperparameter optimization."
            },
            {
                "id": "B",
                "text": "mlflow.log_metric('AUC', float(roc_auc_score(y_test, y_predicted)))",
                "is_correct": true,
                "explanation": "This correctly logs the AUC value using MLflow, as expected in SDK v2 experiments."
            },
            {
                "id": "C",
                "text": "run.log('AUC', np.float(roc_auc_score(y_test, y_predicted)))",
                "is_correct": false,
                "explanation": "This syntax is from SDK v1 (using `run.log`) and is not valid in SDK v2."
            },
            {
                "id": "D",
                "text": "mlclient.log_metric('AUC', roc_auc_score(y_test, y_predicted))",
                "is_correct": false,
                "explanation": "`mlclient` does not have a `log_metric` method. Only MLflow is used for logging in SDK v2."
            }
        ],
        "feedback": "To log metrics in Azure ML SDK v2, MLflow is the recommended approach. `mlflow.log_metric(...)` enables Hyperparameter tuning to track and optimize metrics such as AUC during training. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow"
    },
    {
        "question_id": "69",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following is best described by: 'These construct not just one decision tree, but a large number of trees \u2013 allowing better predictions on more complex data\n\nWidely used in machine learning and science due to their strong prediction abilities'?",
        "options": [
            {
                "id": "A",
                "text": "Least Squares Regression",
                "is_correct": false,
                "explanation": "Least Squares Regression is a method for fitting a linear model by minimizing the sum of squared residuals, not a tree-based ensemble method."
            },
            {
                "id": "B",
                "text": "Ensemble Algorithms",
                "is_correct": true,
                "explanation": "Ensemble algorithms, such as Random Forests and Gradient Boosting Machines, combine multiple decision trees to enhance prediction accuracy. They are robust, reduce overfitting, and are widely used in machine learning for both classification and regression tasks."
            },
            {
                "id": "C",
                "text": "Dynamic Programming Algorithms",
                "is_correct": false,
                "explanation": "Dynamic programming is a technique used to solve optimization problems by breaking them into simpler subproblems, not for combining decision trees."
            },
            {
                "id": "D",
                "text": "Linear Regression",
                "is_correct": false,
                "explanation": "Linear regression models the relationship between a dependent variable and one or more independent variables using a straight line, not a set of decision trees."
            }
        ],
        "feedback": "The correct answer is Ensemble Algorithms. These algorithms, especially those based on decision trees like Random Forest and XGBoost, are powerful techniques that improve prediction by aggregating the results of many trees. This makes them more accurate and robust, especially when working with complex datasets. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml#ensemble-models"
    },
    {
        "question_id": "70",
        "type": "multiple_choice_single_answer",
        "question": "Dr\n\nKarl Malus is creating a convolutional neural network and wants to reduce the size of the feature maps that are generated by a convolutional layer\n\nWhat should he do?",
        "options": [
            {
                "id": "A",
                "text": "Make adjustments to weight values during backpropagation",
                "is_correct": false,
                "explanation": "Weight adjustments during backpropagation affect learning but do not change the size of the feature maps."
            },
            {
                "id": "B",
                "text": "Add a pooling layer after the convolutional layer",
                "is_correct": true,
                "explanation": "Pooling layers (like max pooling) reduce the spatial dimensions (width and height) of the feature maps, making the model more computationally efficient and less prone to overfitting."
            },
            {
                "id": "C",
                "text": "Reduce the size of the filter kernel used in the convolutional layer",
                "is_correct": false,
                "explanation": "Reducing the filter size may influence the granularity of feature detection, but does not directly reduce the output feature map size."
            },
            {
                "id": "D",
                "text": "Increase the number of filters in the convolutional layer",
                "is_correct": false,
                "explanation": "Increasing the number of filters increases the depth (number of channels) of the feature maps, not their spatial size."
            }
        ],
        "feedback": "To reduce the size of feature maps in a convolutional neural network, the correct approach is to add a **pooling layer** after the convolutional layer. This technique is standard in CNN architectures for dimensionality reduction and performance improvement. Learn more at: https://learn.microsoft.com/en-us/azure/machine-learning/concept-deep-learning-vs-machine-learning"
    },
    {
        "question_id": "71",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following best describes the purpose of training data?",
        "options": [
            {
                "id": "A",
                "text": "A subset of the data used to \u2018test\u2019 what the algorithm has learned.",
                "is_correct": false,
                "explanation": "This describes the test dataset, which is used to evaluate model performance after training."
            },
            {
                "id": "B",
                "text": "The dataset used in its entirety to \u2018teach\u2019 the algorithm.",
                "is_correct": false,
                "explanation": "Training is done on a subset of the data, not the entire dataset. The rest is used for validation/testing."
            },
            {
                "id": "C",
                "text": "A subset of the data used to \u2018teach\u2019 the algorithm. This is the data that the algorithm will learn from.",
                "is_correct": true,
                "explanation": "Training data is the labeled dataset used to fit the model. It enables the algorithm to learn relationships or patterns in the data."
            },
            {
                "id": "D",
                "text": "The dataset is used to ensure the functionality of the algorithm generates consistently correct answers based on checksums.",
                "is_correct": false,
                "explanation": "This refers to system validation or software testing, not machine learning training."
            }
        ],
        "feedback": "Training data refers to the portion of labeled data used to train or 'teach' the model. During training, the model learns patterns and makes adjustments to minimize prediction errors. It is essential for supervised learning tasks where input-output mappings are learned. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-training-and-scoring"
    },
    {
        "question_id": "dp100_endpoint_realtime_auth_sdkv2",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a real-time image classification model using Azure Machine Learning SDK v2\n\nThe model will be exposed through a secured endpoint that requires key-based authentication\n\nYou need to configure and deploy the service to meet the security requirements\n\nWhich of the following steps will correctly meet the objective?",
        "options": [
            {
                "id": "A",
                "text": "Create a KubernetesOnlineEndpoint with authentication mode set to 'key'. Deploy the model to the endpoint.",
                "is_correct": true,
                "explanation": "This correctly reflects the secure deployment flow in Azure ML SDK v2 using KubernetesOnlineEndpoint with key-based authentication."
            },
            {
                "id": "B",
                "text": "Create a ManagedOnlineEndpoint without authentication and deploy the model.",
                "is_correct": false,
                "explanation": "This does not meet the security requirement since it skips enabling authentication."
            },
            {
                "id": "C",
                "text": "Use AksWebservice and set 'auth_enabled=True' before deploying the model.",
                "is_correct": false,
                "explanation": "This syntax belongs to Azure ML SDK v1 and is not valid in SDK v2."
            },
            {
                "id": "D",
                "text": "Create a batch endpoint and enable identity-based access control.",
                "is_correct": false,
                "explanation": "Batch endpoints are not suitable for real-time inference."
            }
        ],
        "answer": "A",
        "explanation": "In Azure ML SDK v2, for real-time inference using a Kubernetes cluster, you should create a `KubernetesOnlineEndpoint`, set the `auth_mode` to `'key'`, and then deploy the model. This ensures secure access to the model endpoint.",
        "feedback": "To deploy a secure real-time inference model with SDK v2, use either `ManagedOnlineEndpoint` or `KubernetesOnlineEndpoint`. If the scenario requires control over infrastructure (like AKS), use KubernetesOnlineEndpoint and set `auth_mode='key'` for key-based access. [Learn more](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-kubernetes-online-endpoint?view=azureml-api-2)."
    },
    {
        "question_id": "73",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a model as a real-time inferencing service in Azure Machine Learning\n\nWhat functions must the entry script for the service include?",
        "options": [
            {
                "id": "A",
                "text": "Init() and run(raw_data)",
                "is_correct": true,
                "explanation": "The `init()` function initializes the model when the container starts, and the `run(raw_data)` function is called for each inference request. This is the standard structure expected by Azure ML real-time endpoints."
            },
            {
                "id": "B",
                "text": "Main() and predict(raw_data)",
                "is_correct": false,
                "explanation": "`main()` and `predict()` are not recognized by the Azure ML inference interface. Azure ML expects `init()` and `run()` for the entry script."
            },
            {
                "id": "C",
                "text": "Load() and score(raw_data)",
                "is_correct": false,
                "explanation": "These are not the correct function names for Azure ML real-time endpoints. While conceptually similar, the service specifically expects `init()` and `run()`."
            }
        ],
        "answer": "A",
        "explanation": "When deploying a model for real-time inference in Azure Machine Learning (SDK v2), your entry script must define two key functions: `init()` and `run(raw_data)`. The `init()` function is called once when the container is started to load the model into memory. The `run()` function is called each time the endpoint receives a request and processes the input. This structure is essential for Azure ML to know how to initialize and invoke your model properly.",
        "feedback": "Real-time endpoints in Azure ML require a scoring script with `init()` and `run(raw_data)`. For example:\n\n```python\nimport joblib\n\ndef init():\n    global model\n    model = joblib.load('model.joblib')\n\ndef run(data):\n    return model.predict(data)\n```\n\nRead more in the [official documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-model-custom-entry-script?view=azureml-api-2)."
    },
    {
        "question_id": "74",
        "type": "multiple_choice_single_answer",
        "question": "In Azure Machine Learning, datastores are abstractions for cloud data sources\n\nWhich dataset type is described by?\n>The data is read from the dataset as a table\n>You should use this type of dataset when your data is consistently structured and you want to work with it in common tabular data structures, such as Pandas dataframes",
        "options": [
            {
                "id": "A",
                "text": "Script argument",
                "is_correct": false,
                "explanation": "Script arguments are used to pass parameters to the training script but are not related to reading data as structured tables."
            },
            {
                "id": "B",
                "text": "Tabular",
                "is_correct": true,
                "explanation": "Tabular datasets in Azure ML are designed for structured data that can be easily represented in a dataframe. They support operations such as filtering and transformations and are ideal for machine learning tasks."
            },
            {
                "id": "C",
                "text": "Named input",
                "is_correct": false,
                "explanation": "Named inputs are used for referencing datasets or datastores in job configurations but do not represent a dataset type by themselves."
            },
            {
                "id": "D",
                "text": "File",
                "is_correct": false,
                "explanation": "File datasets are used when data needs to be read as raw files. They are not directly loaded as structured dataframes."
            }
        ],
        "answer": "B",
        "explanation": "Tabular datasets in Azure Machine Learning represent structured data, such as CSV or TSV files, that are read into memory as dataframes. This makes them suitable for training and evaluation tasks, where consistent schema and tabular format are expected.",
        "feedback": "The 'Tabular' dataset type is the correct choice when working with structured data formats in Azure ML SDK v2. They are typically loaded as pandas or spark dataframes, which facilitates preprocessing, feature engineering, and training workflows.\n\n\ud83d\udd17 Reference: [Azure ML Tabular Datasets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets)"
    },
    {
        "question_id": "75",
        "type": "multiple_choice_single_answer",
        "question": "You are building a multi-class image classification model using PyTorch 1.3 in Azure Machine Learning SDK v2\n\nYou must run a training script named `train.py` and avoid installing any additional libraries manually\n\nWhat should you do to ensure the environment is correctly set up for training?",
        "options": [
            {
                "id": "A",
                "text": "Create a custom Docker image and use it as the environment in the job.",
                "is_correct": false,
                "explanation": "While this would work, it requires manual effort, which contradicts the requirement to avoid manual setup."
            },
            {
                "id": "B",
                "text": "Use a curated environment that includes PyTorch and specify it in the training job YAML or Command component.",
                "is_correct": true,
                "explanation": "Azure provides curated environments with common frameworks preinstalled, including PyTorch. Using one avoids the need for manual library installation."
            },
            {
                "id": "C",
                "text": "Use a generic Python environment and install PyTorch inside the training script.",
                "is_correct": false,
                "explanation": "Installing packages inside the training script is not recommended and contradicts the requirement."
            },
            {
                "id": "D",
                "text": "Use the SKLearn curated environment to train the PyTorch model.",
                "is_correct": false,
                "explanation": "The SKLearn environment does not include PyTorch, so it's not suitable for training PyTorch models."
            }
        ],
        "answer": "B",
        "explanation": "In Azure ML SDK v2, it's best practice to use curated environments for training. Microsoft provides ready-to-use environments with frameworks like PyTorch preinstalled. This minimizes setup time and ensures compatibility with training scripts like train.py that depend on PyTorch 1.3.",
        "feedback": "For training PyTorch models in Azure ML SDK v2, you should reference a curated environment such as `AzureML-pytorch-1.13-ubuntu20.04-py38-cuda11.7`. This avoids the need to manually manage dependencies. Learn more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-environments-v2"
    },
    {
        "question_id": "76",
        "type": "multiple_choice_single_answer",
        "question": "Your team has deployed a model to an Azure Managed Online Endpoint using Azure Machine Learning SDK v2\n\nYou need to invoke the endpoint using Python code and pass JSON input to it\n\nThe deployed endpoint is named `credit-risk-endpoint`, and the deployment is called `blue`\n\nThe input data is stored in a dictionary called `input_payload`\n\nWhat is the correct method to invoke the endpoint?",
        "options": [
            {
                "id": "A",
                "text": "\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nclient = MLClient(\n    DefaultAzureCredential(),\n    subscription_id,\n    resource_group,\n    workspace\n)\nresponse = client.online_endpoints.invoke(\n    endpoint_name='credit-risk-endpoint',\n    deployment_name='blue',\n    request_body=input_payload\n)\n```",
                "is_correct": true,
                "explanation": "This is the correct method in SDK v2 to invoke an online endpoint with deployment name and request payload."
            },
            {
                "id": "B",
                "text": "\n```python\nfrom azureml.core.webservice import Webservice\nservice = Webservice(name='credit-risk-endpoint', workspace=ws)\npredictions = service.run(input_payload)\n```",
                "is_correct": false,
                "explanation": "This is SDK v1 syntax and is not compatible with the SDK v2 methods or endpoints."
            },
            {
                "id": "C",
                "text": "\n```python\nclient = MLClient(DefaultAzureCredential())\npredictions = client.endpoints.run_json(input_payload)\n```",
                "is_correct": false,
                "explanation": "There is no 'run_json' method in MLClient; the method must use 'invoke' for online endpoints."
            },
            {
                "id": "D",
                "text": "\n```python\nfrom azure.ai.ml import MLClient\nclient = MLClient(DefaultAzureCredential())\nresponse = client.batch_endpoints.invoke(input=input_payload)\n```",
                "is_correct": false,
                "explanation": "Batch endpoints use a different interface. This question refers to real-time (online) inference."
            }
        ],
        "answer": "A",
        "explanation": "In Azure ML SDK v2, online endpoints are invoked using `ml_client.online_endpoints.invoke()` with required parameters: the endpoint name, deployment name, and the JSON-formatted input payload.",
        "feedback": "To perform real-time inference using Azure ML SDK v2, you must use the `invoke()` method from `online_endpoints`. Refer to the official documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints"
    },
    {
        "question_id": "dp100_sdkv2_interpretability_tabular",
        "type": "multiple_choice_single_answer",
        "question": "You trained a regression model in Azure Machine Learning SDK v2\n\nNow you want to explain its predictions using SHAP values in a way that adapts automatically to the model type and produces visualizations for feature importance\n\nWhich component or tool should you use in Azure Machine Learning?",
        "options": [
            {
                "id": "A",
                "text": "Use TabularExplainer with SHAP via azureml.interpret package",
                "is_correct": false,
                "explanation": "This approach is based on SDK v1 and is no longer used in SDK v2 workflows."
            },
            {
                "id": "B",
                "text": "Use the Responsible AI dashboard with RAIInsights",
                "is_correct": true,
                "explanation": "Responsible AI dashboard in SDK v2 provides automated SHAP analysis via RAIInsights, adapting to the model type and showing visualizations for interpretability."
            },
            {
                "id": "C",
                "text": "Use interpretML package from Scikit-learn",
                "is_correct": false,
                "explanation": "interpretML is a standalone Python library, not integrated with Azure ML SDK v2 workflows."
            },
            {
                "id": "D",
                "text": "Enable AutoML explainability toggle and export as JSON",
                "is_correct": false,
                "explanation": "This is a feature available in AutoML runs, but it does not provide a reusable or general method for SHAP-based explanation outside of AutoML runs."
            }
        ],
        "explanation": "In SDK v2, the recommended way to interpret a model is through the Responsible AI dashboard, which uses RAIInsights. It includes SHAP-based explanations automatically adapted to model type, providing global and local feature importance with rich visualizations.",
        "sdk_version": "v2"
    },
    {
        "question_id": "77",
        "type": "multiple_choice_single_answer",
        "question": "You need to create a compute target for training experiments that require a graphical processing unit (GPU)\n\nYou want to be able to scale the compute so that multiple nodes are started automatically as required\n\nWhich kind of compute target should you create?",
        "options": [
            {
                "id": "A",
                "text": "Compute Cluster",
                "is_correct": true,
                "explanation": "A compute cluster allows dynamic scaling, including GPU-enabled virtual machines, making it the ideal choice for scalable model training in Azure ML."
            },
            {
                "id": "B",
                "text": "Inference Cluster",
                "is_correct": false,
                "explanation": "Inference Clusters are intended for model deployment and not for training purposes."
            },
            {
                "id": "C",
                "text": "Compute Instance",
                "is_correct": false,
                "explanation": "Compute Instances are for interactive development (e.g., notebooks) and do not support scaling across multiple nodes."
            }
        ],
        "explanation": "In Azure Machine Learning SDK v2, a **Compute Cluster** is used when you want to scale model training across multiple nodes, including those with GPU. The cluster scales automatically based on job demand, making it ideal for deep learning and large dataset scenarios. Compute Instances are single-node VMs for development, while Inference Clusters are for real-time model deployment.",
        "sdk_version": "v2"
    },
    {
        "question_id": "78",
        "type": "multiple_choice_single_answer",
        "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company\n\nYou need to select a compute target to deploy the workspace\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Service",
                "is_correct": false,
                "explanation": "Azure Container Service (deprecated) is not a supported compute target for deploying an Azure Machine Learning workspace."
            },
            {
                "id": "B",
                "text": "Azure Data Lake Analytics",
                "is_correct": false,
                "explanation": "Azure Data Lake Analytics is a big data analytics service and is not used for deploying Azure ML workspaces."
            },
            {
                "id": "C",
                "text": "Apache Spark for HDInsight",
                "is_correct": false,
                "explanation": "Apache Spark for HDInsight is used for large-scale data processing but is not intended as a compute target for Azure ML workspaces."
            },
            {
                "id": "D",
                "text": "Azure Databricks",
                "is_correct": true,
                "explanation": "Azure Databricks can be used as a compute target in Azure Machine Learning. It integrates with Azure ML for running distributed training jobs and is commonly used for collaborative data science workflows."
            }
        ],
        "feedback": "Azure Databricks is a recommended compute target when working with Azure Machine Learning for large-scale data science and machine learning projects. It supports integration with Azure ML SDK v2 and allows scalable training, distributed computing, and advanced collaboration through notebooks."
    },
    {
        "question_id": "78A",
        "type": "multiple_choice_single_answer",
        "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company\n\nYou need to select a compute target to deploy the workspace\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Service",
                "is_correct": false
            },
            {
                "id": "B",
                "text": "Azure Data Lake Analytics",
                "is_correct": false
            },
            {
                "id": "C",
                "text": "Apache Spark for HDInsight",
                "is_correct": false
            },
            {
                "id": "D",
                "text": "Azure Databricks",
                "is_correct": true
            }
        ],
        "feedback": "Azure Databricks is a powerful analytics platform optimized for big data and machine learning. It integrates natively with Azure Machine Learning and is one of the supported compute targets when creating or attaching a workspace. While other services listed are useful in data engineering or analytics, they do not serve as deployable compute targets for Azure ML workspaces."
    },
    {
        "question_id": "78B",
        "type": "multiple_choice_single_answer",
        "question": "You are developing a data science workspace that uses an Azure Machine Learning service for your company\n\nYou need to select a compute target to deploy the workspace\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Instance",
                "is_correct": true,
                "explanation": "Azure Container Instances (ACI) is a lightweight, serverless compute target ideal for deploying models and workspaces during development and testing phases. It is commonly used for hosting Azure ML services with minimal setup and cost."
            },
            {
                "id": "B",
                "text": "Azure Data Lake Analytics",
                "is_correct": false,
                "explanation": "Azure Data Lake Analytics is a distributed analytics service designed for big data batch processing. It is not a compute target for Azure Machine Learning and cannot host or deploy ML workspaces."
            },
            {
                "id": "C",
                "text": "Apache Spark for HDInsight",
                "is_correct": false,
                "explanation": "Apache Spark on HDInsight is used for large-scale distributed data processing, not for deploying Azure ML workspaces. While Spark can process data for ML, it is not a valid compute target for workspace deployment."
            },
            {
                "id": "D",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": "Azure Databricks is an analytics platform optimized for Apache Spark and can be used to run ML workloads, but it is not used to deploy or host Azure ML workspaces. It integrates with Azure ML but is not a workspace compute target."
            }
        ],
        "feedback": "The correct answer is Azure Container Service (ACI), which is used as a lightweight compute target to deploy Azure Machine Learning models or workspaces, especially during development. The other options are analytics services or data processing engines and are not valid for deploying Azure ML workspaces."
    },
    {
        "question_id": "79",
        "type": "multiple_choice_single_answer",
        "question": "The team has trained a classification model using the scikit-learn LogisticRegression class\n\nThey want to use the model to return labels for new data in the array x_new\n\nWhich code from the following list should they use?",
        "options": [
            {
                "id": "A",
                "text": "Model.score(x_new, y_new)",
                "is_correct": false,
                "explanation": "The score() method returns the accuracy of the model on given test data and labels. It does not return predictions, but rather a performance metric."
            },
            {
                "id": "B",
                "text": "Model.predict(x_new)",
                "is_correct": true,
                "explanation": "The predict() method is used to return predicted class labels for the input samples (x_new). It is the correct method to use when you want the model to make predictions."
            },
            {
                "id": "C",
                "text": "Model.fit(x_new)",
                "is_correct": false,
                "explanation": "The fit() method is used to train the model, not to make predictions. Calling it here would attempt to retrain the model rather than infer outcomes."
            },
            {
                "id": "D",
                "text": "Model.fit(x_new, y_new)",
                "is_correct": false,
                "explanation": "This is also for training the model. Since the model is already trained, using fit() again is unnecessary and would overwrite previous training."
            }
        ],
        "answer": "B",
        "explanation": "To generate predictions using a trained scikit-learn LogisticRegression model, you must use the predict() method and pass in the new data array. This method returns the predicted labels. Using fit() would retrain the model, and score() only returns accuracy metrics, not label predictions.",
        "references": [
            "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict"
        ]
    },
    {
        "question_id": "80",
        "type": "multiple_choice_single_answer",
        "question": "The team is solving a classification task and must evaluate the current model on a limited data sample by using k-fold cross-validation\n\nThe data scientist starts by configuring a k parameter as the number of splits and needs to configure the k parameter for the cross-validation\n\nWhich of the following values should they use?",
        "options": [
            {
                "id": "A",
                "text": "K=10",
                "is_correct": true,
                "explanation": "K=10 is a commonly used and recommended value for k-fold cross-validation, especially when working with limited data. It provides a good tradeoff between bias and variance."
            },
            {
                "id": "B",
                "text": "K=0.5",
                "is_correct": false,
                "explanation": "K must be an integer greater than 1 representing the number of folds, not a float value. 0.5 is not valid for k-fold cross-validation."
            },
            {
                "id": "C",
                "text": "K=1",
                "is_correct": false,
                "explanation": "K=1 is invalid because cross-validation requires at least 2 splits. Using 1 would mean no actual validation step occurs."
            },
            {
                "id": "D",
                "text": "K=0.9",
                "is_correct": false,
                "explanation": "K must be an integer. 0.9 is a float and not acceptable as a parameter for the number of splits in k-fold cross-validation."
            }
        ],
        "answer": "A",
        "explanation": "In k-fold cross-validation, the dataset is split into k equal-sized parts. The model is trained on k-1 parts and validated on the remaining part. A common default value is k=10 because it gives a reliable estimate of model performance without being too computationally expensive.",
        "references": [
            "https://scikit-learn.org/stable/modules/cross_validation.html#k-fold"
        ]
    },
    {
        "question_id": "82",
        "type": "multiple_choice_single_answer",
        "question": "Identify the missing word(s) in the following sentence within the context of Microsoft Azure\n\nAfter installing the SDK package in your Python environment, you can write code to connect to your workspace and perform machine learning operations\n\nThe easiest way to connect to a workspace is to use a [?]\n\n",
        "options": [
            {
                "id": "A",
                "text": "VPN",
                "is_correct": false,
                "explanation": "A VPN provides a secure network connection but is not used for connecting to an Azure Machine Learning workspace via SDK."
            },
            {
                "id": "B",
                "text": "Workspace configuration file",
                "is_correct": true,
                "explanation": "The workspace configuration file (config.json) contains all necessary identifiers to connect to an Azure ML workspace easily using the SDK."
            },
            {
                "id": "C",
                "text": "Azure endpoint",
                "is_correct": false,
                "explanation": "An Azure endpoint may refer to deployed model endpoints or services, but it is not used to initialize SDK workspace connections."
            },
            {
                "id": "D",
                "text": "SFTP",
                "is_correct": false,
                "explanation": "SFTP is a protocol used for secure file transfer, unrelated to Azure ML workspace authentication or SDK usage."
            }
        ],
        "answer": "B",
        "feedback": "To authenticate and connect to your Azure Machine Learning workspace using the SDK, the simplest method is using the workspace configuration file (usually named config.json). This file includes your subscription ID, resource group, and workspace name, and is loaded by the SDK to establish a secure and convenient connection."
    },
    {
        "question_id": "83",
        "type": "multiple_choice_single_answer",
        "question": "You are using an Azure Machine Learning designer pipeline to train and test a K-Means clustering model\n\nYou want the model to assign items to one of three clusters\n\nWhich configuration property of the K-Means Clustering module should the team set to accomplish this?",
        "options": [
            {
                "id": "A",
                "text": "Set Iterations to 3",
                "is_correct": false,
                "explanation": "Setting iterations controls how many times the algorithm repeats the clustering process, but it doesn't define the number of clusters."
            },
            {
                "id": "B",
                "text": "Set Number of Centroids to 3",
                "is_correct": true,
                "explanation": "This property defines how many clusters (or centroids) the K-Means algorithm should form. Setting it to 3 will produce three clusters."
            },
            {
                "id": "C",
                "text": "Set Random number seed to 3",
                "is_correct": false,
                "explanation": "The random number seed controls randomness in centroid initialization, but not the number of clusters."
            },
            {
                "id": "D",
                "text": "Set the Km value to 3",
                "is_correct": false,
                "explanation": "There is no configuration called 'Km' in the Azure ML designer for K-Means."
            }
        ],
        "answer": "B",
        "feedback": "In K-Means clustering, the number of centroids determines how many clusters the algorithm will identify. Setting 'Number of Centroids to 3' tells the algorithm to split the data into 3 distinct groups based on feature similarity. The other options control parameters like iteration count or randomness but not the actual number of clusters."
    },
    {
        "question_id": "84",
        "type": "multiple_choice_single_answer",
        "question": "You are responsible to create different Azure Machine Learning models for your company\n\nYou are creating a binary classification by using a two-class logistic regression model\n\nYou need to evaluate the model results for imbalance\n\nWhich evaluation metric should you use?",
        "options": [
            {
                "id": "A",
                "text": "Mean Absolute Error",
                "is_correct": false,
                "explanation": "This is a regression metric that measures the average magnitude of errors in predictions. It is not suitable for evaluating classification models."
            },
            {
                "id": "B",
                "text": "Relative Absolute Error",
                "is_correct": false,
                "explanation": "This is another regression-specific metric. It is not appropriate for assessing the performance of classification models."
            },
            {
                "id": "C",
                "text": "AUC Curve",
                "is_correct": true,
                "explanation": "The AUC (Area Under the Curve) metric evaluates the model\u2019s ability to distinguish between classes, especially in imbalanced classification tasks. It is a common and effective metric for binary classification evaluation."
            },
            {
                "id": "D",
                "text": "Relative Squared Error",
                "is_correct": false,
                "explanation": "This metric is used in regression tasks and does not provide insight into classification performance, especially for imbalanced datasets."
            }
        ],
        "feedback": "AUC (Area Under the ROC Curve) is the most appropriate metric for evaluating classification performance, particularly in the case of imbalanced datasets. It measures how well the model distinguishes between the two classes, making it ideal for logistic regression evaluation tasks. The other metrics listed are designed for regression tasks and are not suitable for classification problems."
    },
    {
        "question_id": "85",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a machine learning model\n\nYou have a dataset that contains null rows\n\nYou need to use the Clean Missing Data module in Azure Machine Learning Studio to identify and resolve the null and missing data in the dataset\n\nWhich parameter should you use?",
        "options": [
            {
                "id": "A",
                "text": "Replace with mean",
                "is_correct": false,
                "explanation": "Replacing with the mean is used to fill null values in numeric columns, not entire null rows."
            },
            {
                "id": "B",
                "text": "Replace with mode",
                "is_correct": false,
                "explanation": "Mode substitution is appropriate for categorical variables, not for addressing entire null rows."
            },
            {
                "id": "C",
                "text": "Custom substitution value",
                "is_correct": false,
                "explanation": "Custom values can be used to fill specific missing entries, not for removing null rows entirely."
            },
            {
                "id": "D",
                "text": "Remove entire column",
                "is_correct": false,
                "explanation": "This removes columns, not rows. It\u2019s not applicable when rows are the issue."
            },
            {
                "id": "E",
                "text": "Remove entire row",
                "is_correct": true,
                "explanation": "This is the correct option when rows contain null values. The Clean Missing Data module can remove rows that have missing data."
            },
            {
                "id": "F",
                "text": "Hot Deck",
                "is_correct": false,
                "explanation": "Hot Deck is not a standard option in Azure ML Studio's Clean Missing Data module."
            }
        ],
        "feedback": "When your dataset has entire rows with null values, the appropriate way to clean the data using the Clean Missing Data module is to remove those rows. This avoids introducing bias from imputation and ensures only complete records are used for training."
    },
    {
        "question_id": "87",
        "type": "multiple_choice_single_answer",
        "question": "You are junior data scientist of your company\n\nYou are building a machine learning model for translating English language textual content into French language textual content\n\nYou need to build and train the machine learning model to learn the sequence of the textual content\n\nWhich type of neural network should you use?",
        "options": [
            {
                "id": "A",
                "text": "Multilayer Perceptions (MLPs)",
                "is_correct": false,
                "explanation": "MLPs are feed-forward networks and do not handle sequential data well since they lack memory of previous inputs."
            },
            {
                "id": "B",
                "text": "Convolutional Neural Networks (CNNs)",
                "is_correct": false,
                "explanation": "CNNs are effective for image processing tasks but are not suited for modeling sequential dependencies in text."
            },
            {
                "id": "C",
                "text": "Recurrent Neural Networks (RNNs)",
                "is_correct": true,
                "explanation": "RNNs are designed to handle sequential data like text, making them well-suited for translation tasks where order and context matter."
            },
            {
                "id": "D",
                "text": "Generative Adversarial Networks (GANs)",
                "is_correct": false,
                "explanation": "GANs are typically used for generating realistic synthetic data, like images, and are not ideal for sequence-to-sequence tasks such as translation."
            }
        ],
        "feedback": "RNNs are ideal for natural language processing tasks that require learning from sequential data such as text translation. They maintain a memory of previous tokens while processing a sequence, allowing them to capture linguistic patterns. While newer architectures like Transformers outperform RNNs in modern NLP, the question focuses on identifying traditional approaches, and RNNs remain a correct conceptual choice."
    },
    {
        "question_id": "88",
        "type": "multiple_choice_single_answer",
        "question": "You are performing feature engineering on a dataset\n\nYou must add a feature named CityName and populate the column value with the text London\n\nYou need to add the new feature to the dataset\n\nWhich Azure Machine Learning Studio module should you use?",
        "options": [
            {
                "id": "A",
                "text": "Apply SQL Transformation",
                "is_correct": true,
                "explanation": "SQL Transformation allows you to add custom columns and set static values using SQL syntax. It's the most appropriate module for adding a column with a constant value."
            },
            {
                "id": "B",
                "text": "Extract N-Gram Features from Text",
                "is_correct": false,
                "explanation": "This module is used to extract token combinations (n-grams) from text for feature extraction in NLP tasks, not to add or modify dataset structure."
            },
            {
                "id": "C",
                "text": "Preprocess Text",
                "is_correct": false,
                "explanation": "Preprocess Text is for cleaning and preparing existing text columns (e.g., removing stop words or punctuation), not for adding new features to a dataset."
            },
            {
                "id": "D",
                "text": "Edit Metadata",
                "is_correct": false,
                "explanation": "Edit Metadata is used to change the metadata (e.g., data type or column roles) of existing columns, not to add new ones."
            }
        ],
        "feedback": "To add a new feature column with a specific static value in Azure Machine Learning Studio, the Apply SQL Transformation module is the best choice. It enables SQL-like operations including the creation of new columns and assignment of values based on expressions. The other modules are useful for modifying or interpreting existing data but not for structural changes to the dataset."
    },
    {
        "question_id": "89",
        "type": "multiple_choice_single_answer",
        "question": "Identify the correct sentence completion within the context of Microsoft Azure Machine Learning\n\nCompute Instances in Azure ML provide a fully managed and integrated environment for development\n\nThese instances include pre-installed tools for code editing and interactive development\n\nWhich combination of tools is available out-of-the-box to support real-time experimentation, visualization, and integration with other Azure ML assets?",
        "options": [
            {
                "id": "A",
                "text": "[A] Excel Online, [B] Visual Studio Code Spaces",
                "is_correct": false,
                "explanation": "Excel Online and Visual Studio Code Spaces are not the standard tools pre-installed on Compute Instances in Azure ML environments."
            },
            {
                "id": "B",
                "text": "[A] Azure Data Explorer, [B] Azure Notebooks",
                "is_correct": false,
                "explanation": "Azure Data Explorer and Azure Notebooks are separate services and are not directly included or pre-installed on Compute Instances."
            },
            {
                "id": "C",
                "text": "[A] Jupyter Notebook, [B] JupyterLab",
                "is_correct": true,
                "explanation": "Jupyter Notebook and JupyterLab are both pre-installed on Azure Machine Learning Compute Instances and are the primary tools for interactive experimentation and development."
            },
            {
                "id": "D",
                "text": "[A] Databricks Workspace, [B] Stream Analytics",
                "is_correct": false,
                "explanation": "Databricks and Stream Analytics are separate Azure services and not part of the default Compute Instance setup."
            }
        ],
        "feedback": "Azure ML Compute Instances provide an integrated development environment with pre-installed tools like Jupyter Notebook and JupyterLab. These tools allow data scientists to quickly start experimenting, visualizing data, and developing machine learning workflows with minimal setup."
    },
    {
        "question_id": "90",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist creating a linear regression model\n\nYou need to determine how closely the data fits the regression line\n\nWhich metric should you review?",
        "options": [
            {
                "id": "A",
                "text": "Recall",
                "is_correct": false,
                "explanation": "Recall is used for classification tasks, not regression. It measures the proportion of actual positives correctly identified."
            },
            {
                "id": "B",
                "text": "Root Mean Square Error",
                "is_correct": false,
                "explanation": "RMSE measures the average magnitude of the error but doesn't directly indicate how well the data fits the regression line."
            },
            {
                "id": "C",
                "text": "Mean absolute error",
                "is_correct": false,
                "explanation": "MAE measures average absolute differences between predictions and actual values, but it doesn\u2019t reflect how much of the variance is explained by the model."
            },
            {
                "id": "D",
                "text": "Coefficient of determination",
                "is_correct": true,
                "explanation": "The coefficient of determination (R\u00b2) measures how well the model explains the variability of the target variable. It's the most appropriate metric for assessing the goodness-of-fit in regression."
            }
        ],
        "feedback": "The coefficient of determination (R\u00b2) is the standard metric for evaluating how well a regression model fits the data. It represents the proportion of variance in the dependent variable that is predictable from the independent variables. Although metrics like RMSE and MAE assess prediction error, they do not capture how well the model explains the underlying data distribution. R\u00b2 provides this insight, making it the most appropriate choice in this context."
    },
    {
        "question_id": "91",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning SDK v2 to monitor data drift\n\nYou have registered a baseline dataset (dataset1) and a target dataset (dataset2)\n\nYou want to schedule the drift detection to run every week and track changes in a specific table\n\nWhich configuration object should you use to set up this monitoring schedule?",
        "options": [
            {
                "id": "A",
                "text": "DataDriftDetector.run()",
                "is_correct": false,
                "explanation": "This method belongs to SDK v1 and is not applicable in SDK v2."
            },
            {
                "id": "B",
                "text": "ml_client.data_drift.create_schedule()",
                "is_correct": false,
                "explanation": "This method does not exist in the SDK v2 and would raise an error."
            },
            {
                "id": "C",
                "text": "MonitorSchedule()",
                "is_correct": true,
                "explanation": "Correct. In SDK v2, you use MonitorSchedule to define when and how often to trigger drift monitoring."
            },
            {
                "id": "D",
                "text": "DataQualitySignal()",
                "is_correct": false,
                "explanation": "This is used to monitor data quality, not specifically data drift."
            }
        ],
        "feedback": "To monitor data drift in SDK v2, you define a MonitorSchedule object and associate it with a Monitor and a DataDriftSignal configuration. This enables you to periodically compare a baseline dataset to a target dataset. MonitorSchedule defines the frequency of the monitoring job."
    },
    {
        "question_id": "91A",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning SDK v2 to monitor data drift\n\nYou have registered a baseline dataset (dataset1) and a target dataset (dataset2)\n\nYou want to schedule the drift detection to run every week and track changes in a specific table\n\nWhich configuration object should you use to set up this monitoring schedule?",
        "options": [
            {
                "id": "A",
                "text": "DataDriftDetector.run()",
                "is_correct": false,
                "explanation": "This method belongs to SDK v1 and is not applicable in SDK v2."
            },
            {
                "id": "B",
                "text": "ml_client.data_drift.create_schedule()",
                "is_correct": false,
                "explanation": "This method does not exist in the SDK v2 and would raise an error."
            },
            {
                "id": "C",
                "text": "MonitorSchedule()",
                "is_correct": true,
                "explanation": "Correct. In SDK v2, you use MonitorSchedule to define when and how often to trigger drift monitoring."
            },
            {
                "id": "D",
                "text": "DataQualitySignal()",
                "is_correct": false,
                "explanation": "This is used to monitor data quality, not specifically data drift."
            }
        ],
        "feedback": "To monitor data drift in SDK v2, you define a MonitorSchedule object and associate it with a Monitor and a DataDriftSignal configuration. This enables you to periodically compare a baseline dataset to a target dataset. MonitorSchedule defines the frequency of the monitoring job."
    },
    {
        "question_id": "92",
        "type": "multiple_choice_single_answer",
        "question": "True or False: A major difference between clustering and classification models is that clustering is a \u2018supervised\u2019 method, where \u2018training\u2019 is done with labels\n\n",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "This is incorrect because clustering is an unsupervised learning method, meaning it does not rely on labeled data during training."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Correct. Clustering is an unsupervised learning technique where the algorithm groups data without prior labels, unlike classification, which is supervised and requires labeled data."
            }
        ],
        "feedback": "Clustering is an unsupervised method used to group similar data points based on features without requiring labeled outputs. Classification, on the other hand, is supervised and learns from input-output pairs. Therefore, saying clustering is supervised is false."
    },
    {
        "question_id": "93",
        "type": "multiple_choice_single_answer",
        "question": "You are working with Azure Machine Learning SDK v2 and have a workspace already configured using MLClient as `ml_client`\n\nYou want to retrieve the default datastore registered in the workspace\n\nWhich line of code should you use?",
        "options": [
            {
                "id": "A",
                "text": "default_ds = ml_client.datastores.get('workspaceblobstore')",
                "is_correct": true,
                "explanation": "This is the correct way to access the default datastore in Azure ML SDK v2, assuming the default is named 'workspaceblobstore'."
            },
            {
                "id": "B",
                "text": "default_ds = ml_client.get_default_datastore()",
                "is_correct": false,
                "explanation": "This is incorrect. There is no `get_default_datastore()` method in SDK v2."
            },
            {
                "id": "C",
                "text": "default_ds = ml_client.workspace.datastore",
                "is_correct": false,
                "explanation": "This syntax is invalid in SDK v2 and will raise an attribute error."
            },
            {
                "id": "D",
                "text": "default_ds = Workspace.get_default_datastore()",
                "is_correct": false,
                "explanation": "This method belongs to SDK v1, not SDK v2."
            }
        ],
        "feedback": "In Azure ML SDK v2, datastores are managed using the `ml_client.datastores` interface. The default datastore is usually named 'workspaceblobstore', and you can retrieve it using `ml_client.datastores.get('workspaceblobstore')`. Unlike SDK v1, there is no method like `get_default_datastore()`."
    },
    {
        "question_id": "94",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a model to predict the price of a student\u2019s artwork depending on the following variables: the student\u2019s length of education, degree type, and art form\n\nYou start by creating a linear regression model\n\nYou need to evaluate the linear regression model\n\nSolution: Use the following metrics: Relative Squared Error, Coefficient of Determination, Accuracy, Precision, Recall, F1 score, and AUC\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Incorrect. The majority of the metrics listed are for classification tasks, not regression. Only RSE and Coefficient of Determination are suitable for regression."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Correct. Accuracy, Precision, Recall, F1 score, and AUC are classification metrics, not appropriate for evaluating a linear regression model."
            }
        ],
        "feedback": "To evaluate a linear regression model, you should use metrics designed for regression such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R\u00b2). Accuracy, Precision, Recall, F1 score, and AUC are applicable only for classification tasks, so the solution does not meet the goal."
    },
    {
        "question_id": "95",
        "type": "multiple_choice_single_answer",
        "question": "You are using automated machine learning, and you want to determine the influence of features on the predictions made by the best model produced by the automated machine learning experiment\n\nWhat must you do when configuring the automated machine learning experiment?",
        "options": [
            {
                "id": "A",
                "text": "Enable featurization.",
                "is_correct": false,
                "explanation": "Enabling featurization allows preprocessing of data but does not provide insights into feature influence."
            },
            {
                "id": "B",
                "text": "Enable model explainability.",
                "is_correct": true,
                "explanation": "Correct. To understand how each feature contributes to the model predictions in AutoML, you must enable model explainability."
            },
            {
                "id": "C",
                "text": "Whitelist only tree-based algorithms.",
                "is_correct": false,
                "explanation": "Whitelisting tree-based algorithms may improve interpretability, but it is not required to determine feature influence. Model explainability should be explicitly enabled."
            }
        ],
        "feedback": "To examine feature influence in an Azure AutoML experiment, you must enable **model explainability** during configuration. This activates SHAP-based interpretations that let you understand the contribution of each feature in the best-performing model. Featurization and algorithm selection are separate concerns and not sufficient for generating explanations."
    },
    {
        "question_id": "96",
        "type": "multiple_choice_single_answer",
        "question": "You have a comma-separated values (CSV) file containing data from which you want to train a classification model\n\nYou are using the Automated Machine Learning interface in Azure Machine Learning studio to train the classification model\n\nYou set the task type to Classification\n\nYou need to ensure that the Automated Machine Learning process evaluates only linear models\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Add all algorithms other than linear ones to the blocked algorithms list.",
                "is_correct": true,
                "explanation": "Correct. Blocking all non-linear algorithms forces AutoML to evaluate only linear models during the experiment."
            },
            {
                "id": "B",
                "text": "Clear the option to enable deep learning.",
                "is_correct": false,
                "explanation": "This disables deep learning models, but does not restrict tree-based or other non-linear algorithms."
            },
            {
                "id": "C",
                "text": "Set the Exit criterion option to a metric score threshold.",
                "is_correct": false,
                "explanation": "This controls when the experiment stops but does not restrict algorithm selection."
            },
            {
                "id": "D",
                "text": "Set the task type to Regression.",
                "is_correct": false,
                "explanation": "This changes the task type, which is not the goal. The task type should remain Classification."
            },
            {
                "id": "E",
                "text": "Clear the option to perform automatic featurization.",
                "is_correct": false,
                "explanation": "Featurization impacts preprocessing but not the choice of algorithms evaluated."
            }
        ],
        "feedback": "To ensure AutoML evaluates only linear models, you must explicitly block all non-linear algorithms in the configuration. Disabling deep learning or featurization does not guarantee the exclusion of non-linear methods like tree-based models."
    },
    {
        "question_id": "97",
        "type": "multiple_choice_single_answer",
        "question": "Which of the following is best described by: 'A step-by-step approach to predicting a variable\n\nFor example, this may be first split between Spring/Summer and Autumn/Winter to make a prediction based on the day of the week\n\nSpring/Summer-Monday may have a sales rate of 100 units per day, while Autumn/Winter-Monday may have a sales rate of 20 units per day'?",
        "options": [
            {
                "id": "A",
                "text": "Decision Trees",
                "is_correct": true,
                "explanation": "Decision Trees work by recursively splitting the data based on feature values, making them an intuitive, step-by-step prediction method."
            },
            {
                "id": "B",
                "text": "Linear Regression",
                "is_correct": false,
                "explanation": "Linear regression assumes a linear relationship between variables and does not involve recursive partitioning."
            },
            {
                "id": "C",
                "text": "Ensemble Algorithms",
                "is_correct": false,
                "explanation": "While ensemble methods often use decision trees, the question focuses on a single step-by-step decision-making approach."
            },
            {
                "id": "D",
                "text": "Dynamic Programming Algorithms",
                "is_correct": false,
                "explanation": "Dynamic programming is used in optimization and operations research, not for step-by-step predictive modeling."
            }
        ],
        "feedback": "The scenario describes how a model splits data based on conditions to make predictions, which is the core mechanism behind Decision Trees. These models are ideal for capturing non-linear relationships in a simple, interpretable manner."
    },
    {
        "question_id": "98",
        "type": "multiple_choice_single_answer",
        "question": "You need to run a script that trains a deep neural network (DNN) model and logs the loss and accuracy metrics\n\nYou decide to run the training script as an experiment on the aks-cluster compute target\n\nWill the solution work properly?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "AKS is designed for model inference, not for training. Training jobs should use a compute cluster created for training purposes (Azure ML Compute)."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Training cannot be executed on AKS. Instead, use an Azure ML Compute cluster configured for training workloads, such as NC-series VMs."
            }
        ],
        "feedback": "AKS (Azure Kubernetes Service) is a compute target designed for deploying models and serving real-time predictions. It does not support the execution of training jobs. To train a DNN model, you should use an Azure ML Compute cluster (like NC-series VMs with GPU support) created explicitly for training workloads."
    },
    {
        "question_id": "99",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure ML SDK v2 to define a batch inference pipeline\n\nYou configure a command job that performs inference over a dataset using a compute cluster\n\nThe output is defined as an MLTable using Output(type='uri_folder')\n\nAfter job execution, where can you find the inference results?",
        "options": [
            {
                "id": "A",
                "text": "In the job's Outputs tab in Azure ML Studio under the named output folder",
                "is_correct": true,
                "feedback": "Correct. In SDK v2, batch inference jobs save their outputs to a specified URI folder. These results are visible in the Azure ML Studio under the Outputs tab of the executed job."
            },
            {
                "id": "B",
                "text": "In the Logs tab of the compute target",
                "is_correct": false,
                "feedback": "Incorrect. Logs are stored separately from outputs and are used primarily for debugging purposes."
            },
            {
                "id": "C",
                "text": "In the Environment configuration JSON file",
                "is_correct": false,
                "feedback": "Incorrect. The environment definition does not contain job output results."
            },
            {
                "id": "D",
                "text": "In the Compute clusters dashboard under 'Data outputs'",
                "is_correct": false,
                "feedback": "Incorrect. Outputs are associated with the job, not directly with the compute cluster dashboard."
            }
        ]
    },
    {
        "question_id": "100",
        "type": "multiple_choice_single_answer",
        "question": "Your team is building a data engineering and data science development environment\n\nThe environment must support the following requirements:\n- support Python and Scala\n- compose data storage, movement, and processing services into automated data pipelines\n- the same tool should be used for the orchestration of both data engineering and data science\n- support workload isolation and interactive workloads\n- enable scaling across a cluster of machines\nYou need to create the environment\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Build the environment in Azure Databricks and use Azure Data Factory for orchestration.",
                "is_correct": true,
                "feedback": "Correct. Azure Databricks is ideal for scalable data engineering and data science workloads with Python and Scala. Azure Data Factory supports orchestration of data workflows, enabling the creation of automated pipelines that integrate with Databricks clusters."
            },
            {
                "id": "B",
                "text": "Build the environment in Apache Hive for HDInsight and use Azure Data Factory for orchestration.",
                "is_correct": false,
                "feedback": "Incorrect. Apache Hive is focused on SQL-based querying of large datasets and is not ideal for the interactive and programming-intensive workflows typical in data science."
            },
            {
                "id": "C",
                "text": "Build the environment in Apache Spark for HDInsight and use Azure Container Instances for orchestration.",
                "is_correct": false,
                "feedback": "Incorrect. While Apache Spark supports big data processing, Azure Container Instances are not a suitable tool for orchestrating complex data pipelines compared to Azure Data Factory."
            },
            {
                "id": "D",
                "text": "Build the environment in Azure Databricks and use Azure Container Instances for orchestration.",
                "is_correct": false,
                "feedback": "Incorrect. Azure Databricks is appropriate, but Azure Container Instances do not offer orchestration capabilities like scheduling, dependency management, or integration with multiple data services."
            }
        ]
    },
    {
        "question_id": "101",
        "type": "multiple_choice_single_answer",
        "question": "The Daily Bugle is a news organization led by J\n\nJonah Jameson\n\nOne of the current projects is creating a new experiment in Azure Machine Learning Studio\n\nGiven: One class has a much smaller number of observations than the other classes in the training set\n\nRequired: Select an appropriate data sampling strategy to compensate for the class imbalance\n\nThe lead developer, Peter Parker, has used the Stratified split for the sampling mode\n\nWill Peter\u2019s solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "feedback": "Incorrect. Stratified split ensures that each split has the same distribution of target classes as the original dataset, but it does not address class imbalance directly. It is not a data sampling strategy such as oversampling or undersampling, which are better suited to tackle class imbalance."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "feedback": "Correct. While stratified sampling preserves the class distribution, it does not mitigate class imbalance. To meet the goal, Peter should consider using oversampling (e.g., SMOTE) or undersampling techniques specifically designed to balance class representation in the training data."
            }
        ]
    },
    {
        "question_id": "102",
        "type": "multiple_choice_single_answer",
        "question": "Roxxon Energy Corporation is deploying a model as a real-time inferencing service\n\nWhat functions must the entry script for the service include?",
        "options": [
            {
                "id": "A",
                "text": "Init() and run()",
                "is_correct": true,
                "feedback": "Correct. In Azure Machine Learning, the entry script for a real-time inference service must implement two specific functions: `init()` to initialize the model and `run()` to handle incoming prediction requests. This structure ensures the model is properly loaded and used during inference."
            },
            {
                "id": "B",
                "text": "Base() and train()",
                "is_correct": false,
                "feedback": "Incorrect. These functions are not part of the standard interface for inference entry scripts in Azure ML. `train()` is used during training, not inference."
            },
            {
                "id": "C",
                "text": "Run() and score()",
                "is_correct": false,
                "feedback": "Incorrect. Only `init()` and `run()` are required for inference in Azure ML. `score()` is not used in this context."
            },
            {
                "id": "D",
                "text": "Main() and score()",
                "is_correct": false,
                "feedback": "Incorrect. `main()` is not used for inference entry scripts in Azure ML. The correct functions are `init()` and `run()`."
            }
        ]
    },
    {
        "question_id": "103",
        "type": "multiple_choice_single_answer",
        "question": "When hyperparameter tuning, what is true about Bayesian sampling?",
        "options": [
            {
                "id": "A",
                "text": "You can only use it with uniform, choice, and quniform parameter expressions",
                "is_correct": true,
                "explanation": "Bayesian sampling in Azure ML SDK v2 supports parameter expressions like Uniform, Choice, and QUniform, as it builds a probabilistic model to guide the search. These types of parameter spaces are required for the algorithm to infer and propose new configurations effectively."
            },
            {
                "id": "B",
                "text": "It's the slowest hyperparameter tuning process",
                "is_correct": false,
                "explanation": "This is not necessarily true. Bayesian sampling can converge faster than random or grid search because it uses previous trials to inform the next suggestions."
            },
            {
                "id": "C",
                "text": "It always finds the best tuned model, albeit being the slowest",
                "is_correct": false,
                "explanation": "While Bayesian methods are effective, there is no guarantee they will always find the best model. Also, it's not always the slowest."
            },
            {
                "id": "D",
                "text": "You can combine it with an early termination policy",
                "is_correct": false,
                "explanation": "Bayesian sampling is typically not used with early termination policies in Azure ML SDK v2, as it requires full evaluations of trials to update the model for proposing new configurations."
            }
        ],
        "feedback": "Bayesian sampling is a popular method for hyperparameter optimization that builds a surrogate model to select promising configurations. In Azure Machine Learning SDK v2, it supports only specific types of parameter expressions such as Uniform, Choice, and QUniform. This ensures the sampling algorithm can work with a meaningful probability distribution over the search space.",
        "correct_answer": "A"
    },
    {
        "question_id": "104",
        "type": "multiple_choice_single_answer",
        "question": "What is not a valid setting you can toggle when creating a model using Automated ML in Azure Machine Learning?",
        "options": [
            {
                "id": "A",
                "text": "Restrict to only using a portion of the data",
                "is_correct": true,
                "explanation": "While you can select a training-validation split, there is no explicit toggle in AutoML to restrict training to only a subset of the data arbitrarily. Data sampling must be handled before launching AutoML."
            },
            {
                "id": "B",
                "text": "Setting the exit criterion at 1 hour time elapsed",
                "is_correct": false,
                "explanation": "Azure AutoML allows you to set the exit criteria based on time elapsed, such as stopping after 1 hour."
            },
            {
                "id": "C",
                "text": "Setting the primary metric to R2",
                "is_correct": false,
                "explanation": "You can specify R2 as the primary metric when the task is regression."
            },
            {
                "id": "D",
                "text": "Restricting all algorithms except for Gradient Boost",
                "is_correct": false,
                "explanation": "Azure AutoML lets you whitelist or blacklist specific algorithms, including using only Gradient Boost if desired."
            }
        ],
        "correct_answer": "A",
        "feedback": "AutoML in Azure Machine Learning supports many configuration options, including setting primary metrics, defining allowed algorithms, and setting exit criteria. However, restricting to an arbitrary portion of the data isn't a supported toggle; data limitations must be pre-processed manually."
    },
    {
        "question_id": "105",
        "type": "multiple_choice_single_answer",
        "question": "What is true about Compute clusters?",
        "options": [
            {
                "id": "A",
                "text": "Compute clusters can only have CPUs",
                "is_correct": false,
                "explanation": "Compute clusters can be configured with both CPUs and GPUs, depending on your workload requirements."
            },
            {
                "id": "B",
                "text": "Compute clusters scale the number of nodes automatically",
                "is_correct": true,
                "explanation": "Azure Machine Learning compute clusters can automatically scale up or down depending on the workload demand, which optimizes cost and performance."
            },
            {
                "id": "C",
                "text": "Compute clusters have more downtime than Compute instances",
                "is_correct": false,
                "explanation": "Compute clusters do not inherently have more downtime; their uptime depends on configuration and Azure availability."
            },
            {
                "id": "D",
                "text": "Compute clusters can only have 1 node",
                "is_correct": false,
                "explanation": "One of the key benefits of compute clusters is the ability to scale out to multiple nodes, not being limited to a single node."
            }
        ],
        "correct_answer": "B",
        "feedback": "Compute clusters in Azure ML are designed to support scalability by automatically adjusting the number of nodes based on current job requirements. This makes them ideal for running scalable training jobs efficiently."
    },
    {
        "question_id": "106",
        "type": "multiple_choice_single_answer",
        "question": "In an SKlearn LinearRegression model, what is the difference between 'predict' and 'score' methods?",
        "options": [
            {
                "id": "A",
                "text": ".predict only takes the X frame, whereas .score takes an X and Y frame",
                "is_correct": true,
                "explanation": ".predict() is used to generate predictions from the input features (X) only, while .score() requires both the features X and true labels Y to compute the R\u00b2 score."
            },
            {
                "id": "B",
                "text": ".predict takes an X and Y frame, and .score only takes the X frame",
                "is_correct": false,
                "explanation": ".predict() does not use the labels (Y); it only requires input features (X). This option reverses the correct usage."
            },
            {
                "id": "C",
                "text": ".predict predicts the accuracy of the validation set, where .score provides a model scoring metric (R2)",
                "is_correct": false,
                "explanation": ".predict() generates predicted values; it does not measure accuracy or R\u00b2. That is the job of .score()."
            },
            {
                "id": "D",
                "text": ".predict predicts the label whereas .score calculates the MSE",
                "is_correct": false,
                "explanation": ".score() for LinearRegression in scikit-learn returns the R\u00b2 score, not the mean squared error."
            }
        ],
        "answer": "A",
        "feedback": "The correct distinction is that `.predict(X)` is used only for generating predictions from input features, while `.score(X, y)` evaluates the model\u2019s performance using both inputs and ground truth labels by computing the R\u00b2 metric. Many candidates confuse `.score()` with metrics like MSE, but in LinearRegression it returns R\u00b2 by default."
    },
    {
        "question_id": "107",
        "type": "multiple_choice_single_answer",
        "question": "You are using Python SDK v2\n\nYou have a Data Asset called `pricehistory`\n\nYou load the data from the asset, modify it with Pandas, and then register the modified version using the MLClient instance:\n\n```python\nml_client.data.create_or_update(\n    Data(\n        name='pricehistory',\n        version='2',\n        path='./updated_data',\n        type=AssetTypes,URI_FOLDER\n    )\n)\n```\n\nWhat will be the result of running this code?",
        "options": [
            {
                "id": "A",
                "text": "The Data Asset 'pricehistory' is updated with the new contents, and version 2 is created",
                "is_correct": true,
                "explanation": "When a Data Asset with a new version is registered using `create_or_update`, Azure ML registers it as a new version under the same name."
            },
            {
                "id": "B",
                "text": "The original 'pricehistory' asset is overwritten with the new content, keeping the same version",
                "is_correct": false,
                "explanation": "Existing versions cannot be overwritten in SDK v2; a new version must be specified."
            },
            {
                "id": "C",
                "text": "An error will occur because the asset name 'pricehistory' already exists",
                "is_correct": false,
                "explanation": "The asset name can be reused as long as the version is unique. No error is raised."
            },
            {
                "id": "D",
                "text": "A new Data Asset is created with a different name since versioning is not supported in SDK v2",
                "is_correct": false,
                "explanation": "Versioning is supported in SDK v2 using the `version` field."
            }
        ],
        "answer": "A",
        "feedback": "In Azure Machine Learning SDK v2, you can manage Data Assets with explicit versioning. Using `ml_client.data.create_or_update()` with the same asset name but a new version creates a new version without affecting the previous ones. This supports reproducibility and traceability of data versions.\n\nReference: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-register-data-assets?view=azureml-api-2"
    },
    {
        "question_id": "108",
        "type": "multiple_choice_single_answer",
        "question": "Which type of deployment enables a user to submit multiple observations (X rows) and then receive multiple predictions (Y values)?",
        "options": [
            {
                "id": "A",
                "text": "Batch only",
                "is_correct": false,
                "explanation": "Batch endpoints support processing multiple inputs at once, but this capability is not exclusive to batch deployments."
            },
            {
                "id": "B",
                "text": "Real-time only",
                "is_correct": false,
                "explanation": "Real-time endpoints also support multiple inputs, especially when using MLflow or custom deployments, but this is not the only option."
            },
            {
                "id": "C",
                "text": "Neither",
                "is_correct": false,
                "explanation": "Both batch and real-time deployments can handle multiple rows of input; this option is incorrect."
            },
            {
                "id": "D",
                "text": "Real-time and Batch",
                "is_correct": true,
                "explanation": "Both real-time endpoints and batch endpoints in Azure Machine Learning can process multiple observations and return multiple outputs. Real-time endpoints are optimized for low-latency scoring of online requests (e.g., REST), while batch endpoints are used for asynchronous, high-volume scoring tasks across large datasets."
            }
        ],
        "answer": "D",
        "feedback": "In Azure ML, real-time endpoints can be configured to handle multiple records in a single request, particularly if the deployed model supports batch-style inputs (like arrays or DataFrames). Batch endpoints, on the other hand, are explicitly designed for large-scale inference over datasets stored in blob storage or folders. Both methods support input/output of multiple rows, depending on model and implementation. For more: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where"
    },
    {
        "question_id": "109",
        "type": "multiple_choice_single_answer",
        "question": "What is the purpose of a differential privacy solution?",
        "options": [
            {
                "id": "A",
                "text": "Protect individual data by adding non-statistical noise",
                "is_correct": false,
                "explanation": "This is incorrect. Differential privacy works by adding carefully calibrated statistical noise to data."
            },
            {
                "id": "B",
                "text": "Protect individual data by adding data access controls",
                "is_correct": false,
                "explanation": "Access controls restrict who can view data, but this is not the core mechanism of differential privacy."
            },
            {
                "id": "C",
                "text": "Protect individual data by adding statistical noise",
                "is_correct": true,
                "explanation": "Correct. Differential privacy protects individual data by injecting controlled, statistical noise into the dataset or the query results to prevent the identification of individuals while preserving overall patterns."
            },
            {
                "id": "D",
                "text": "Protect individual data by adding privacy access regulations",
                "is_correct": false,
                "explanation": "This option is unrelated to how differential privacy works. Regulations are external legal frameworks, not algorithmic techniques."
            }
        ],
        "answer": "C",
        "feedback": "Differential privacy is a mathematical framework for quantifying and limiting the privacy risks associated with the release of statistical data. It works by adding statistical noise\u2014usually Laplacian or Gaussian\u2014to query results, so that the presence or absence of any single individual in the data has a minimal impact on the output. This approach is essential for scenarios like federated learning or public data release in Azure's Responsible AI ecosystem. More info: https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai"
    },
    {
        "question_id": "110",
        "type": "multiple_choice_single_answer",
        "question": "You are working locally and need to connect to an Azure Machine Learning workspace using the Azure ML Python SDK v2\n\nWhat is required to establish the connection using DefaultAzureCredential?",
        "options": [
            {
                "id": "A",
                "text": "Create a config.json file with subscription, resource group, and workspace name, and load it using MLClient.",
                "is_correct": true,
                "explanation": "The SDK v2 uses `MLClient` from `azure.ai.ml` to connect to a workspace, typically by loading details from a config.json file in combination with `DefaultAzureCredential` for authentication."
            },
            {
                "id": "B",
                "text": "Use the Workspace.from_config() method to initialize the workspace from the config file.",
                "is_correct": false,
                "explanation": "This method is specific to SDK v1 and not available in SDK v2. SDK v2 requires MLClient and Azure Identity credentials."
            },
            {
                "id": "C",
                "text": "Authenticate using interactive login before calling MLWorkspaceClient.connect().",
                "is_correct": false,
                "explanation": "There is no `MLWorkspaceClient.connect()` method in SDK v2. The correct class is `MLClient` and it uses `DefaultAzureCredential` or similar credential objects."
            },
            {
                "id": "D",
                "text": "Use a managed identity assigned to your compute instance to connect without configuration.",
                "is_correct": false,
                "explanation": "While managed identity is supported in cloud contexts, a local connection still typically requires a `config.json` file unless the full context is injected via environment variables or interactive login."
            }
        ],
        "feedback": "To connect to a workspace using Azure ML SDK v2 locally, you need to create a `config.json` file with workspace metadata (subscription ID, resource group, and workspace name). Then, use the `MLClient` class from `azure.ai.ml` along with `DefaultAzureCredential` from `azure.identity`. Example:\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\ncred = DefaultAzureCredential()\nml_client = MLClient.from_config(credential=cred)\n```\nMore info: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment?tabs=python"
    },
    {
        "question_id": "111",
        "type": "multiple_choice_single_answer",
        "question": "What service is not included in, or deployed with, Azure Machine Learning?",
        "options": [
            {
                "id": "A",
                "text": "Storage Accounts",
                "is_correct": false,
                "explanation": "Storage Accounts are essential for Azure ML to store data, models, logs, and intermediate outputs."
            },
            {
                "id": "B",
                "text": "Key Vault",
                "is_correct": false,
                "explanation": "Azure ML uses Key Vault to securely store secrets such as connection strings and credentials."
            },
            {
                "id": "C",
                "text": "App Service",
                "is_correct": true,
                "explanation": "Azure App Service is not part of Azure Machine Learning. It's designed for hosting web apps, not for managing ML workflows or models."
            },
            {
                "id": "D",
                "text": "Container Registry",
                "is_correct": false,
                "explanation": "Azure Container Registry is used by Azure ML to store Docker images for environments and deployments."
            }
        ],
        "feedback": "Azure Machine Learning integrates and automatically provisions several Azure resources such as Storage Accounts, Azure Key Vault, and Container Registry. However, Azure App Service is not part of the Azure ML ecosystem and is not used in ML workflows or deployments. App Service is intended for web app hosting, not for managing ML assets or compute."
    },
    {
        "question_id": "112",
        "type": "multiple_choice_single_answer",
        "question": "What Compute would be best suited for training a Computer Vision model?",
        "options": [
            {
                "id": "A",
                "text": "Compute instance with high RAM, CPU, 2 cores",
                "is_correct": false,
                "explanation": "This option provides sufficient memory but lacks the GPU needed for efficient image processing tasks in computer vision."
            },
            {
                "id": "B",
                "text": "Compute cluster with CPU, medium RAM, and 16 nodes",
                "is_correct": true,
                "explanation": "Although it allows for parallel processing, using only CPUs makes this setup suboptimal for training deep learning-based computer vision models."
            },
            {
                "id": "C",
                "text": "Compute cluster with GPU, medium RAM, and 16 nodes",
                "is_correct": false,
                "explanation": "Despite being the most appropriate option for computer vision tasks due to GPU availability, it was marked incorrect, likely due to a mistake in the answer key."
            },
            {
                "id": "D",
                "text": "Compute instance with low RAM, CPU, 2 cores",
                "is_correct": false,
                "explanation": "This answer is technically incorrect for training computer vision models, which benefit from GPUs. The answer key appears to be flawed."
            }
        ],
        "feedback": "Computer vision models, especially deep learning ones like CNNs, require significant computational resources. A compute cluster with GPUs is the most appropriate choice for such tasks, as GPUs handle matrix operations and backpropagation much faster than CPUs. The selected correct answer is likely an error in the exam system and should be carefully re-evaluated when studying."
    },
    {
        "question_id": "113",
        "type": "multiple_choice_single_answer",
        "question": "What is the difference between data assets and data stores?",
        "options": [
            {
                "id": "A",
                "text": "Data stores are abstracted locations for data, whereas data assets are data processing power that is used to model",
                "is_correct": false,
                "explanation": "This is incorrect. Data assets represent the actual data, not compute resources."
            },
            {
                "id": "B",
                "text": "Data stores are abstracted locations for data, whereas data assets are abstracted tables / files of data",
                "is_correct": true,
                "explanation": "Correct. A data store refers to the underlying storage system (like Azure Blob Storage or ADLS), while a data asset in Azure ML represents a specific dataset or file within that store, versioned and used for training or inference."
            },
            {
                "id": "C",
                "text": "Data stores are tools used to consume data from the public (using Azure Data Store), whereas data assets are abstracted tables / files of data",
                "is_correct": false,
                "explanation": "Data stores are not limited to public data and are more general in function than stated here."
            },
            {
                "id": "D",
                "text": "Data stores used to consume data from the public (using Azure Data Store), whereas data assets are data processing power that is used to model",
                "is_correct": false,
                "explanation": "Incorrect on both accounts. Data assets are not compute resources, and data stores are not limited to public data."
            }
        ],
        "feedback": "In Azure Machine Learning, data stores are connection configurations to storage locations (e.g., Azure Blob, ADLS). Data assets are references to datasets or files stored in those data stores, and they are versioned entities used in training and pipeline steps. Knowing the distinction is essential for data management and reproducibility in ML workflows."
    },
    {
        "question_id": "114",
        "type": "multiple_choice_single_answer",
        "question": "What language does Azure Data Explorer use?",
        "options": [
            {
                "id": "A",
                "text": "Python (Python SDK Library)",
                "is_correct": false,
                "explanation": "Python is a client language supported via SDKs, but it is not the query language used by Azure Data Explorer."
            },
            {
                "id": "B",
                "text": "Query Language",
                "is_correct": false,
                "explanation": "This is too generic to be correct. Azure Data Explorer uses a specific query language."
            },
            {
                "id": "C",
                "text": "Kusto Query Language",
                "is_correct": true,
                "explanation": "Correct. Azure Data Explorer uses Kusto Query Language (KQL) for querying large volumes of structured, semi-structured, and unstructured data efficiently."
            },
            {
                "id": "D",
                "text": "Structured Query Language",
                "is_correct": false,
                "explanation": "SQL is not the native language used by Azure Data Explorer. KQL is specifically designed for that platform."
            }
        ],
        "feedback": "Azure Data Explorer uses Kusto Query Language (KQL), a read-only request language used to process and analyze large volumes of data. KQL is optimized for speed and scale in telemetry and log analytics scenarios and is distinct from SQL or other traditional query languages."
    },
    {
        "question_id": "115",
        "type": "multiple_choice_single_answer",
        "question": "You have created a real-time inference model and deployed it to an endpoint\n\nYou want to test it using the REST API\n\nWhich of the following is true?",
        "options": [
            {
                "id": "A",
                "text": "You can pass multiple records at a time and the data does not need to be in JSON format",
                "is_correct": false,
                "explanation": "Incorrect. Data must be passed in JSON format for REST API inference calls."
            },
            {
                "id": "B",
                "text": "You can only pass one record at a time and the data needs to be in JSON format",
                "is_correct": false,
                "explanation": "This is incorrect. Real-time endpoints can accept multiple records if formatted correctly in a JSON array."
            },
            {
                "id": "C",
                "text": "You can pass multiple records at a time and the data needs to be in JSON format",
                "is_correct": true,
                "explanation": "Correct. Azure ML real-time endpoints accept JSON-formatted input, and can handle multiple records in a single request using an array format."
            },
            {
                "id": "D",
                "text": "You can only pass one record at a time and the data does not need to be in JSON format",
                "is_correct": false,
                "explanation": "Incorrect. JSON format is required for REST API requests to Azure ML real-time endpoints."
            }
        ],
        "feedback": "When sending data to a deployed Azure Machine Learning real-time endpoint using the REST API, the data must be in JSON format. The endpoint supports batching of requests, meaning that multiple records can be submitted at once as a JSON array. This improves performance and reduces latency for batch inference operations."
    },
    {
        "question_id": "118",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a data pipeline in Azure Machine Learning Studio Designer, and you want to take the Data Asset and remove any outliers for a particular column\n\nWhat is the best component to do that?",
        "options": [
            {
                "id": "A",
                "text": "Normalize Values",
                "is_correct": false,
                "explanation": "Normalize Values scales the data to a specific range but does not remove or adjust outliers explicitly."
            },
            {
                "id": "B",
                "text": "Clean Missing Data",
                "is_correct": false,
                "explanation": "Clean Missing Data is used for handling nulls or missing entries but is not designed for removing outliers."
            },
            {
                "id": "C",
                "text": "Clip Values",
                "is_correct": true,
                "explanation": "Clip Values is the correct choice for outlier handling. It limits the data to a specified range, effectively removing or adjusting outliers."
            },
            {
                "id": "D",
                "text": "Normalize Data",
                "is_correct": false,
                "explanation": "Normalize Data helps with standardizing data distribution but does not address outliers directly."
            }
        ],
        "feedback": "The 'Clip Values' component in Azure ML Studio Designer is specifically designed to handle outliers by setting upper and lower bounds for a column. This is ideal when you want to remove or adjust extreme values that can skew model performance. Other options like normalization or cleaning missing data do not address outliers directly."
    },
    {
        "question_id": "117",
        "type": "drag_and_drop_ordering",
        "question": "You are planning to host practical training to acquaint staff with Docker for Windows\n\nStaff devices must support the installation of Docker\n\nWhich of the following are requirements for this installation? Arrange the correct requirements in order\n\n",
        "options": [
            {
                "id": "1",
                "text": "2 GB of system RAM"
            },
            {
                "id": "2",
                "text": "4 GB of system RAM"
            },
            {
                "id": "3",
                "text": "BIOS-enabled virtualization"
            },
            {
                "id": "4",
                "text": "Microsoft Hardware-Assisted Virtualization Detection Tool"
            },
            {
                "id": "5",
                "text": "Windows 10 64-bit"
            },
            {
                "id": "6",
                "text": "Windows 10 32-bit"
            }
        ],
        "correct_order": [
            "2",
            "3",
            "5"
        ],
        "feedback": "(Order does not matter). To install Docker Desktop on Windows, the system must meet the following requirements: at least 4 GB of RAM (not 2 GB), virtualization enabled in BIOS, and Windows 10 64-bit. Options like Windows 10 32-bit or diagnostic tools like the Virtualization Detection Tool are not direct installation requirements."
    },
    {
        "question_id": "116",
        "type": "drag_and_drop_ordering",
        "question": "You have been tasked with moving data into Azure Blob Storage for the purpose of supporting Azure Machine Learning\n\nWhich of the following can be used to complete your task? Arrange the options that are valid in the correct order\n\n",
        "options": [
            {
                "id": "1",
                "text": "AzCopy"
            },
            {
                "id": "2",
                "text": "Bulk Copy Program (BCP)"
            },
            {
                "id": "3",
                "text": "SSIS"
            },
            {
                "id": "4",
                "text": "Bulk Insert SQL Query"
            },
            {
                "id": "5",
                "text": "Azure Storage Explorer"
            }
        ],
        "correct_order": [
            "1",
            "3",
            "5"
        ],
        "feedback": "To move data into Azure Blob Storage, AzCopy is a command-line tool designed specifically for transferring blobs. SSIS is an ETL tool that supports blob targets, and Azure Storage Explorer provides a GUI to interact with blob containers. BCP and Bulk Insert SQL Query are intended for database-level operations and are not suitable for direct blob storage interaction."
    },
    {
        "question_id": "dp100_001",
        "type": "multiple_choice_single_answer",
        "question": "You need to monitor your training jobs in Azure Machine Learning\n\nYou configure 'Diagnostic settings' and select the category 'AmlRunStatusChangedEvent'\n\nDoes this solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": "This approach enables job status logs to be sent to Log Analytics or other monitoring tools, allowing comprehensive monitoring."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": "Incorrect, because configuring Diagnostic settings with AmlRunStatusChangedEvent is the correct method."
            }
        ],
        "feedback": "The 'AmlRunStatusChangedEvent' enables tracking job execution status via Azure Monitor or Log Analytics.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_002",
        "type": "multiple_choice_single_answer",
        "question": "You want to log metrics, parameters, and tags from an MLflow run\n\nWhich MLflow function retrieves this information?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.get_run(run_id)",
                "is_correct": true,
                "explanation": "mlflow.get_run retrieves a Run object that contains metrics, parameters, and tags associated with the run."
            },
            {
                "id": "B",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "log_param logs a single parameter but does not retrieve run metadata."
            },
            {
                "id": "C",
                "text": "mlflow.autolog()",
                "is_correct": false,
                "explanation": "autolog enables automatic logging but does not retrieve existing data."
            },
            {
                "id": "D",
                "text": "mlflow.pyfunc.save_model()",
                "is_correct": false,
                "explanation": "This function saves a model but does not handle metric or parameter logging."
            }
        ],
        "feedback": "mlflow.get_run is used to access metadata including metrics, params, and tags from a run.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_003",
        "type": "multiple_choice_single_answer",
        "question": "You need to log data from your job runs using MLflow\n\nYou use the `Run.log()`\n\nDoes this solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Run.log() is part of the Azure ML SDK, not MLflow. It does not meet the requirement when MLflow is specified."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Correct, MLflow has its own API for logging such as mlflow.log_metric, mlflow.log_param, etc."
            }
        ],
        "feedback": "When using MLflow, logging should be done with MLflow API functions like mlflow.log_param, not AzureML SDK's Run.log().",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_004",
        "type": "multiple_choice_single_answer",
        "question": "You need to create a compute instance that supports ML pipeline training in Azure ML Designer v2\n\nYou create an Azure Databricks environment\n\nDoes this solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": "Azure Databricks cannot be used directly as a compute target in Designer v2."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "Correct. Designer v2 only supports Azure ML Compute as a compute target for pipeline training."
            }
        ],
        "feedback": "Azure ML Designer v2 only works with AML Compute targets like clusters or instances, not Databricks or HDInsight.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_005",
        "type": "multiple_choice_single_answer",
        "question": "You plan to read Parquet-format data from a folder that is not registered as a dataset using Azure ML SDK v2\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a v2 job with Dataset.File.from_files()",
                "is_correct": false,
                "explanation": "Dataset.File.from_files() is from SDK v1 and not compatible with SDK v2 workflows."
            },
            {
                "id": "B",
                "text": "Create a v2 job with AssetTypes.URI_FOLDER and ml_client.jobs.create_or_update()",
                "is_correct": true,
                "explanation": "Correct. URI_FOLDER is the correct type for referencing a folder of data in SDK v2."
            },
            {
                "id": "C",
                "text": "Create a v2 job with AssetTypes.URI_FILE and ml_client.jobs.create_or_update()",
                "is_correct": false,
                "explanation": "URI_FILE is for individual files, not folders. Parquet data often consists of multiple files."
            }
        ],
        "feedback": "Use AssetTypes.URI_FOLDER to work with a directory of Parquet files in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_006",
        "type": "multiple_choice_single_answer",
        "question": "You want to deploy a machine learning model for scalable real-time inference\n\nYou need to ensure low latency and autoscaling capabilities\n\nWhich deployment target should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Instance (ACI)",
                "is_correct": false,
                "explanation": "ACI is lightweight and suitable for testing, but it doesn't offer autoscaling for production workloads."
            },
            {
                "id": "B",
                "text": "Azure Kubernetes Service (AKS)",
                "is_correct": true,
                "explanation": "AKS supports autoscaling and is recommended for real-time inference at scale."
            },
            {
                "id": "C",
                "text": "Azure ML Compute Cluster",
                "is_correct": false,
                "explanation": "Compute clusters are mainly used for training, not for inference endpoints."
            },
            {
                "id": "D",
                "text": "Azure Batch",
                "is_correct": false,
                "explanation": "Azure Batch is better suited for batch inference and parallel processing."
            }
        ],
        "feedback": "AKS is the best choice for deploying scalable, real-time inference endpoints in Azure ML.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_007",
        "type": "multiple_choice_single_answer",
        "question": "You are configuring a pipeline in Azure ML SDK v2\n\nYou want to use a component that loads data from a registered MLTable asset\n\nWhat input type should you define?",
        "options": [
            {
                "id": "A",
                "text": "Input(type='uri_file')",
                "is_correct": false,
                "explanation": "uri_file is for single files, not MLTable datasets."
            },
            {
                "id": "B",
                "text": "Input(type='mltable')",
                "is_correct": true,
                "explanation": "This is the correct type to use for MLTable assets in Azure ML SDK v2."
            },
            {
                "id": "C",
                "text": "Input(type='string')",
                "is_correct": false,
                "explanation": "String inputs are for parameters, not datasets."
            },
            {
                "id": "D",
                "text": "Input(type='uri_folder')",
                "is_correct": false,
                "explanation": "While uri_folder works with folders, it's not used for registered MLTable assets."
            }
        ],
        "feedback": "Use `Input(type='mltable')` when passing a registered MLTable asset into a pipeline component.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_008",
        "type": "multiple_choice_single_answer",
        "question": "You are using MLflow with Azure Machine Learning SDK v2\n\nYou want to automatically capture parameters, metrics, and models during training\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "This function logs a single parameter, but it doesn't enable automatic logging."
            },
            {
                "id": "B",
                "text": "mlflow.autolog()",
                "is_correct": true,
                "explanation": "mlflow.autolog() enables automatic logging of metrics, parameters, and models during training."
            },
            {
                "id": "C",
                "text": "mlflow.start_run()",
                "is_correct": false,
                "explanation": "This starts a run context but does not perform any logging by itself."
            },
            {
                "id": "D",
                "text": "mlflow.register_model()",
                "is_correct": false,
                "explanation": "This function is used to register models, not for logging during training."
            }
        ],
        "feedback": "Use mlflow.autolog() to simplify tracking during model training in Azure ML with SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_009",
        "type": "multiple_choice_single_answer",
        "question": "You are configuring a job with Azure ML CLI v2 and want to pass a folder of data located in a default datastore\n\nWhich input type should you use?",
        "options": [
            {
                "id": "A",
                "text": "uri_file",
                "is_correct": false,
                "explanation": "uri_file is intended for single file inputs, not folders."
            },
            {
                "id": "B",
                "text": "mltable",
                "is_correct": false,
                "explanation": "mltable refers to a structured tabular format, not arbitrary folders."
            },
            {
                "id": "C",
                "text": "uri_folder",
                "is_correct": true,
                "explanation": "uri_folder is the correct type for referencing a folder in a datastore or blob."
            },
            {
                "id": "D",
                "text": "string",
                "is_correct": false,
                "explanation": "string is used for scalar inputs like hyperparameters, not for data assets."
            }
        ],
        "feedback": "Use uri_folder when you want to pass a directory as input to a job in Azure ML SDK v2 or CLI v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_010",
        "type": "multiple_choice_single_answer",
        "question": "You have a component in a pipeline that outputs a folder of images\n\nYou want to log this output using MLflow in Azure ML SDK v2\n\nWhich function should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_image()",
                "is_correct": false,
                "explanation": "This logs a single image, not an entire folder of images."
            },
            {
                "id": "B",
                "text": "mlflow.log_artifact(path)",
                "is_correct": true,
                "explanation": "mlflow.log_artifact logs a local file or directory as an artifact to the current run."
            },
            {
                "id": "C",
                "text": "mlflow.set_tag()",
                "is_correct": false,
                "explanation": "mlflow.set_tag adds metadata but does not log files or directories."
            },
            {
                "id": "D",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "This function logs a single scalar parameter, not files or folders."
            }
        ],
        "feedback": "To log an entire folder (e.g., image outputs), use mlflow.log_artifact with a directory path.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_011",
        "type": "multiple_choice_single_answer",
        "question": "You want to schedule a model retraining job weekly using Azure Machine Learning\n\nWhich tool should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Data Factory",
                "is_correct": true,
                "explanation": "Azure Data Factory can trigger Azure ML pipelines on a schedule or based on events like new data."
            },
            {
                "id": "B",
                "text": "Azure DevOps",
                "is_correct": false,
                "explanation": "Azure DevOps can schedule pipelines but is not the standard approach for ML retraining jobs."
            },
            {
                "id": "C",
                "text": "Azure Blob Storage",
                "is_correct": false,
                "explanation": "Blob Storage is a data source, not a tool for scheduling or orchestrating jobs."
            },
            {
                "id": "D",
                "text": "Azure Batch",
                "is_correct": false,
                "explanation": "Azure Batch is for parallel processing jobs, not scheduling ML pipelines."
            }
        ],
        "feedback": "Azure Data Factory is a recommended service for scheduling and orchestrating ML workflows.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_012",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure ML SDK v2 and want to deploy a model to a local endpoint for testing\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.online_endpoints.create_or_update()",
                "is_correct": false,
                "explanation": "This method is used for cloud-based endpoints, not local testing."
            },
            {
                "id": "B",
                "text": "ml_client.batch_endpoints.create_or_update()",
                "is_correct": false,
                "explanation": "This is used for batch inference, not local or real-time testing."
            },
            {
                "id": "C",
                "text": "ml_client.online_endpoints.invoke_local()",
                "is_correct": true,
                "explanation": "This method allows you to test locally without deploying to the cloud."
            },
            {
                "id": "D",
                "text": "ml_client.models.download()",
                "is_correct": false,
                "explanation": "This is used to download a model, not for deploying or testing locally."
            }
        ],
        "feedback": "To test a model locally, use `invoke_local()` on a local endpoint in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_013",
        "type": "multiple_choice_single_answer",
        "question": "You want to track the RGB values of an image transformation during an experiment using MLflow\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_dict()",
                "is_correct": true,
                "explanation": "mlflow.log_dict allows logging a dictionary, such as RGB values, as a JSON artifact."
            },
            {
                "id": "B",
                "text": "mlflow.log_image()",
                "is_correct": false,
                "explanation": "mlflow.log_image is for visual image files, not structured dictionaries."
            },
            {
                "id": "C",
                "text": "mlflow.log_metric()",
                "is_correct": false,
                "explanation": "mlflow.log_metric is used for scalar values like accuracy, not RGB dictionaries."
            },
            {
                "id": "D",
                "text": "mlflow.register_model()",
                "is_correct": false,
                "explanation": "mlflow.register_model is used for registering models, not logging metadata."
            }
        ],
        "feedback": "Use mlflow.log_dict to capture structured data like dictionaries during experiment runs.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_014",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a model for testing in Azure ML and want a low-cost option that initializes quickly and does not require managing infrastructure\n\nWhich compute target should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Kubernetes Service (AKS)",
                "is_correct": false,
                "explanation": "AKS is more suitable for production deployments with autoscaling, but it's more complex and costly to manage."
            },
            {
                "id": "B",
                "text": "Azure Container Instances (ACI)",
                "is_correct": true,
                "explanation": "ACI is designed for quick, cost-effective deployment of models without infrastructure management."
            },
            {
                "id": "C",
                "text": "Azure Batch",
                "is_correct": false,
                "explanation": "Azure Batch is ideal for batch scoring and not suited for real-time, lightweight inference."
            },
            {
                "id": "D",
                "text": "Compute Cluster",
                "is_correct": false,
                "explanation": "Compute clusters are best for training jobs, not real-time model deployment."
            }
        ],
        "feedback": "ACI is the optimal choice for test deployments with low overhead in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_015",
        "type": "multiple_choice_single_answer",
        "question": "You are registering a model with MLflow after training\n\nWhich function should you use to create a named, versioned model in the registry?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.register_model()",
                "is_correct": true,
                "explanation": "This function registers a model with a specified name and creates a version in the model registry."
            },
            {
                "id": "B",
                "text": "mlflow.log_model()",
                "is_correct": false,
                "explanation": "This logs the model to the current run but does not add it to the registry."
            },
            {
                "id": "C",
                "text": "mlflow.save_model()",
                "is_correct": false,
                "explanation": "This saves the model locally but does not track it in the model registry."
            },
            {
                "id": "D",
                "text": "mlflow.get_model_version()",
                "is_correct": false,
                "explanation": "This retrieves model version info but does not register a model."
            }
        ],
        "feedback": "To make a model discoverable and versioned in Azure ML's registry, use mlflow.register_model().",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_016",
        "type": "multiple_choice_single_answer",
        "question": "You are monitoring data drift in Azure Machine Learning using the SDK v2\n\nWhich component should you configure to define the logic for drift detection?",
        "options": [
            {
                "id": "A",
                "text": "DataDriftSchedule",
                "is_correct": false,
                "explanation": "There is no component named DataDriftSchedule in SDK v2. The correct object is MonitorSchedule."
            },
            {
                "id": "B",
                "text": "MonitorSchedule",
                "is_correct": true,
                "explanation": "MonitorSchedule defines when and how monitoring jobs like data drift detection should run."
            },
            {
                "id": "C",
                "text": "DriftMetricsLogger",
                "is_correct": false,
                "explanation": "DriftMetricsLogger does not exist in Azure ML SDK v2."
            },
            {
                "id": "D",
                "text": "RunConfig",
                "is_correct": false,
                "explanation": "RunConfig is used in SDK v1 for configuring jobs, not for monitoring in SDK v2."
            }
        ],
        "feedback": "Use MonitorSchedule to set up and automate drift detection jobs in Azure Machine Learning SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_017",
        "type": "multiple_choice_single_answer",
        "question": "You want to restrict training in Azure ML to only a specific algorithm, such as 'XGBoostClassifier'\n\nWhich property should you configure in the AutoML job?",
        "options": [
            {
                "id": "A",
                "text": "blocked_training_algorithms",
                "is_correct": false,
                "explanation": "This property excludes algorithms but does not explicitly restrict to a single one."
            },
            {
                "id": "B",
                "text": "allowed_training_algorithms",
                "is_correct": true,
                "explanation": "allowed_training_algorithms specifies exactly which algorithms can be used during training."
            },
            {
                "id": "C",
                "text": "primary_metric",
                "is_correct": false,
                "explanation": "This defines how models are evaluated but not which algorithms are used."
            },
            {
                "id": "D",
                "text": "enable_early_termination",
                "is_correct": false,
                "explanation": "This is used to stop underperforming trials early, not to limit algorithm selection."
            }
        ],
        "feedback": "To restrict training to a specific algorithm in AutoML, use the allowed_training_algorithms setting.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_018",
        "type": "multiple_choice_single_answer",
        "question": "You want to save a trained scikit-learn model using MLflow so it can be registered and deployed in Azure ML\n\nWhich function should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.sklearn.log_model()",
                "is_correct": true,
                "explanation": "This function logs a scikit-learn model to the current MLflow run so it can be registered and deployed."
            },
            {
                "id": "B",
                "text": "mlflow.register_model()",
                "is_correct": false,
                "explanation": "This function registers a model already logged to MLflow; it does not save the model initially."
            },
            {
                "id": "C",
                "text": "mlflow.pyfunc.log_model()",
                "is_correct": false,
                "explanation": "pyfunc is for generic Python models; for scikit-learn, use the dedicated mlflow.sklearn module."
            },
            {
                "id": "D",
                "text": "mlflow.set_model()",
                "is_correct": false,
                "explanation": "There is no function called set_model in MLflow."
            }
        ],
        "feedback": "Use mlflow.sklearn.log_model() to save and track a scikit-learn model in MLflow for deployment in Azure ML.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_019",
        "type": "multiple_choice_single_answer",
        "question": "You are deploying a model to a managed online endpoint in Azure ML using SDK v2\n\nWhat type of YAML configuration is required?",
        "options": [
            {
                "id": "A",
                "text": "endpoint.yml and deployment.yml",
                "is_correct": true,
                "explanation": "Deploying to a managed online endpoint requires two YAML files: one for the endpoint and one for the deployment configuration."
            },
            {
                "id": "B",
                "text": "model.yml and job.yml",
                "is_correct": false,
                "explanation": "These files are used for model registration and running jobs, not endpoint deployment."
            },
            {
                "id": "C",
                "text": "compute.yml and pipeline.yml",
                "is_correct": false,
                "explanation": "These relate to training environments and pipelines, not model deployment endpoints."
            },
            {
                "id": "D",
                "text": "component.yml only",
                "is_correct": false,
                "explanation": "component.yml defines pipeline steps or components, not endpoints."
            }
        ],
        "feedback": "Use `endpoint.yml` to define the endpoint and `deployment.yml` to define how the model is deployed to it.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_020",
        "type": "multiple_choice_single_answer",
        "question": "You want to use MLflow in Azure Machine Learning SDK v2 to log a confusion matrix as an image after model evaluation\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_dict()",
                "is_correct": false,
                "explanation": "This logs structured data like dictionaries, not image files."
            },
            {
                "id": "B",
                "text": "mlflow.log_image()",
                "is_correct": true,
                "explanation": "mlflow.log_image() is used to log visual artifacts such as confusion matrices as images."
            },
            {
                "id": "C",
                "text": "mlflow.log_metric()",
                "is_correct": false,
                "explanation": "This logs scalar values like accuracy or precision, not images."
            },
            {
                "id": "D",
                "text": "mlflow.set_tag()",
                "is_correct": false,
                "explanation": "This is used for metadata tagging, not for storing evaluation visualizations."
            }
        ],
        "feedback": "To save visual outputs like confusion matrices, use mlflow.log_image() with a generated image file.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_021",
        "type": "multiple_choice_single_answer",
        "question": "You want to enable early termination in an AutoML classification job to avoid wasting resources on poorly performing trials\n\nWhich setting should you configure?",
        "options": [
            {
                "id": "A",
                "text": "enable_onnx_compatible_models",
                "is_correct": false,
                "explanation": "This controls model compatibility with ONNX but not early stopping."
            },
            {
                "id": "B",
                "text": "enable_early_termination",
                "is_correct": true,
                "explanation": "This setting ensures underperforming trials are stopped early to optimize resource use."
            },
            {
                "id": "C",
                "text": "trial_timeout_minutes",
                "is_correct": false,
                "explanation": "This limits how long each trial can run, but doesn't stop underperformers automatically."
            },
            {
                "id": "D",
                "text": "max_trials",
                "is_correct": false,
                "explanation": "This sets the total number of trials, not whether they should stop early."
            }
        ],
        "feedback": "To save costs and training time in AutoML jobs, enable the 'enable_early_termination' setting.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_022",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure ML SDK v2 and need to create a workspace programmatically\n\nWhich class should you use?",
        "options": [
            {
                "id": "A",
                "text": "MLClient.create_workspace()",
                "is_correct": false,
                "explanation": "There is no such method in MLClient. Workspaces are created using a Workspace object and `begin_create()`."
            },
            {
                "id": "B",
                "text": "Workspace()",
                "is_correct": true,
                "explanation": "The Workspace class is used to define the configuration of a workspace before creating it with MLClient."
            },
            {
                "id": "C",
                "text": "Experiment.create()",
                "is_correct": false,
                "explanation": "This is used to create experiments, not workspaces."
            },
            {
                "id": "D",
                "text": "Run.get_context()",
                "is_correct": false,
                "explanation": "This is used for retrieving the context of a run, not for workspace creation."
            }
        ],
        "feedback": "Use the Workspace class and `ml_client.workspaces.begin_create()` to programmatically create a workspace.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_023",
        "type": "multiple_choice_single_answer",
        "question": "You want to deploy a PyTorch model to an online endpoint in Azure ML SDK v2\n\nWhich environment image should you use?",
        "options": [
            {
                "id": "A",
                "text": "mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04",
                "is_correct": false,
                "explanation": "This image is commonly used for training, not optimized for serving PyTorch models."
            },
            {
                "id": "B",
                "text": "azureml:PyTorch-1.13-ubuntu20.04-py38-cuda11.7:1",
                "is_correct": true,
                "explanation": "This environment is designed specifically for PyTorch model training and inference, with GPU support."
            },
            {
                "id": "C",
                "text": "azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1",
                "is_correct": false,
                "explanation": "This image is for sklearn models and does not include PyTorch."
            },
            {
                "id": "D",
                "text": "azureml:TensorFlow-2.9-ubuntu20.04-py38:1",
                "is_correct": false,
                "explanation": "This is for TensorFlow models, not PyTorch."
            }
        ],
        "feedback": "Use a PyTorch-specific Azure ML environment for deploying models built with the PyTorch framework.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "q_008",
        "type": "multiple_choice_single_answer",
        "question": "You need to log data from your job runs using MLflow\n\n. You are interested in logging metrics.\n\n You use `mlflow.log_param()`\n\n Does the solution meet the requirements?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": "`mlflow.log_param()` is used to log parameters (hyperparameters), not metrics. Since the question requires logging metrics, this option does not meet the goal."
            }
        ],
        "feedback": "`mlflow.log_metric()` should be used instead.",
        "include_in_bank": true
    },
    {
        "question_id": "q_013",
        "type": "multiple_choice_single_answer",
        "question": "What should you use to register the model?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.pyfunc",
                "is_correct": true,
                "explanation": "The `mlflow.pyfunc` module allows defining and registering custom models that combine different frameworks. It is not recommended to use `mlflow.autolog()` or `mlflow.log_params()` as they do not handle custom models properly, and `load_model()` is only used to load a model, not to register it."
            },
            {
                "id": "B",
                "text": "mlflow.autolog()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "mlflow.log_params()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "mlflow.<flavor>.load_model()",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "The `mlflow.pyfunc` module allows defining and registering custom models that combine different frameworks. It is not recommended to use `mlflow.autolog()` or `mlflow.log_params()` as they do not handle custom models properly, and `load_model()` is only used to load a model, not to register it.",
        "include_in_bank": true
    },
    {
        "question_id": "q_014",
        "type": "multiple_choice_single_answer",
        "question": "What should you do to read/write this folder directly from an ML script?",
        "options": [
            {
                "id": "A",
                "text": "Create a v2 job with `Dataset.File.from_files()` and `ml_client.jobs.create_or_update()`",
                "is_correct": false,
                "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
            },
            {
                "id": "B",
                "text": "Create a v2 job with `AssetTypes.URI_FOLDER` and `ml_client.jobs.create_or_update()`",
                "is_correct": true,
                "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
            },
            {
                "id": "C",
                "text": "Create a v2 job with `AssetTypes.URI_FILE` and `ml_client.jobs.create_or_update()`",
                "is_correct": false,
                "explanation": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment."
            }
        ],
        "feedback": "When the Parquet file is in a folder, you must use `AssetTypes.URI_FOLDER` to reference the entire folder as an input resource and then create the job with `ml_client.jobs.create_or_update()`, which sends the folder's contents to the execution environment.",
        "include_in_bank": true
    },
    {
        "question_id": "q_016",
        "type": "multiple_choice_single_answer",
        "question": "You have built a custom model that uses multiple elements from different frameworks\n\nYou need to log the model\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.pyfunc",
                "is_correct": true,
                "explanation": "You should use the `mlflow.pyfunc` module to log custom models that combine multiple frameworks or inference logic not natively supported by MLflow. This module defines a generic file-system format for Python models and provides utilities to save and load them in that format."
            },
            {
                "id": "B",
                "text": "mlflow.autolog()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "mlflow.log_params()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "mlflow.<flavor>.load_model()",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "You should use the `mlflow.pyfunc` module to log custom models that combine multiple frameworks or inference logic not natively supported by MLflow. This module defines a generic file-system format for Python models and provides utilities to save and load them in that format.",
        "include_in_bank": true
    },
    {
        "question_id": "q_017",
        "type": "multiple_choice_multiple_answer",
        "question": "You have access to column\u2011oriented data in Parquet format within a folder\n\nThe data is not registered as a dataset\n\nYou want to read/write the folder in a job using the Azure ML SDK v2\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a v2 job using `Dataset.File.from_files()` and submit with `ml_client.jobs.create_or_update()`",
                "is_correct": true,
                "explanation": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it."
            },
            {
                "id": "B",
                "text": "Create a v2 job using `AssetTypes.URI_FOLDER` and submit with `ml_client.jobs.from_config()`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create a v2 job using `AssetTypes.URI_FILE` and submit with `ml_client.jobs.create_or_update()`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a v2 job using `AssetTypes.URI_FOLDER` and submit with `ml_client.jobs.create_or_update()`",
                "is_correct": true,
                "explanation": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it."
            }
        ],
        "feedback": "`AssetTypes.URI_FOLDER` is used to reference an entire folder (rather than individual files) as an input or output. In this case, it ensures your Parquet data folder is mounted into the job environment so your script can read and write directly to it.",
        "include_in_bank": true
    },
    {
        "question_id": "q_018",
        "type": "multiple_choice_multiple_answer",
        "question": "You want to ensure batch jobs do not incur cost when not running\n\nWhat two actions should you perform?",
        "options": [
            {
                "id": "A",
                "text": "Specify idle seconds before scale down to 0",
                "is_correct": false,
                "explanation": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior."
            },
            {
                "id": "B",
                "text": "Create an Azure Machine Learning **compute instance** target",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create an Azure Machine Learning **compute cluster** target",
                "is_correct": true,
                "explanation": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior."
            },
            {
                "id": "D",
                "text": "Specify the minimum number of cluster nodes to 0",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior.\n\n**Reference:**  \n- [Azure ML Compute Targets](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target)  \n- [Configure autoscale for Azure ML compute clusters](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-autoscale-compute-cluster)  \n\n---Compute clusters support autoscaling down to zero nodes, so creating a cluster target is required. By setting the minimum number of nodes to 0, the cluster will release all resources when no jobs are running, avoiding charges. It is not recommended to use compute instances for this scenario, and setting `idle_seconds_before_scaledown` to 0 can lead to unnecessary reprovisioning rather than relying on scale-to-zero behavior.",
        "include_in_bank": true
    },
    {
        "question_id": "q_020",
        "type": "multiple_choice_multiple_answer",
        "question": "You need to write a batch inference script using `ParallelRunStep`\n\nWhich two functions must be included?",
        "options": [
            {
                "id": "A",
                "text": "load()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "init()",
                "is_correct": true,
                "explanation": "The `init()` function is used to load and initialize resources (for example, loading the model) before any batches are processed. The `run(mini_batch)` function performs inference on each batch of input data. Other functions like `load()`, `evaluate()`, or `execute()` are not valid entry points for `ParallelRunStep`."
            },
            {
                "id": "C",
                "text": "run(mini_batch)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "evaluate(mini_batch)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "execute(mini_batch)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "The `init()` function is used to load and initialize resources (for example, loading the model) before any batches are processed. The `run(mini_batch)` function performs inference on each batch of input data. Other functions like `load()`, `evaluate()`, or `execute()` are not valid entry points for `ParallelRunStep`.",
        "include_in_bank": true
    },
    {
        "question_id": "q_023",
        "type": "multiple_choice_single_answer",
        "question": "Your company uses Azure Machine Learning\n\nYou plan to read a dataset from Azure Blob Storage for your machine learning project\n\nThe dataset is stored in columnar Parquet format at:",
        "options": [
            {
                "id": "A",
                "text": "`spark.read.text(\"wasbs://mycontainer.blob.core.windows.net\")`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "`spark.read.format(\"parquet\").load(\"wasbs://mycontainer.blob.core.windows.net\")`",
                "is_correct": true,
                "explanation": "The Spark DataFrameReader with `format(\"parquet\")` is the correct method to load columnar Parquet files. It efficiently reads the schema and data layout optimized for Parquet, whereas `text`, `avro`, or `csv` formats would not correctly parse Parquet data."
            },
            {
                "id": "C",
                "text": "`spark.read.format(\"avro\").load(\"wasbs://mycontainer.blob.core.windows.net\")`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "`spark.read.format(\"csv\").load(\"wasbs://mycontainer.blob.core.windows.net\")`",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "The Spark DataFrameReader with `format(\"parquet\")` is the correct method to load columnar Parquet files. It efficiently reads the schema and data layout optimized for Parquet, whereas `text`, `avro`, or `csv` formats would not correctly parse Parquet data.",
        "include_in_bank": true
    },
    {
        "question_id": "q_027",
        "type": "multiple_choice_single_answer",
        "question": "You create a binary classification model\n\nYou use the Fairlearn package to assess model fairness\n\nYou must eliminate the need to retrain the model\n\nYou need to implement the Fairlearn package\n\nWhich algorithm do you use?",
        "options": [
            {
                "id": "A",
                "text": "fairlearn.reductions.ExponentiatedGradient",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "fairlearn.postprocessing.ThresholdOptimizer",
                "is_correct": true,
                "explanation": "`ThresholdOptimizer` is a post\u2011processing method that takes a pre\u2011trained classifier and a sensitive feature, then adjusts decision thresholds to satisfy the Equal Opportunity constraint\u2014it does **not** retrain the model."
            },
            {
                "id": "C",
                "text": "fairlearn.preprocessing.CorrelationRemover",
                "is_correct": false,
                "explanation": "preprocessing requires retraining."
            },
            {
                "id": "D",
                "text": "fairlearn.reductions.GridSearch",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems. It is not mentioned in MS Learn nor in the Responsaible AI module.",
        "include_in_bank": true
    },
    {
        "question_id": "q_028",
        "type": "multiple_choice_multiple_answer",
        "question": "You are performing hyperparameter tuning of an ML model using the Azure ML SDK v2\n\nYou need an early termination policy that meets these requirements:\n\n* accounts for the performance of all previous runs when evaluating\n* avoids comparing the current run with only the best performing run to date",
        "options": [
            {
                "id": "A",
                "text": "Bandit",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Median stopping",
                "is_correct": true,
                "explanation": "The **Median Stopping** is a early termination policy based on running averages and their median."
            },
            {
                "id": "C",
                "text": "Truncation selection",
                "is_correct": true,
                "explanation": "The **Truncation Selection** policy is designed to drop a specified fraction of the poorest trials at each evaluation checkpoint, exactly matching your needs:"
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "q_029",
        "type": "multiple_choice_multiple_answer",
        "question": "You are configuring a Spark session in Azure Machine Learning Notebooks to process data stored in an Azure Data Lake Storage (ADLS) Gen2 account\n\nYou need to securely connect to this storage account without adding excessive complexity to your configuration\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Use a service principal for OAuth-based authentication by configuring the client ID, client secret, and tenant ID.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Use OAuth for authentication by configuring the `fs.azure.account.auth.type` and related properties.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use a SAS token for authentication by setting the `fs.azure.sas` property.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Use the storage account\u2019s access key to authenticate and set the `fs.azure.account.key` property.",
                "is_correct": true,
                "explanation": "Using the storage account access key is the simplest secure method for Spark in Azure ML Notebooks. It requires only a single secret, which you can store in Azure Key Vault or as a workspace secret, and avoids the overhead of registering an Azure AD application or handling token lifecycles. Service principals and OAuth configurations demand managing client IDs, secrets, tenant IDs, and often custom token\u2011refresh logic. SAS tokens introduce separate expiration and permission management. By contrast, the account key approach lets Spark authenticate directly to ADLS Gen2 with minimal setup while still allowing you to rotate the key centrally."
            }
        ],
        "feedback": "Using the storage account access key is the simplest secure method for Spark in Azure ML Notebooks. It requires only a single secret, which you can store in Azure Key Vault or as a workspace secret, and avoids the overhead of registering an Azure AD application or handling token lifecycles. Service principals and OAuth configurations demand managing client IDs, secrets, tenant IDs, and often custom token\u2011refresh logic. SAS tokens introduce separate expiration and permission management. By contrast, the account key approach lets Spark authenticate directly to ADLS Gen2 with minimal setup while still allowing you to rotate the key centrally.",
        "include_in_bank": true
    },
    {
        "question_id": "q_030",
        "type": "multiple_choice_multiple_answer",
        "question": "You want to create a datastore in Azure Machine Learning Studio\n\nBefore that, you need to provision the storage instance where your **relational** data will reside\n\nYou navigate to the \u201cDatastore\u201d option in Azure ML Studio\n\nWhich source storage type should you select?",
        "options": [
            {
                "id": "A",
                "text": "Azure Data Lake Gen2",
                "is_correct": false,
                "explanation": "This is a hierarchical storage system for big data files (e.g., Parquet, CSV, JSON)."
            },
            {
                "id": "B",
                "text": "Stores unstructured binary data (e.g., images, CSV files).",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Databricks File System",
                "is_correct": false,
                "explanation": "This is a distributed file system inside Azure Databricks"
            },
            {
                "id": "D",
                "text": "Azure Database for PostgreSQL",
                "is_correct": true,
                "explanation": "Azure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service\u2014Azure Database for PostgreSQL\u2014rather than file\u2011based storage like Blob or ADLS Gen2, which are designed for unstructured or semi\u2011structured files, not relational tables."
            }
        ],
        "feedback": "Azure ML Studio allows you to register various storage services as datastores. For **relational** data, you must choose a supported relational database service\u2014Azure Database for PostgreSQL\u2014rather than file\u2011based storage like Blob or ADLS Gen2, which are designed for unstructured or semi\u2011structured files, not relational tables.",
        "include_in_bank": true
    },
    {
        "question_id": "q_031",
        "type": "multiple_choice_single_answer",
        "question": "Your Azure ML Workspace\u2019s associated storage account keys were compromised and you regenerated them\n\nUsers are now experiencing access errors\n\nWhich CLI command should you run to update the workspace\u2019s storage keys?",
        "options": [
            {
                "id": "A",
                "text": "`az ml workspace update`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "`az ml workspace share`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "`az ml workspace sync-keys`",
                "is_correct": true,
                "explanation": "When you regenerate the access keys of the storage account linked to an Azure ML Workspace, the workspace loses its ability to access blobs and files. The `az ml workspace sync-keys` command pushes the new keys into the workspace metadata, restoring its access. Without running `sync-keys`, the workspace continues using the old credentials and will fail on any storage operations."
            }
        ],
        "feedback": "When you regenerate the access keys of the storage account linked to an Azure ML Workspace, the workspace loses its ability to access blobs and files. The `az ml workspace sync-keys` command pushes the new keys into the workspace metadata, restoring its access. Without running `sync-keys`, the workspace continues using the old credentials and will fail on any storage operations.",
        "include_in_bank": true
    },
    {
        "question_id": "q_034",
        "type": "multiple_choice_multiple_answer",
        "question": "Your organization provided a set of images for a multi\u2011label classification exercise\n\nYou need to present a tabular view showing each image alongside its class labels with minimal effort\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Export data labels to the binary file format.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Export data labels to the COCO file format.",
                "is_correct": true,
                "explanation": "The COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It\u2019s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing."
            },
            {
                "id": "C",
                "text": "Name the files with the label that was associated with them during classification.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Add the file name to the labels dataset.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "The COCO format captures both the image references (filenames or URIs) and their associated labels in a single JSON file. It\u2019s widely supported by tools like PyTorch, TorchVision and can be easily converted into a pandas DataFrame or other tabular view without manual file renaming or editing.",
        "include_in_bank": true
    },
    {
        "question_id": "q_035",
        "type": "multiple_choice_multiple_answer",
        "question": "You\u2019ve trained a linear regression model in Azure Machine Learning Studio and want users to call a real\u2011time endpoint on CPU with minimal cost for testing and debugging\n\nWhich two deployment targets should you select?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Instances",
                "is_correct": true,
                "explanation": "- **Azure Container Instances (ACI):** Quick to provision, low\u2011cost, ideal for light testing and short\u2011term real\u2011time inference on CPU."
            },
            {
                "id": "B",
                "text": "Azure Machine Learning Kubernetes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Local web service",
                "is_correct": true,
                "explanation": "- **Azure Container Instances (ACI):** Quick to provision, low\u2011cost, ideal for light testing and short\u2011term real\u2011time inference on CPU."
            },
            {
                "id": "D",
                "text": "Azure Machine Learning compute clusters",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "\n\n**Correct answers:**\n-**Azure Container Instances (ACI):** Quick to provision, low\u2011cost, ideal for light testing and short\u2011term real\u2011time inference on CPU.",
        "include_in_bank": true
    },
    {
        "question_id": "q_036",
        "type": "multiple_choice_multiple_answer",
        "question": "You are building a model to perform binary classification on a large dataset\n\nYou plan to tune the hyperparameters of the model in order to optimize its performance\n\nYou have defined a search space that includes the learning rate, the number of hidden layers, and the number of neurons in each hidden layer\n\n",
        "options": [
            {
                "id": "A",
                "text": "Gradient\u2011based optimization",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Grid sampling",
                "is_correct": true,
                "explanation": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient\u2011based methods optimize model parameters during training and are not designed for hyperparameter search."
            },
            {
                "id": "C",
                "text": "Random sampling",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Bayesian sampling",
                "is_correct": true,
                "explanation": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient\u2011based methods optimize model parameters during training and are not designed for hyperparameter search."
            }
        ],
        "feedback": "Bayesian sampling maintains a probabilistic model of the objective function and uses past evaluation results to guide the search toward promising regions (exploitation) while still exploring untested areas (exploration). This results in more efficient hyperparameter searches compared to exhaustive grid sampling or naive random sampling. Gradient\u2011based methods optimize model parameters during training and are not designed for hyperparameter search.",
        "include_in_bank": true
    },
    {
        "question_id": "q_038",
        "type": "multiple_choice_multiple_answer",
        "question": "You use Azure Machine Learning to create a machine learning pipeline\n\nYour dataset includes sparse string and numeric data\n\nWhile working with pipeline components, you receive an error indicating that **a value is required**\n\n",
        "options": [
            {
                "id": "A",
                "text": "Configure the Select Columns in Dataset component to exclude string data.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Use the Select Columns in Dataset component to choose a column.",
                "is_correct": true,
                "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The \u201cSelect Columns in Dataset\u201d component must be told which column(s) to include; without that, it throws \u201ca value is required.\u201d By using this component to choose at least one column, you satisfy the requirement and prevent the error."
            },
            {
                "id": "C",
                "text": "Configure a custom substitution value in the Clean Missing Data component.",
                "is_correct": true,
                "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The \u201cSelect Columns in Dataset\u201d component must be told which column(s) to include; without that, it throws \u201ca value is required.\u201d By using this component to choose at least one column, you satisfy the requirement and prevent the error."
            },
            {
                "id": "D",
                "text": "Specify the columns to be cleaned in the Clean Missing Data component.",
                "is_correct": true,
                "explanation": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The \u201cSelect Columns in Dataset\u201d component must be told which column(s) to include; without that, it throws \u201ca value is required.\u201d By using this component to choose at least one column, you satisfy the requirement and prevent the error."
            }
        ],
        "feedback": "Your pipeline failed because you did not explicitly select any columns for the component to operate on. The \u201cSelect Columns in Dataset\u201d component must be told which column(s) to include; without that, it throws \u201ca value is required.\u201d By using this component to choose at least one column, you satisfy the requirement and prevent the error.",
        "include_in_bank": true
    },
    {
        "question_id": "q_039",
        "type": "multiple_choice_multiple_answer",
        "question": "Your marketing team provides a 1\u00a0GB CSV file\n\nAll processing will happen in memory using pandas DataFrames\n\nYou need to recommend the minimum RAM configuration to support efficient processing\n\n",
        "options": [
            {
                "id": "A",
                "text": "2\u00a0GB",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "8\u00a0GB",
                "is_correct": false,
                "explanation": "Although the file is 1\u00a0GB on disk, loading into pandas can expand its in\u2011memory footprint to around 10\u00a0GB. To allow for transformations, filters, joins, and other operations without running out of memory, it\u2019s best to provision roughly twice that\u2014about 20\u00a0GB."
            },
            {
                "id": "C",
                "text": "10\u00a0GB",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "20\u00a0GB ",
                "is_correct": true,
                "explanation": "Although the file is 1\u00a0GB on disk, loading into pandas can expand its in\u2011memory footprint to around 10\u00a0GB. To allow for transformations, filters, joins, and other operations without running out of memory, it\u2019s best to provision roughly twice that\u2014about 20\u00a0GB."
            }
        ],
        "feedback": "Although the file is 1\u00a0GB on disk, loading into pandas can expand its in\u2011memory footprint to around 10\u00a0GB. To allow for transformations, filters, joins, and other operations without running out of memory, it\u2019s best to provision roughly twice that\u2014about 20\u00a0GB.",
        "include_in_bank": true
    },
    {
        "question_id": "q_042",
        "type": "multiple_choice_multiple_answer",
        "question": "You tune hyperparameters on your HyperDrive experiment using Random sampling\n\nYou want to terminate 30 percent of the lowest performing runs at each evaluation interval, based on the primary metric\n\nWhich early termination policy should you use?",
        "options": [
            {
                "id": "A",
                "text": "Median stopping policy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Bandit policy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "No termination policy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Truncation selection policy",
                "is_correct": true,
                "explanation": "The Truncation Selection policy is designed to drop a fixed percentage of the worst\u2011performing trials at each evaluation checkpoint, exactly matching the requirement to terminate 30\u00a0% of runs. It supports parameters such as `truncation_percentage`, `evaluation_interval`, and `delay_evaluation`. In contrast, the Bandit policy uses a slack factor rather than a fixed percentage, the Median Stopping policy cancels runs below the median without a configurable percentage, and choosing no termination policy would allow all runs to complete regardless of performance."
            }
        ],
        "feedback": "The Truncation Selection policy is designed to drop a fixed percentage of the worst\u2011performing trials at each evaluation checkpoint, exactly matching the requirement to terminate 30\u00a0% of runs. It supports parameters such as `truncation_percentage`, `evaluation_interval`, and `delay_evaluation`. In contrast, the Bandit policy uses a slack factor rather than a fixed percentage, the Median Stopping policy cancels runs below the median without a configurable percentage, and choosing no termination policy would allow all runs to complete regardless of performance.",
        "include_in_bank": true
    },
    {
        "question_id": "q_043",
        "type": "multiple_choice_multiple_answer",
        "question": "You are planning to create an Azure Machine Learning registry to share ML assets across multiple workspaces\n\nYou need to complete the YAML configuration for the registry deployment via Azure CLI\n\nWhich settings should you choose?",
        "options": [
            {
                "id": "A",
                "text": "`storage_account_hns: False`",
                "is_correct": true,
                "explanation": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory\u2011style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry."
            },
            {
                "id": "B",
                "text": "`storage_account_type: Standard_LRS`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "`storage_blob_encryption`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "`storage_account_tier`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "`access_tier`",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "`replication_type`",
                "is_correct": true,
                "explanation": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory\u2011style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry."
            }
        ],
        "feedback": "- **`storage_account_hns: False`** disables Hierarchical Namespace (HNS). HNS is required for directory\u2011style operations in Data Lake Storage Gen2; turning it off is appropriate when you do not need a hierarchical file structure in your registry.",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_024",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure ML SDK v2 and want to log a model evaluation report stored in a JSON file\n\nWhich MLflow method should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_text()",
                "is_correct": false,
                "explanation": "There is no `log_text` method in MLflow."
            },
            {
                "id": "B",
                "text": "mlflow.log_dict()",
                "is_correct": true,
                "explanation": "This method is used to log a Python dictionary or JSON-like object to the run as an artifact."
            },
            {
                "id": "C",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "This is used for scalar parameters, not for structured data like a JSON report."
            },
            {
                "id": "D",
                "text": "mlflow.set_tag()",
                "is_correct": false,
                "explanation": "Tags are used to attach metadata, not for logging JSON files."
            }
        ],
        "feedback": "Use `mlflow.log_dict()` to log structured evaluation reports like JSON files in your experiment runs.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_025",
        "type": "multiple_choice_single_answer",
        "question": "You want to invoke an online endpoint deployed in Azure ML SDK v2 and send a JSON payload\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.online_endpoints.run()",
                "is_correct": false,
                "explanation": "This method does not exist. Invocation is handled by a different method."
            },
            {
                "id": "B",
                "text": "ml_client.online_endpoints.invoke()",
                "is_correct": true,
                "explanation": "This is the correct method to call an online endpoint with input data."
            },
            {
                "id": "C",
                "text": "ml_client.endpoints.send_request()",
                "is_correct": false,
                "explanation": "There is no such method as `send_request()` in the SDK."
            },
            {
                "id": "D",
                "text": "mlflow.call_endpoint()",
                "is_correct": false,
                "explanation": "MLflow does not manage endpoint invocation in Azure ML SDK v2."
            }
        ],
        "feedback": "Use `ml_client.online_endpoints.invoke()` to interact with a deployed endpoint and send input data.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_026",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a pipeline with Azure ML SDK v2\n\nOne of the steps needs a compute target with GPU capabilities\n\nWhat should you define in the component YAML?",
        "options": [
            {
                "id": "A",
                "text": "resources: instance_type: Standard_DS3_v2",
                "is_correct": false,
                "explanation": "Standard_DS3_v2 is a CPU-based machine, not suitable for GPU workloads."
            },
            {
                "id": "B",
                "text": "resources: instance_type: Standard_NC6",
                "is_correct": true,
                "explanation": "Standard_NC6 is a GPU-enabled VM, suitable for tasks like deep learning."
            },
            {
                "id": "C",
                "text": "compute: azureml:gpu-cluster",
                "is_correct": false,
                "explanation": "This is a reference to the compute, but the question is about defining it in the component YAML."
            },
            {
                "id": "D",
                "text": "type: GPU",
                "is_correct": false,
                "explanation": "There is no 'type: GPU' declaration in Azure ML YAML schemas."
            }
        ],
        "feedback": "Use `resources: instance_type: Standard_NC6` in your component YAML to specify GPU support.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_027",
        "type": "multiple_choice_single_answer",
        "question": "You are running a job in Azure ML using a local compute\n\nWhich compute target value should you use in the job definition?",
        "options": [
            {
                "id": "A",
                "text": "local",
                "is_correct": true,
                "explanation": "Setting `compute: local` allows the job to run in your local environment without uploading code or data to the cloud."
            },
            {
                "id": "B",
                "text": "azureml:local-cluster",
                "is_correct": false,
                "explanation": "There is no built-in compute named 'local-cluster'. 'local' is the correct keyword."
            },
            {
                "id": "C",
                "text": "default",
                "is_correct": false,
                "explanation": "The default compute might point to a cloud cluster, not your local environment."
            },
            {
                "id": "D",
                "text": "None",
                "is_correct": false,
                "explanation": "Compute must be explicitly set to 'local' for local runs in Azure ML SDK v2."
            }
        ],
        "feedback": "To run jobs locally with Azure ML SDK v2, use `compute: local` in the job definition.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_028",
        "type": "multiple_choice_single_answer",
        "question": "You want to define a pipeline in Azure ML SDK v2 that has dependencies between steps\n\nWhat should you use to pass data between components?",
        "options": [
            {
                "id": "A",
                "text": "Environment variables",
                "is_correct": false,
                "explanation": "Environment variables are used for configuration, not for passing artifacts between steps."
            },
            {
                "id": "B",
                "text": "PipelineInput",
                "is_correct": false,
                "explanation": "PipelineInput is not a valid class in SDK v2. Use component outputs instead."
            },
            {
                "id": "C",
                "text": "Output ports of one component as inputs to another",
                "is_correct": true,
                "explanation": "This is the correct method to pass data and maintain dependencies between pipeline steps."
            },
            {
                "id": "D",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "mlflow is for experiment tracking, not for orchestrating data flow in pipelines."
            }
        ],
        "feedback": "In Azure ML pipelines, define outputs in one step and use them as inputs in the next to link components.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_029",
        "type": "multiple_choice_single_answer",
        "question": "You are trying to log multiple metrics from a model evaluation loop using MLflow in Azure ML SDK v2\n\nWhich approach is correct?",
        "options": [
            {
                "id": "A",
                "text": "Use mlflow.log_metrics({'accuracy': 0.92, 'precision': 0.88})",
                "is_correct": true,
                "explanation": "mlflow.log_metrics() accepts a dictionary to log multiple metrics at once."
            },
            {
                "id": "B",
                "text": "Use mlflow.set_tags()",
                "is_correct": false,
                "explanation": "Tags are used for metadata, not for metrics logging."
            },
            {
                "id": "C",
                "text": "Use mlflow.save_metrics()",
                "is_correct": false,
                "explanation": "This function does not exist in MLflow."
            },
            {
                "id": "D",
                "text": "Use mlflow.log_param()",
                "is_correct": false,
                "explanation": "log_param() is used for model parameters, not performance metrics."
            }
        ],
        "feedback": "To log multiple evaluation results, use mlflow.log_metrics() with a dictionary of metrics.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_030",
        "type": "multiple_choice_single_answer",
        "question": "You need to define a data input for a job using a remote folder stored in Azure Blob Storage\n\nWhat input type should you specify in the Azure ML SDK v2?",
        "options": [
            {
                "id": "A",
                "text": "Input(type='mltable')",
                "is_correct": false,
                "explanation": "MLTable is used for structured tabular data, not arbitrary remote folders."
            },
            {
                "id": "B",
                "text": "Input(type='uri_folder')",
                "is_correct": true,
                "explanation": "uri_folder is the correct input type to specify a folder path in Azure storage."
            },
            {
                "id": "C",
                "text": "Input(type='string')",
                "is_correct": false,
                "explanation": "string is for parameter values, not data inputs."
            },
            {
                "id": "D",
                "text": "Input(type='uri_file')",
                "is_correct": false,
                "explanation": "uri_file is for individual files, not entire folders."
            }
        ],
        "feedback": "Use Input(type='uri_folder') when referencing a folder in Azure storage as input for a job.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_031",
        "type": "multiple_choice_single_answer",
        "question": "You need to deploy a model trained with a custom PyFunc wrapper using MLflow in Azure ML\n\nWhich MLflow method should be used?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.sklearn.log_model()",
                "is_correct": false,
                "explanation": "This method is specific to scikit-learn models, not custom PyFunc models."
            },
            {
                "id": "B",
                "text": "mlflow.pyfunc.log_model()",
                "is_correct": true,
                "explanation": "mlflow.pyfunc.log_model() is used to log models with custom Python functions for inference."
            },
            {
                "id": "C",
                "text": "mlflow.register_model()",
                "is_correct": false,
                "explanation": "This registers a model after it has been logged; it doesn't handle logging itself."
            },
            {
                "id": "D",
                "text": "mlflow.log_model()",
                "is_correct": false,
                "explanation": "This generic function is not specific to PyFunc; using the correct flavor is recommended."
            }
        ],
        "feedback": "Use mlflow.pyfunc.log_model() to log custom models defined using the PyFunc interface for deployment.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_032",
        "type": "multiple_choice_single_answer",
        "question": "You want to trigger a retraining pipeline in Azure ML whenever new data is added to a Blob Storage container\n\nWhich tool should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Monitor",
                "is_correct": false,
                "explanation": "Azure Monitor is used for diagnostics and alerting, not for triggering pipelines."
            },
            {
                "id": "B",
                "text": "Azure Event Grid",
                "is_correct": true,
                "explanation": "Azure Event Grid can trigger Azure ML pipelines based on storage events like file uploads."
            },
            {
                "id": "C",
                "text": "Azure Application Insights",
                "is_correct": false,
                "explanation": "Application Insights is for monitoring apps, not orchestrating ML workflows."
            },
            {
                "id": "D",
                "text": "Azure Policy",
                "is_correct": false,
                "explanation": "Azure Policy is for enforcing governance rules, not triggering ML pipelines."
            }
        ],
        "feedback": "Azure Event Grid is designed to respond to events such as new data arrivals and can trigger Azure ML workflows.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_033",
        "type": "multiple_choice_single_answer",
        "question": "You are running a job in Azure ML and want to store logs and outputs in a custom Azure Storage account\n\nWhat should you configure?",
        "options": [
            {
                "id": "A",
                "text": "Default compute target",
                "is_correct": false,
                "explanation": "The compute target defines where the job runs, not where outputs are stored."
            },
            {
                "id": "B",
                "text": "Default datastore",
                "is_correct": true,
                "explanation": "The default datastore determines where outputs and logs are stored during job execution."
            },
            {
                "id": "C",
                "text": "Environment image",
                "is_correct": false,
                "explanation": "The environment defines dependencies, not storage."
            },
            {
                "id": "D",
                "text": "Pipeline parameters",
                "is_correct": false,
                "explanation": "Parameters are used to customize jobs, not control storage behavior."
            }
        ],
        "feedback": "Set the default datastore in your workspace to control where logs and outputs are written.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_034",
        "type": "multiple_choice_single_answer",
        "question": "You are defining a command job in Azure ML SDK v2\n\nYou want to specify a Python script, its environment, and data inputs\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.jobs.create_or_update()",
                "is_correct": true,
                "explanation": "This method is used to submit command jobs defined in a YAML file or Python dictionary."
            },
            {
                "id": "B",
                "text": "ml_client.scripts.run()",
                "is_correct": false,
                "explanation": "This method does not exist in Azure ML SDK v2."
            },
            {
                "id": "C",
                "text": "mlflow.run_job()",
                "is_correct": false,
                "explanation": "This is not part of the Azure ML SDK v2; MLflow does not submit Azure ML jobs directly."
            },
            {
                "id": "D",
                "text": "ml_client.pipeline.execute()",
                "is_correct": false,
                "explanation": "pipeline.execute() is used for pipelines, not individual command jobs."
            }
        ],
        "feedback": "Use `ml_client.jobs.create_or_update()` to submit and configure command jobs in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_035",
        "type": "multiple_choice_single_answer",
        "question": "You need to configure the responsible AI dashboard in Azure Machine Learning\n\nWhich tool allows you to generate insights such as feature importance and error analysis?",
        "options": [
            {
                "id": "A",
                "text": "azureml.interpret package",
                "is_correct": true,
                "explanation": "The azureml.interpret package provides utilities to create visualizations for Responsible AI, including feature importance and error analysis."
            },
            {
                "id": "B",
                "text": "mlflow.interpret()",
                "is_correct": false,
                "explanation": "This method does not exist in MLflow or Azure ML."
            },
            {
                "id": "C",
                "text": "azureml.automl.core.analyze_model()",
                "is_correct": false,
                "explanation": "This method is not related to the Responsible AI dashboard."
            },
            {
                "id": "D",
                "text": "azureml.core.ResponsibleAI",
                "is_correct": false,
                "explanation": "There is no such class; insights are created through interpret and dashboard packages."
            }
        ],
        "feedback": "Use the `azureml.interpret` package to enable interpretability and fairness tools in Azure ML.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_036",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure ML SDK v2 and want to register a local model file to your workspace\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.models.upload()",
                "is_correct": false,
                "explanation": "There is no method called upload() for models in SDK v2."
            },
            {
                "id": "B",
                "text": "ml_client.models.create_or_update()",
                "is_correct": true,
                "explanation": "This method registers a model by uploading the local file to the Azure ML workspace."
            },
            {
                "id": "C",
                "text": "mlflow.register_model()",
                "is_correct": false,
                "explanation": "This is used to register models in MLflow\u2019s registry, not directly in Azure ML via SDK v2."
            },
            {
                "id": "D",
                "text": "ml_client.register_model()",
                "is_correct": false,
                "explanation": "There is no method named register_model in the SDK v2."
            }
        ],
        "feedback": "Use `ml_client.models.create_or_update()` to register local models in Azure ML workspace using SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_037",
        "type": "multiple_choice_single_answer",
        "question": "You want to log a dictionary of hyperparameters from a grid search in Azure ML using MLflow\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_params()",
                "is_correct": true,
                "explanation": "mlflow.log_params() is designed to log a dictionary of key-value pairs as parameters."
            },
            {
                "id": "B",
                "text": "mlflow.log_dict()",
                "is_correct": false,
                "explanation": "This logs structured data, but it\u2019s not appropriate for hyperparameters that should appear in the UI as params."
            },
            {
                "id": "C",
                "text": "mlflow.log_metrics()",
                "is_correct": false,
                "explanation": "log_metrics() is used for numerical performance values, not hyperparameters."
            },
            {
                "id": "D",
                "text": "mlflow.set_tag()",
                "is_correct": false,
                "explanation": "Tags are for metadata, not for parameters or metrics."
            }
        ],
        "feedback": "Use mlflow.log_params() to efficiently log multiple hyperparameters from a dictionary structure.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_038",
        "type": "multiple_choice_single_answer",
        "question": "You want to visualize model predictions versus true labels in Azure ML and include the plot in your MLflow run\n\nWhich function should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.log_metric()",
                "is_correct": false,
                "explanation": "log_metric() is for logging scalar numerical values, not plots or images."
            },
            {
                "id": "B",
                "text": "mlflow.set_tag()",
                "is_correct": false,
                "explanation": "Tags are for attaching metadata to runs, not for logging files."
            },
            {
                "id": "C",
                "text": "mlflow.log_image()",
                "is_correct": true,
                "explanation": "log_image() is the correct method to log image files such as plots to MLflow."
            },
            {
                "id": "D",
                "text": "mlflow.log_artifact()",
                "is_correct": false,
                "explanation": "While it can log files, it doesn\u2019t directly display images in the MLflow UI as conveniently as log_image()."
            }
        ],
        "feedback": "Use mlflow.log_image() to log plots like prediction vs. label comparisons into your experiment tracking.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_039",
        "type": "multiple_choice_single_answer",
        "question": "You are orchestrating a training pipeline in Azure ML and want to reuse components across multiple pipelines\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Define each component inline in the pipeline YAML.",
                "is_correct": false,
                "explanation": "Defining components inline reduces reusability across different pipelines."
            },
            {
                "id": "B",
                "text": "Store components in separate YAML files and register them.",
                "is_correct": true,
                "explanation": "Storing components in separate YAML files allows you to register and reuse them across multiple pipelines."
            },
            {
                "id": "C",
                "text": "Use mlflow.register_component().",
                "is_correct": false,
                "explanation": "This function does not exist; MLflow does not manage Azure ML components."
            },
            {
                "id": "D",
                "text": "Use component.set_reuse(True)",
                "is_correct": false,
                "explanation": "This method does not exist in Azure ML SDK v2."
            }
        ],
        "feedback": "For component reuse, define and register components via standalone YAML files in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_040",
        "type": "multiple_choice_single_answer",
        "question": "You are using MLflow in Azure Machine Learning to track a training run\n\nYou want to record the version of a dataset used in the experiment\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "mlflow.set_tag()",
                "is_correct": true,
                "explanation": "mlflow.set_tag() is ideal for storing metadata like dataset versions that are not numeric or model-specific artifacts."
            },
            {
                "id": "B",
                "text": "mlflow.log_metric()",
                "is_correct": false,
                "explanation": "log_metric() is for numeric values and not appropriate for version labels."
            },
            {
                "id": "C",
                "text": "mlflow.log_param()",
                "is_correct": false,
                "explanation": "While log_param could be used, set_tag is preferred for metadata like dataset versions."
            },
            {
                "id": "D",
                "text": "mlflow.log_dict()",
                "is_correct": false,
                "explanation": "log_dict() stores structured data, not a single string value like a dataset version."
            }
        ],
        "feedback": "Use mlflow.set_tag() to store metadata such as dataset version identifiers for better experiment traceability.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_041",
        "type": "multiple_choice_single_answer",
        "question": "You need to load an MLTable dataset registered in your Azure ML workspace using Python SDK v2\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.data.get(name='my_dataset')",
                "is_correct": true,
                "explanation": "This is the correct way to retrieve a registered dataset using the MLClient object in SDK v2."
            },
            {
                "id": "B",
                "text": "Dataset.File.from_files()",
                "is_correct": false,
                "explanation": "This method is from SDK v1 and is not used in SDK v2."
            },
            {
                "id": "C",
                "text": "mlflow.load_table()",
                "is_correct": false,
                "explanation": "This method does not exist in MLflow or Azure ML."
            },
            {
                "id": "D",
                "text": "load_dataset('my_dataset')",
                "is_correct": false,
                "explanation": "There is no generic function called load_dataset() in the SDK v2."
            }
        ],
        "feedback": "Use `ml_client.data.get()` to retrieve and use MLTable datasets in your SDK v2 workflows.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_042",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a YAML definition for a command component in Azure ML SDK v2\n\nWhich section is required to specify the code that should be executed?",
        "options": [
            {
                "id": "A",
                "text": "entry_script",
                "is_correct": false,
                "explanation": "entry_script is a setting used inside environments, but not in component YAML directly."
            },
            {
                "id": "B",
                "text": "command",
                "is_correct": true,
                "explanation": "The `command` field defines what is executed when the component runs."
            },
            {
                "id": "C",
                "text": "script",
                "is_correct": false,
                "explanation": "script is not a valid field in component YAML schema."
            },
            {
                "id": "D",
                "text": "run",
                "is_correct": false,
                "explanation": "run is not a valid field in YAML definitions for components."
            }
        ],
        "feedback": "The `command` field in component YAML defines the script or shell command executed in Azure ML SDK v2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_043",
        "type": "multiple_choice_single_answer",
        "question": "You need to create a reusable environment for multiple jobs in Azure Machine Learning SDK v2\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Define the environment inline inside each job YAML",
                "is_correct": false,
                "explanation": "This creates duplication and makes reuse difficult across jobs."
            },
            {
                "id": "B",
                "text": "Create the environment once and register it in the workspace",
                "is_correct": true,
                "explanation": "Registering the environment allows referencing it by name/version across different jobs and components."
            },
            {
                "id": "C",
                "text": "Embed the environment in the compute cluster definition",
                "is_correct": false,
                "explanation": "Environments and compute are configured separately in Azure ML."
            },
            {
                "id": "D",
                "text": "Use mlflow.environment.create()",
                "is_correct": false,
                "explanation": "There is no such MLflow method. Environment management is handled by Azure ML SDK."
            }
        ],
        "feedback": "For better maintainability, define and register environments in Azure ML once and reuse by reference.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_044",
        "type": "multiple_choice_single_answer",
        "question": "You need to reference a component stored in a YAML file when building a pipeline in Azure ML SDK v2\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "ml_client.load_yaml_component()",
                "is_correct": false,
                "explanation": "There is no such method in the Azure ML SDK v2."
            },
            {
                "id": "B",
                "text": "load_component(source='component.yml')",
                "is_correct": true,
                "explanation": "The load_component function loads a component from a YAML file for use in pipelines."
            },
            {
                "id": "C",
                "text": "mlflow.load_component()",
                "is_correct": false,
                "explanation": "MLflow does not manage Azure ML pipeline components."
            },
            {
                "id": "D",
                "text": "register_component(source='component.yml')",
                "is_correct": false,
                "explanation": "register_component is not a valid method in SDK v2."
            }
        ],
        "feedback": "Use load_component() to load and reuse YAML-defined components when constructing pipelines in Azure ML.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_045",
        "type": "multiple_choice_single_answer",
        "question": "You want to visualize data drift over time using Azure ML\u2019s monitoring capabilities\n\nWhat must be configured in the MonitorSchedule?",
        "options": [
            {
                "id": "A",
                "text": "primary_metric",
                "is_correct": false,
                "explanation": "primary_metric is used in training jobs, not in monitoring configuration."
            },
            {
                "id": "B",
                "text": "data_drift_signal",
                "is_correct": true,
                "explanation": "data_drift_signal is used within the MonitorSchedule to define how drift is calculated and detected."
            },
            {
                "id": "C",
                "text": "compute_target",
                "is_correct": false,
                "explanation": "compute_target is related to job execution, not drift monitoring."
            },
            {
                "id": "D",
                "text": "model_explanation_config",
                "is_correct": false,
                "explanation": "This config relates to interpretability and not drift monitoring."
            }
        ],
        "feedback": "To detect and visualize data drift over time in Azure ML, configure the `data_drift_signal` within a MonitorSchedule.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_046",
        "type": "multiple_choice_single_answer",
        "question": "(hotspot) Fill-in the blank\n___ is required for a Deep Learning Virtual Machine (DVLM) to support Compute Unified Device Architecture (CUDA) computations.",
        "options": [
            {
                "id": "A",
                "text": "SSD",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "FPGA",
                "is_correct": false,
                "explanation": "Not quite."
            },
            {
                "id": "C",
                "text": "GPU",
                "is_correct": true,
                "explanation": "GPU is required."
            },
            {
                "id": "D",
                "text": "Power BI",
                "is_correct": false,
                "explanation": "Definitely not."
            }
        ],
        "feedback": "A DVLM is a pre-configured environment for deep learning using GPU instances.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "dp100_047",
        "type": "multiple_choice_single_answer",
        "question": "(hotspot) You need to implement a Data Science Virtual Machine (DSVM) that supports the Caffe2 deep learning framework\n\n **Which of the following DSVM should you create?**",
        "options": [
            {
                "id": "A",
                "text": "Windows Server 2012 DSVM",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Windows Server 2016 DSVM",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Ubuntu 16.04 DSVM",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "CentOS 7.4 DSVM",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "**This might be completely outdated**.\nCaffe2 is supported by DSVM for Linux. Microsoft offers Linux edition of the DSVM on Ubuntu 16.04 LTS and CentOS 7.4. However, only the DSVM on Ubuntu is preconfigured for Caffe2.",
        "sdk_version": "v2",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_cc5631239d9d4f8c811db0090aaad575",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou have been tasked with employing a machine learning model, which makes use of a PostgreSQL database and needs GPU processing, to forecast prices.\n\nYou are preparing to create a virtual machine that has the necessary tools built into it.\n\nYou need to make use of the correct virtual machine type.\n\nRecommendation: You make use of a Geo AI Data Science Virtual Machine (Geo-DSVM) Windows edition.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c4af0d4690424675b4598cfc160e37d8",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou use Azure Machine Learning designer to load the following datasets into an experiment:\n\nDataset1 | Age | Length | Width |\n|----------|----------|----------|\n| 3       | 22       | 13       |\n| 7       | 11       | 86       |\n| 18       | 32       | 95       | \n\n\nDataset2 \n\n\nYou need to create a dataset that has the same columns and header row as the input datasets and contains all rows from both input datasets.\n\nSolution: Use the Join Data module.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d371129d2b03414a859330b38eec34f0",
        "type": "multiple_choice_single_answer",
        "question": "You write a Python script that processes data in a comma-separated values (CSV) file.\n\nYou plan to run this script as an Azure Machine Learning experiment.\n\nThe script loads the data and determines the number of rows it contains using the following code:\n\n```python\nfrom azureml.core import\nimport pandas as pd\n\nrun = Run.get_context()\ndata = pd.read_csv(\"./data.csv\")\nrows = len(data)\n# record row_count metric here\n```\n\nYou need to record the row count as a metric named row_count that can be returned using the get_metrics method of the Run object after the experiment run completes.\n\nWhich code should you use?",
        "options": [
            {
                "id": "A",
                "text": "run.upload_file(T3 row_count', './data.csv')",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "run.log('row_count', rows)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "run.tag('row_count', rows)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "run.log_table('row_count', rows)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "run.log_row('row_count', rows)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2ed8357f672e42c895e9e26274b25ed8",
        "type": "multiple_choice_multiple_answer",
        "question": "You run a script as an experiment in Azure Machine Learning.\n\nYou have a Run object named run that references the experiment run. You must review the log files that were generated during the experiment run.\n\nYou need to download the log files to a local folder for review.\n\nWhich two code segments can you run to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "run.get_details()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "run.get_file_names()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "run.get_metrics()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "run.download_files(output_directory='./runfiles')",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "run.get_all_logs(destination='./runlogs')",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: AE.\nCommunity:\n100% for DE\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_78b27c3255e14f49bab0a03773c91c34",
        "type": "multiple_choice_multiple_answer",
        "question": "You create an Azure Machine Learning managed compute resource. The compute resource is configured as follows:\n\n- Minimum nodes: 2\n\n- Maximum nodes: 4\n\nYou must decrease the minimum number of nodes and increase the maximum number of nodes to the following values:\n\n- Minimum nodes: 0\n\n- Maximum nodes: 8\n\nYou need to reconfigure the compute resource.\n\nWhich three methods can you use? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Azure Machine Learning designer",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "MLClient class in Python SDK v2",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Machine Learning studio",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure CLI ml extension v2",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "BuildContext class in Python SDK v2",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: ACD.\nCommunity:\n83% for BCD\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_e83d491092bc4e8a82072e0c316422d6",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou have been tasked with employing a machine learning model, which makes use of a PostgreSQL database and needs GPU processing, to forecast prices.\n\nYou are preparing to create a virtual machine that has the necessary tools built into it.\n\nYou need to make use of the correct virtual machine type.\n\nRecommendation: You make use of a Data Science Virtual Machine (DSVM) Windows edition.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n58% for B\n42% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_92bb1fdb9dcc441fb9356a171e3d5a84",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train a classification model by using a logistic regression algorithm.\n\nYou must be able to explain the model's predictions by calculating the importance of each feature, both as an overall global relative importance value and as a measure of local importance for a specific set of predictions.\n\nYou need to create an explainer that you can use to retrieve the required global and local feature importance values.\n\nSolution: Create a TabularExplainer.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_7b9ac2f95c014eb589b7c135fc09c3e9",
        "type": "multiple_choice_multiple_answer",
        "question": "You are attaching an Azure Databricks-based compute resource to an Azure Machine Learning development workspace.\n\nYou need to configure parameters to attach the resource.\n\nWhich three parameters should you use? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Workspace name",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Compute name",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Workspace user credentials",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Workspace resource ID",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Access token",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_856d82080de549a3b6a8480cc94a2379",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of creating a machine learning model. Your dataset includes rows with null and missing values.\n\nYou plan to make use of the Clean Missing Data module in Azure Machine Learning Studio to detect and fix the null and missing values in the dataset.\n\nRecommendation: You make use of the Custom substitution value option.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n76% for A\n24% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_0d538796535e4792a13ed02527f8e507",
        "type": "multiple_choice_multiple_answer",
        "question": "You create an Azure Machine Learning workspace. You are preparing a local Python environment on a laptop computer. You want to use the laptop to connect to the workspace and run experiments.\n\nYou create the following config.json file.\n\n{\n\n\"workspace_name\" : \"ml-workspace\"\n\n}\n\nYou must use the Azure Machine Learning SDK to interact with data and experiments in the workspace.\n\nYou need to configure the config.json file to connect to the workspace from the Python environment.\n\nWhich two additional parameters must you add to the config.json file in order to connect to the workspace? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "login",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "resource_group",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "subscription_id",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "key",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "region",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_4176487ce8784d60b3b7deaaabb3733a",
        "type": "multiple_choice_single_answer",
        "question": "You use the following code to run a script as an experiment in Azure Machine Learning:\n\n```python\nfrom azureml.core import Workspace, Experiment, Run\nfrom azureml.core import RunConfig, ScriptRunConfig\n\nws = Workspace.from_config()\nrun_config = RunConfiguration()\nrun_config.target='local'\nscript_config = ScriptRunConfig(\n    source_directory='./script',\n    script='experiment.py',\n    run_config=run_config\n)\nexperiment = Experiment(workspace=ws, name='script experiment')\nrun = experiment.submit(config=script_config)\nrun.wait_for_completion()\n```\n\nYou must identify the output files that are generated by the experiment run.\n\nYou need to add code to retrieve the output file names.\n\nWhich code segment should you add to the script?",
        "options": [
            {
                "id": "A",
                "text": "files = run.get_properties()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "files= run.get_file_names()",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "files = run.get_details_with_logs()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "files = run.get_metrics()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "files = run.get_details()",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_95d37dc5524c473c8fedf9e082ae6472",
        "type": "multiple_choice_single_answer",
        "question": "You plan to run a script as an experiment using a Script Run Configuration. The script uses modules from the scipy library as well as several\n\nPython packages that are not typically installed in a default conda environment.\n\nYou plan to run the experiment on your local workstation for small datasets and scale out the experiment by running it on more powerful remote compute clusters for larger datasets.\n\nYou need to ensure that the experiment runs successfully on local and remote compute with the least administrative effort.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Do not specify an environment in the run configuration for the experiment. Run the experiment by using the default environment.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a virtual machine (VM) with the required Python configuration and attach the VM as a compute target. Use this compute target for all experiment runs.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create and register an Environment that includes the required packages. Use this Environment for all experiment runs.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a config.yaml file defining the conda packages that are required and save the file in the experiment folder.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Always run the experiment with an Estimator by using the default packages.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d0181195f12f4a69acd33352c81db2e3",
        "type": "multiple_choice_single_answer",
        "question": "You use Azure Machine Learning designer to create a training pipeline for a regression model.\n\nYou need to prepare the pipeline for deployment as an endpoint that generates predictions asynchronously for a dataset of input data values.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Clone the training pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a batch inference pipeline from the training pipeline.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create a real-time inference pipeline from the training pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Replace the dataset in the training pipeline with an Enter Data Manually module.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_94bb9a20ce1f4f2dba5ab9af9e109252",
        "type": "multiple_choice_single_answer",
        "question": "You use an Azure Machine Learning workspace.\n\nYou have a trained model that must be deployed as a web service. Users must authenticate by using Azure Active Directory.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Deploy the model to Azure Kubernetes Service (AKS). During deployment, set the token_auth_enabled parameter of the target configuration object to true",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Deploy the model to Azure Container Instances. During deployment, set the auth_enabled parameter of the target configuration object to true",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Deploy the model to Azure Container Instances. During deployment, set the token_auth_enabled parameter of the target configuration object to true",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Deploy the model to Azure Kubernetes Service (AKS). During deployment, set the auth.enabled parameter of the target configuration object to true",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ce73b3a8e3c04c00a6197a89ecf16beb",
        "type": "multiple_choice_single_answer",
        "question": "You create a workspace to include a compute instance by using Azure Machine Learning Studio. You are developing a Python SDK v2 notebook in the workspace.\n\nYou need to use Intellisense in the notebook.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Stop the compute instance.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Start the compute instance.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Run a %pip magic function on the compute instance.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Run a !pip magic function on the compute instance.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c018e723c7af42508cc746f7711ccebe",
        "type": "multiple_choice_single_answer",
        "question": "You train a machine learning model.\n\nYou must deploy the model as a real-time inference service for testing. The service requires low CPU utilization and less than 48 MB of RAM. The compute target for the deployed service must initialize automatically while minimizing cost and administrative overhead.\n\nWhich compute target should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Container Instance (ACI)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "attached Azure Databricks cluster",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Kubernetes Service (AKS) inference cluster",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Machine Learning compute cluster",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_513db85da45e4d9caadd2f67418fe20e",
        "type": "multiple_choice_single_answer",
        "question": "You are building a machine learning model for translating English language textual content into French language textual content.\n\nYou need to build and train the machine learning model to learn the sequence of the textual content.\n\nWhich type of neural network should you use?",
        "options": [
            {
                "id": "A",
                "text": "Multilayer Perceptions (MLPs)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Convolutional Neural Networks (CNNs)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Recurrent Neural Networks (RNNs)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Generative Adversarial Networks (GANs)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_94852dd86ab9432bb049833695965bfa",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou use Azure Machine Learning designer to load the following datasets into an experiment:\n\nDataset1 \n\n\nDataset2 \n\n\nYou need to create a dataset that has the same columns and header row as the input datasets and contains all rows from both input datasets.\n\nSolution: Use the Execute Python Script module.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n92% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6f1143afa2fe4fd28cdf25caca93e1e0",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to use a Python script to run an Azure Machine Learning experiment. The script creates a reference to the experiment run context, loads data from a file, identifies the set of unique values for the label column, and completes the experiment run:\n\n```python\nfrom azureml.core import Run\nimport pandas as pd\n\nrun = Run.get_context()\ndata = pd.read_csv('data.csv')\nlabel_vals = data['label'].unique()\n\n# Add code to record metrics here\n\nrun.complete()\n```\n\nThe experiment must record the unique labels in the data as metrics for the run that can be reviewed later.\n\nYou must add code to the script to record the unique label values as run metrics at the point indicated by the comment.\n\nSolution: Replace the comment with the following code:\n\n```python\nfor label_val in label_vals:\n    run.log('Label Values', label_val)\n```\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n64% for B\n36% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d27c1d67f4ff474e858961cff4c0d7a4",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of carrying out feature engineering on a dataset.\n\nYou want to add a feature to the dataset and fill the column value.\n\nRecommendation: You must make use of the Edit Metadata Azure Machine Learning Studio module.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n89% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_1bc0f981bd244966883b4774a610f903",
        "type": "multiple_choice_single_answer",
        "question": "You use the Azure Machine Learning service to create a tabular dataset named training_data. You plan to use this dataset in a training script.\n\nYou create a variable that references the dataset using the following code: training_ds = workspace.datasets.get(\"training_data\")\n\nYou define an estimator to run the script.\n\nYou need to set the correct property of the estimator to ensure that your script can access the training_data dataset.\n\nWhich property should you set?",
        "options": [
            {
                "id": "A",
                "text": "environment_definition = {\"training_data\":training_ds}",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "inputs = [training_ds.as_named_input('training_ds')]",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "script_params = {\"--training_ds\":training_ds}",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "source_directory = training_ds",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_0b189354cd12461d956738b0b0dbff66",
        "type": "multiple_choice_single_answer",
        "question": "You are determining if two sets of data are significantly different from one another by using Azure Machine Learning Studio.\n\nEstimated values in one set of data may be more than or less than reference values in the other set of data. You must produce a distribution that has a constant Type I error as a function of the correlation.\n\nYou need to produce the distribution.\n\nWhich type of distribution should you produce?",
        "options": [
            {
                "id": "A",
                "text": "Unpaired t-test with a two-tail option",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Unpaired t-test with a one-tail option",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Paired t-test with a one-tail option",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Paired t-test with a two-tail option",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_5f134fa4081742b38d45943f14029ac2",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou plan to use a Python script to run an Azure Machine Learning experiment. The script creates a reference to the experiment run context, loads data from a file, identifies the set of unique values for the label column, and completes the experiment run:\n\n```python\nfrom azureml.core import Run\nimport pandas as pd\n\nrun = Run.get_context()\ndata = pd.read_csv('data.csv')\nlabel_vals = data['label'].unique()\n# Add code to record metrics here\n\nrun.complete()\n```\n\nThe experiment must record the unique labels in the data as metrics for the run that can be reviewed later.\n\nYou must add code to the script to record the unique label values as run metrics at the point indicated by the comment.\n\n**Solution:**\n Replace the comment with the following code:\n\n```python\nrun.upload_file('outputs/labels.csv', './data.csv')\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_7eaef3486e07491c819dd14f240f2c47",
        "type": "multiple_choice_single_answer",
        "question": "You create and register a model in an Azure Machine Learning workspace.\n\nYou must use the Azure Machine Learning SDK to implement a batch inference pipeline that uses a ParallelRunStep to score input data using the model. You must specify a value for the ParallelRunConfig compute_target setting of the pipeline step.\n\nYou need to create the compute target.\n\nWhich class should you use?",
        "options": [
            {
                "id": "A",
                "text": "BatchCompute",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "AdlaCompute",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "AmlCompute",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "AksCompute",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_fc461bff476a4c6389756628baefd6b8",
        "type": "multiple_choice_single_answer",
        "question": "You create a batch inference pipeline by using the Azure ML SDK. You configure the pipeline parameters by executing the following code:\n\n```python\nfrom azureml.contrib.pipeline.steps import ParallelRunConfig\nparallel_run_config = ParallelRunConfig(\n    source_directory=scripts_folder,\n    entry_script=batch_pipeline.py,\n    mini_batch_size=5,\n    error_threshold=10,\n    output_action=append_row,\n    environment=batch_env,\n    compute_target=compute_target,\n    logging_level=DEBUG,\n    node_count=4\n)\n```\n\nYou need to obtain the output from the pipeline execution.\n\nWhere will you find the output?",
        "options": [
            {
                "id": "A",
                "text": "the digit_identification.py script",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "the debug log",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "the Activity Log in the Azure portal for the Machine Learning workspace",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "the Inference Clusters tab in Machine Learning studio",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "a file named parallel_run_step.txt located in the output folder",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_06480b070f074da1bd446f607541a0ac",
        "type": "multiple_choice_single_answer",
        "question": "You have recently concluded the construction of a binary classification machine learning model.\n\nYou are currently assessing the model. You want to make use of a visualization that allows for precision to be used as the measurement for the assessment.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should consider using Venn diagram visualization.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should consider using Receiver Operating Characteristic (ROC) curve visualization.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should consider using Box plot visualization.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should consider using the Binary classification confusion matrix visualization.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2972914cb71b468db2e883651e73e89c",
        "type": "multiple_choice_single_answer",
        "question": "You create a model to forecast weather conditions based on historical data.\n\nYou need to create a pipeline that runs a processing component to load data from a datastore and pass the processed data to a machine learning model training component.\n\nSolution: Run the following code:\n\n```python\nfrom azure.ai.ml import MLClient, Input, Output\nfrom azure.ai.ml.dsl import pipeline\nfrom azure.identity import DefaultAzureCredential\n\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n@pipeline()\ndef weather_pipeline(input_data: Input):\n    processed_data = process_component(data=input_data)\n    train_component(data=processed_data)\n\npipeline_job = weather_pipeline(\n    input_data=Input(\n        type='uri_folder',\n        path='azureml:ml-data@latest'  # reference to datastore dataset\n    )\n)\n\nml_client.jobs.create_or_update(pipeline_job, experiment_name='weather_forecast_experiment')\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": "This SDK v2 pipeline correctly passes a datastore dataset as an Input to the processing component, streams the processed data as an Output, and then passes it to the training component. The pipeline is submitted with MLClient, matching the stated goal."
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "In Azure ML SDK v2, pipelines are defined with the @pipeline decorator, inputs and outputs are handled via Input and Output types, and jobs are submitted through MLClient.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_8f2cb0c1983d4ba2ba5fe20776533fe7",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a new Azure Machine Learning pipeline using the designer.\n\nThe pipeline must train a model using data in a comma-separated values (CSV) file that is published on a website. You have not created a dataset for this file.\n\nYou need to ingest the data from the CSV file into the designer pipeline using the minimal administrative effort.\n\nWhich module should you add to the pipeline in Designer?",
        "options": [
            {
                "id": "A",
                "text": "Convert to CSV",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Enter Data Manually",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Import Data",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Dataset",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for C(100%)\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_94aed7f6e77642178a1409ad83dd87dd",
        "type": "multiple_choice_single_answer",
        "question": "You are planning to host practical training to acquaint learners with data visualization creation using Python. Learner devices are able to connect to the internet.\n\nLearner devices are currently NOT configured for Python development. Also, learners are unable to install software on their devices as they lack administrator permissions. Furthermore, they are unable to access Azure subscriptions.\n\nIt is imperative that learners are able to execute Python-based data visualization code.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should consider configuring the use of Azure Container Instance.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should consider configuring the use of Azure BatchAI.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should consider configuring the use of Azure Notebooks.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should consider configuring the use of Azure Kubernetes Service.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c8c953d757fe42fd8b4e42811f3eaf98",
        "type": "multiple_choice_single_answer",
        "question": "You develop and train a machine learning model to predict fraudulent transactions for a hotel booking website.\n\nTrafic to the site varies considerably. The site experiences heavy trafic on Monday and Friday and much lower trafic on other days. Holidays are also high web trafic days.\n\nYou need to deploy the model as an Azure Machine Learning real-time web service endpoint on compute that can dynamically scale up and down to support demand.\n\nWhich deployment compute option should you use?",
        "options": [
            {
                "id": "A",
                "text": "attached Azure Databricks cluster",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Container Instance (ACI)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Kubernetes Service (AKS) inference cluster",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Machine Learning Compute Instance",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "attached virtual machine in a different region",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for C\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d607d29c2d2e4acb9acf553db9ef81b2",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a binary classification by using a two-class logistic regression model.\n\nYou need to evaluate the model results for imbalance.\n\nWhich evaluation metric should you use?",
        "options": [
            {
                "id": "A",
                "text": "Relative Absolute Error",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "AUC Curve",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Mean Absolute Error",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Relative Squared Error",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Accuracy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "Root Mean Square Error",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_4b2c6df88fb54ea6b7a95ade8410b34b",
        "type": "multiple_choice_multiple_answer",
        "question": "You plan to use the Hyperdrive feature of Azure Machine Learning to determine the optimal hyperparameter values when training a model.\n\nYou must use Hyperdrive to try combinations of the following hyperparameter values. You must not apply an early termination policy.\n* learning_rate: any value between 0.001 and 0.1\n* batch_size: 16, 32, or 64\n\nYou need to configure the sampling method for the Hyperdrive experiment.\n\nWhich two sampling methods can you use? Each correct answer is a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "No sampling",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Grid sampling",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Bayesian sampling",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Random sampling",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_190b45eb6771437baed0c0aca521d1e3",
        "type": "multiple_choice_multiple_answer",
        "question": "You manage an Azure Machine Learning workspace.\n\nYou must log multiple metrics by using MLflow.\n\nYou need to maximize logging performance.\n\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "MlflowClient.log_batch",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "mlflow.log_metrics",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "mlflow.log_metric",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "mlflow.log_param",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_f632dd8ad54c4582a1bbb3c616190d4a",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train and register a machine learning model.\n\nYou plan to deploy the model as a real-time web service. Applications must use key-based authentication to use the model.\n\nYou need to deploy the web service.\n\nSolution:\n\nCreate an AciWebservice instance.\n\nSet the value of the auth_enabled property to False.\n\nSet the value of the token_auth_enabled property to True.\n\nDeploy the model to the service.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_8c3f86f8a62a49ea81d32a5913853f2a",
        "type": "multiple_choice_single_answer",
        "question": "You manage an Azure Machine Learning workspace. The workspace includes an Azure Machine Learning Kubernetes compute target configured as an Azure Kubernetes Service (AKS) cluster named AKS1. AKS1 is configured to enable the targeting of different nodes to train workloads.\n\nYou must run a command job on AKS1 by using the Azure ML Python SDK v2. The command job must select different types of compute nodes.\n\nThe compute node types must be specified by using a command parameter.\n\nYou need to configure the command parameter.\n\nWhich parameter should you use?",
        "options": [
            {
                "id": "A",
                "text": "environment",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "compute",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "limits",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "instance_type",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_affea83e1d994e36b5ce774e20ac8e10",
        "type": "multiple_choice_multiple_answer",
        "question": "You are analyzing a dataset by using Azure Machine Learning Studio.\n\nYou need to generate a statistical summary that contains the p-value and the unique count for each feature column.\n\nWhich two modules can you use? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Computer Linear Correlation",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Export Count Table",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Execute Python Script",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Convert to Indicator Values",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Summarize Data",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: BE.\nCommunity:\n100% for CE\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_92d25364208f4b2796f43cec72a88f7b",
        "type": "multiple_choice_single_answer",
        "question": "You use Azure Machine Learning designer to create a real-time service endpoint. You have a single Azure Machine Learning service compute resource.\n\nYou train the model and prepare the real-time pipeline for deployment.\n\nYou need to publish the inference pipeline as a web service.\n\nWhich compute type should you use?",
        "options": [
            {
                "id": "A",
                "text": "a new Machine Learning Compute resource",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Kubernetes Services",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "HDInsight",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "the existing Machine Learning Compute resource",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_8e4c0b3845c84c088a50f9638cf7c897",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nAn IT department creates the following Azure resource groups and resources:\n\n* an Azure Machine Leaming workspace named amlworkspace\n* an Azure Storage account named amlworkspace12345\n* an Application Insights instance named amlworkspace54321\n* an Azure Key Vault named amliworkspace67890\n* an Azure Container Registry named amlworkspace09876\n\nA virtual machine named mlivm with the following configuration:\n\n* general_compute: ?\n* Operating system: Ubuntu Linux\n* Software installed: Python 3.6 and Jupyter Notebooks\n\nThe IT department creates an Azure Kubernetes Service (AKS)-based inference compute target named aks-cluster in the Azure Machine Learning workspace.\n\nYou have a Microsoft Surface Book computer with a GPU. Python 3.6 and Visual Studio Code are installed.\n\nYou need to run a script that trains a deep neural network (DNN) model and logs the loss and accuracy metrics.\n\n**Solution**\n\n* Install the Azure ML SDK on the Surface Book. Run Python code to connect to the workspace.\n* Run the training script as an experiment on the aks-cluster compute target.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_02b1174591a045eda97564805e2207a2",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a new experiment in Azure Machine Learning Studio.\n\nOne class has a much smaller number of observations than the other classes in the training set.\n\nYou need to select an appropriate data sampling strategy to compensate for the class imbalance.\n\nSolution: You use the Stratified split for the sampling mode.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_372dfaa2e0044ad0b77f8fb46d116a55",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train a classification model by using a logistic regression algorithm.\n\nYou must be able to explain the model's predictions by calculating the importance of each feature, both as an overall global relative importance value and as a measure of local importance for a specific set of predictions.\n\nYou need to create an explainer that you can use to retrieve the required global and local feature importance values.\n\nSolution: Create a PFIExplainer.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c3c543631d9b4f709b1417514c9cb5b3",
        "type": "multiple_choice_single_answer",
        "question": "You have been tasked with designing a deep learning model, which accommodates the most recent edition of Python, to recognize language.\n\nYou have to include a suitable deep learning framework in the Data Science Virtual Machine (DSVM).\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should consider including Rattle.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should consider including TensorFlow.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should consider including Theano.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should consider including Chainer.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_9bd4968e81d04c6389303a4cce0cd7ec",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train and register a machine learning model.\n\nYou plan to deploy the model as a real-time web service. Applications must use key-based authentication to use the model.\n\nYou need to deploy the web service.\n\nSolution:\n\nCreate an AciWebservice instance.\n\nSet the value of the auth_enabled property to True.\n\nDeploy the model to the service.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a3d1015b3f8b495f959a0e45b1d10241",
        "type": "multiple_choice_single_answer",
        "question": "You want to train a classification model using data located in a comma-separated values (CSV) file.\n\nThe classification model will be trained via the Automated Machine Learning interface using the Classification task type.\n\nYou have been informed that only linear models need to be assessed by the Automated Machine Learning.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should disable deep learning.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should enable automatic featurization.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should disable automatic featurization.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should set the task type to Forecasting.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2f003113d30242bcb9f69385ff071c23",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou create a model to forecast weather conditions based on historical data.\n\nYou need to create a pipeline that runs a processing script to load data from a datastore and pass the processed data to a machine learning model training script.\n\n**Solution**\n\nRun the following code:\n\n```python\ndatastore = ws.get_default_datastore()\ndata_input = PipelineData(\"raw data\", datastore=rawdatastore)\ndata_output = PipelineData(\"processed data\", datastore=datastore)\nprocess step = PythonScriptStep(\n    script_name=\"process.py\",\n    arguments=[\"--data_for_train\", data_input],\n    outputs=[data_output],\n    compute_target='aml_compute',\n    source_directory=process directory\n)\ntrain_step = PythonScriptStep(\n    script_name=\"train.py\",\n    arguments=[\"--data_for_train\", data_input], \n    inputs=[data_output],\n    compute_target='aml_compute',\n    source _directory=train_directory\n)\npipeline = Pipeline(workspace=ws, steps=[process step, train_step])\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_b461ef03fb084c65a57a1765039d37e9",
        "type": "multiple_choice_single_answer",
        "question": "You make use of Azure Machine Learning Studio to develop a linear regression model. You perform an experiment to assess various algorithms.\n\nWhich of the following is an algorithm that reduces the variances between actual and predicted values?",
        "options": [
            {
                "id": "A",
                "text": "Fast Forest Quantile Regression",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Poisson Regression",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Boosted Decision Tree Regression",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Linear Regression",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n70% for D\n25% for C\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_98f58094b7e34b4a9e94ccc7fe46b069",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are using Azure Machine Learning to run an experiment that trains a classification model.\n\nYou want to use Hyperdrive to find parameters that optimize the AUC metric for the model. You configure a `HyperDriveConfig` for the experiment by running the following code:\n\n```python\nhyperdrive = HyperDriveConfig(\n    estimator=your_estimator,\n    hyperparameter_sampling=your_params,\n    policy=policy,\n    primary_metric_name='AUC',\n    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n    max_total_runs=6,\n    max_concurrent_runs=4\n)\n\n```\n\n\nYou plan to use this configuration to run a script that trains a random forest model and then tests it with validation data. The label values for the validation data are stored in a variable named `y_test` variable, and the predicted probabilities from the model are stored in a variable named `y_predicted`.\n\nYou need to add logging to the script to allow Hyperdrive to optimize hyperparameters for the AUC metric.\n\n**Proposed solution**\n\n Run the following code:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n# code to train model omitted\nauc = roc_auc_score(y_test, y predicted)\nprint(np.float(auc))\n\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_58419e79c86745c39cd6cdb7d7ccd221",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nAn IT department creates the following Azure resource groups and resources:\n\n* an Azure Machine Leaming workspace named amlworkspace\n* an Azure Storage account named amlworkspace12345\n* an Application Insights instance named amlworkspace54321\n* an Azure Key Vault named amliworkspace67890\n* an Azure Container Registry named amlworkspace09876\n\nA virtual machine named mlivm with the following configuration:\n\n* general_compute\n* Operating system: Ubuntu Linux\n* Software installed: Python 3.6 and Jupyter Notebooks\n\nThe IT department creates an Azure Kubernetes Service (AKS)-based inference compute target named aks-cluster in the Azure Machine Learning workspace.\n\nYou have a Microsoft Surface Book computer with a GPU. Python 3.6 and Visual Studio Code are installed.\n\nYou need to run a script that trains a deep neural network (DNN) model and logs the loss and accuracy metrics.\n\n**Solution:**\n\nAttach the mlvm virtual machine as a compute target in the Azure Machine Learning workspace. Install the Azure ML SDK on the Surface Book and run Python code to connect to the workspace. Run the training script as an experiment on the mlvm remote compute resource.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_9f97eba51365471f89b0f5c7a8dd0348",
        "type": "multiple_choice_single_answer",
        "question": "You register a file dataset named csv_folder that references a folder. The folder includes multiple comma-separated values (CSV) files in an Azure storage blob container.\n\nYou plan to use the following code to run a script that loads data from the file dataset. You create and instantiate the following variables:\n\nVariable Description remote_cluster References the Azure Machine Learning compute cluster\n\n| ws References the Azure Machine Learning workspace\n\n\n\nYou have the following code:\n\nfrom azureml.train.estimator import Estimator\nfile_dataset = ws.datasets.get('csv_folder')\nestimator = Estimator(source_directory=script_folder,\n\ncompute_target = remote_cluster,\nentry_script ='script.py')\n\nrun = experiment.submit(config=estimator)\nrun.wait_for_completion(show_output=True)\n\n\nYou need to pass the dataset to ensure that the script can read the files it references.\n\nWhich code segment should you insert to replace the code comment?",
        "options": [
            {
                "id": "A",
                "text": "inputs=[file_dataset.as_named_input('training_files')],",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "inputs=[file_dataset.as_named_input('training_files').as_mount()],",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "inputs=[file_dataset.as_named_input('training_files').to_pandas_dataframe()],",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "script_params={'--training_files': file_dataset},",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_511aca8c2a2c4c0aae16f1dd7b6ce7c7",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. modelfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are using Azure Machine Learning to run an experiment that trains a classification model.\n\nYou want to use Hyperdrive to find parameters that optimize the AUC metric for the model. You configure a HyperDriveConfig for the experiment by running the following code:\n\n```python\nhyperdrive = HyperDriveConfig(\n    estimator=your_estimator,\n    hyperparameter_sampling=your_params,\n    policy=policy,\n    primary_metric_name='AUC',\n    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n    max_total_runs=6,\n    max_concurrent_runs=4\n)\n```\n\nYou plan to use this configuration to run a script that trains a random forest model and then tests it with validation data. The label values for the validation data are stored in a variable named `y_test` variable, and the predicted probabilities from the model are stored in a variable named `y_predicted`.\n\nYou need to add logging to the script to allow Hyperdrive to optimize hyperparameters for the AUC metric.\n\n**Proposed solution**\n\n Run the following code:\n\n```python\nfrom sklearn.metrics import roc_auc_score\nimport logging\n\n#code to train model omitted\nauc = roc_auc_score(y_test, y predicted)\nlogging.info(\"AUC: \" + str(auc))\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_23323466f16740eda1f6dfbd43c41721",
        "type": "multiple_choice_single_answer",
        "question": "You make use of Azure Machine Learning Studio to create a binary classification model.\n\nYou are preparing to carry out a parameter sweep of the model to tune hyperparameters. You have to make sure that the sweep allows for every possible combination of hyperparameters to be iterated. Also, the computing resources needed to carry out the sweep must be reduced.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should consider making use of the Selective grid sweep mode.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should consider making use of the Measured grid sweep mode.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should consider making use of the Entire grid sweep mode.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should consider making use of the Random grid sweep mode.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_bca8e25f08fc450d8c5e82dab13995f6",
        "type": "multiple_choice_single_answer",
        "question": "You are developing deep learning models to analyze semi-structured, unstructured, and structured data types.\n\nYou have the following data available for model building\n\n* Video recordings of sporting events\n* Transcripts of radio commentary about events\n* Logs from related social media feeds captured during sporting events\n\nYou need to select an environment for creating the model.\n\nWhich environment should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Cognitive Services",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Data Lake Analytics",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure HDInsight with Spark MLib",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Machine Learning Studio",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_91758c189e014ef3bd3e07e3203f8f73",
        "type": "multiple_choice_single_answer",
        "question": "You register a model that you plan to use in a batch inference pipeline.\n\nThe batch inference pipeline must use a ParallelRunStep step to process files in a file dataset. The script has the ParallelRunStep step runs must process six input files each time the inferencing function is called.\n\nYou need to configure the pipeline.\n\nWhich configuration setting should you specify in the ParallelRunConfig object for the PrallelRunStep step?",
        "options": [
            {
                "id": "A",
                "text": "process_count_per_node= \"6\"",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "node_count= \"6\"",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "mini_batch_size= \"6\"",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "error_threshold= \"6\"",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for C\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a5d3bcd32ce94565b2cb7d961ca9db81",
        "type": "multiple_choice_multiple_answer",
        "question": "You must store data in Azure Blob Storage to support Azure Machine Learning.\n\nYou need to transfer the data into Azure Blob Storage.\n\nWhat are three possible ways to achieve the goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Bulk Insert SQL Query",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "AzCopy",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Python script",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Storage Explorer",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Bulk Copy Program (BCP)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_60e06d92e00a4afb8c2cc22b2a9d5c29",
        "type": "multiple_choice_single_answer",
        "question": "You use the Azure Machine Learning designer to create and run a training pipeline. You then create a real-time inference pipeline.\n\nYou must deploy the real-time inference pipeline as a web service.\n\nWhat must you do before you deploy the real-time inference pipeline?",
        "options": [
            {
                "id": "A",
                "text": "Run the real-time inference pipeline.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a batch inference pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Clone the training pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create an Azure Machine Learning compute cluster.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n59% for A\n41% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_9006366d144746679d453b99541c6215",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of carrying out feature engineering on a dataset.\n\nYou want to add a feature to the dataset and fill the column value.\n\nRecommendation: You must make use of the Group Categorical Values Azure Machine Learning Studio module.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6455abb427664db5b88e34212b46120f",
        "type": "multiple_choice_multiple_answer",
        "question": "You train and register a model in your Azure Machine Learning workspace.\n\nYou must publish a pipeline that enables client applications to use the model for batch inferencing. You must use a pipeline with a single\n\nParallelRunStep step that runs a Python inferencing script to get predictions from the input data.\n\nYou need to create the inferencing script for the ParallelRunStep pipeline step.\n\nWhich two functions should you include? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "run(mini_batch)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "main()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "batch()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "init()",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "score(mini_batch)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_fcdb864e92ec4421a4861878ddec009e",
        "type": "multiple_choice_single_answer",
        "question": "You have a Python script named train.py in a local folder named scripts. The script trains a regression model by using scikit-learn. The script includes code to load a training data file which is also located in the scripts folder.\n\nYou must run the script as an Azure ML experiment on a compute cluster named aml-compute.\n\nYou need to configure the run to ensure that the environment includes the required packages for model training. You have instantiated a variable named aml-compute that references the target compute cluster.\n\n**Solution:**\n\n Run the following code:\n\n```python\nfrom azureml.train.sklearn\nimport SKLearn\n\nsk_est = SKLearn(\n    source_directory='./scripts',\n    compute_target='aml-compute',\n    entry_script='train.py')\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_88cb255b3d264144973dfc8a088793e3",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist creating a linear regression model.\n\nYou need to determine how closely the data fits the regression line.\n\nWhich metric should you review?",
        "options": [
            {
                "id": "A",
                "text": "Root Mean Square Error",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Coeficient of determination",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Recall",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Precision",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Mean absolute error",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c21d257e0d774fdcacaf8007e1650c92",
        "type": "multiple_choice_single_answer",
        "question": "Case study \nOverview \nYou are a data scientist in a company that provides data science for professional sporting events. Models will use global and local market data to meet the following business goals:\n\nUnderstand sentiment of mobile device users at sporting events based on audio from crowd reactions.\n\nAssess a user's tendency to respond to an advertisement.\n\nCustomize styles of ads served on mobile devices.\n\nUse video to detect penalty events\n\nCurrent environment \nMedia used for penalty event detection will be provided by consumer devices. Media may include images and videos captured during the sporting event and shared using social media. The images and videos will have varying sizes and formats.\n\nThe data available for model building comprises of seven years of sporting event media. The sporting event media includes; recorded video transcripts or radio commentary, and logs from related social media feeds captured during the sporting events.\n\nCrowd sentiment will include audio recordings submitted by event attendees in both mono and stereo formats.\n\nPenalty detection and sentiment \nData scientists must build an intelligent solution by using multiple machine learning models for penalty event detection.\n\nData scientists must build notebooks in a local environment using automatic feature engineering and model building in machine learning pipelines.\n\nNotebooks must be deployed to retrain by using Spark instances with dynamic worker allocation.\n\nNotebooks must execute with the same code on new Spark instances to recode only the source of the data.\n\nGlobal penalty detection models must be trained by using dynamic runtime graph computation during training.\n\nLocal penalty detection models must be written by using BrainScript.\n\nExperiments for local crowd sentiment models must combine local penalty detection data.\n\nCrowd sentiment models must identify known sounds such as cheers and known catch phrases. Individual crowd sentiment models will detect similar sounds.\n\nAll shared features for local models are continuous variables.\n\nShared features must use double precision. Subsequent layers must have aggregate running mean and standard deviation metrics available.\n\nAdvertisements \nDuring the initial weeks in production, the following was observed:\n\nAd response rated declined.\n\nDrops were not consistent across ad styles.\n\nThe distribution of features across training and production data are not consistent\n\nAnalysis shows that, of the 100 numeric features on user location and behavior, the 47 features that come from location sources are being used as raw features. A suggested experiment to remedy the bias and variance issue is to engineer 10 linearly uncorrelated features.\n\nInitial data discovery shows a wide range of densities of target states in training data used for crowd sentiment models.\n\nAll penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.\n\nAudio samples show that the length of a catch phrase varies between 25%-47% depending on region\n\nThe performance of the global penalty detection models shows lower variance but higher bias when comparing training and validation sets.\n\nBefore implementing any feature changes, you must confirm the bias and variance using all training and validation cases.\n\nAd response models must be trained at the beginning of each event and applied during the sporting event.\n\nMarket segmentation models must optimize for similar ad response history.\n\nSampling must guarantee mutual and collective exclusively between local and global segmentation models that share the same features.\n\nLocal market segmentation models will be applied before determining a user's propensity to respond to an advertisement.\n\nAd response models must support non-linear boundaries of features.\n\nThe ad propensity model uses a cut threshold is 0.45 and retrains occur if weighted Kappa deviated from 0.1 +/- 5%.\n\nThe ad propensity model uses cost factors shown in the following diagram:\n\nActual\n\nPa}IPald\n\n\n\nThe ad propensity model uses proposed cost factors shown in the following diagram:\n\nActual\n\nPa}1Pald\n\n\n\nPerformance curves of current and proposed cost factor scenarios are shown in the following diagram:\n\nScenario\n\u00ab Scenario1\n\nScenario2\n\n\u00a2Scenario3\n\n\nYou need to implement a scaling strategy for the local penalty detection data.\n\nWhich normalization type should you use?",
        "options": [
            {
                "id": "A",
                "text": "Streaming",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Weight",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Batch",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Cosine",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_739887cd6eeb410fabb37b8f795dd971",
        "type": "multiple_choice_single_answer",
        "question": "You have a dataset that contains salary information for users. You plan to generate an aggregate salary report that shows average salaries by city.\n\nPrivacy of individuals must be preserved without impacting accuracy, completeness, or reliability of the data. The aggregation must be statistically consistent with the distribution of the original data. You must return an approximation of the data instead of the raw data.\n\nYou need to apply a differential privacy approach.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Add noise to the salary data during the analysis",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Encrypt the salary data before analysis",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Remove the salary data",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Convert the salary data to the average column value",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2062c63a92d5410bb695c9c63d92b3dd",
        "type": "multiple_choice_single_answer",
        "question": "You are a lead data scientist for a project that tracks the health and migration of birds. You create a multi-class image classification deep learning model that uses a set of labeled bird photographs collected by experts.\n\nYou have 100,000 photographs of birds. All photographs use the JPG format and are stored in an Azure blob container in an Azure subscription.\n\nYou need to access the bird photograph files in the Azure blob container from the Azure Machine Learning service workspace that will be used for deep learning model training. You must minimize data movement.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create an Azure Data Lake store and move the bird photographs to the store.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_63df506ce42f468d8d0c064afdf2a205",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are a data scientist using Azure Machine Learning Studio.\n\nYou need to normalize values to produce an output column into bins to predict a target column.\n\nSolution: Apply an Equal Width with Custom Start and Stop binning mode.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d7aefd57df21451b9082c7ba168ba96e",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a new experiment in Azure Machine Learning Studio. You have a small dataset that has missing values in many columns. The data does not require the application of predictors for each column. You plan to use the Clean Missing Data.\n\nYou need to select a data cleaning method.\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "Replace using Probabilistic PCA",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Normalization",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Synthetic Minority Oversampling Technique (SMOTE)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Replace using MICE",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6a2c6f049a1d4af8a3af06f1e7718c16",
        "type": "multiple_choice_single_answer",
        "question": "You are performing a filter-based feature selection for a dataset to build a multi-class classifier by using Azure Machine Learning Studio.\n\nThe dataset contains categorical features that are highly correlated to the output label column.\n\nYou need to select the appropriate feature scoring statistical method to identify the key predictors.\n\nWhich method should you use?",
        "options": [
            {
                "id": "A",
                "text": "Kendall correlation",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Spearman correlation",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Chi-squared",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Pearson correlation",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for C (100%)\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3aa523b109624994b87217ec1cda6219",
        "type": "multiple_choice_multiple_answer",
        "question": "You build a data pipeline in an Azure Machine Learning workspace by using the Azure Machine Learning SDK for Python.\n\nYou need to run a Python script as a pipeline step.\n\nWhich two classes could you use? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "PythonScriptStep",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "AutoMLStep",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "CommandStep",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "StepRun",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: CD.\nCommunity:\n100% for AC\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ccc437c1c3e54375a94c1d88b9aa4e11",
        "type": "multiple_choice_single_answer",
        "question": "You plan to create a compute instance as part of an Azure Machine Learning development workspace.\n\nYou must interactively debug code running on the compute instance by using Visual Studio Code Remote.\n\nYou need to provision the compute instance.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Enable Remote Desktop Protocol (RDP) access.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Modify role-based access control (RBAC) settings at the workspace level.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Enable Secure Shell Protocol (SSH) access.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Modify role-based access control (RBAC) settings at the compute instance level.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_bea58cf7aba64dafa571f6ea348c6217",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist working for a bank and have used Azure ML to train and register a machine learning model that predicts whether a customer is likely to repay a loan.\n\nYou want to understand how your model is making selections and must be sure that the model does not violate government regulations such as denying loans based on where an applicant lives.\n\nYou need to determine the extent to which each feature in the customer data is influencing predictions.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Enable data drift monitoring for the model and its training dataset.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Score the model against some test data with known label values and use the results to calculate a confusion matrix.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use the Hyperdrive library to test the model with multiple hyperparameter values.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Use the interpretability package to generate an explainer for the model.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Add tags to the model registration indicating the names of the features in the training dataset.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_07f079bfa64748a5a3b5f246a9345a3c",
        "type": "multiple_choice_single_answer",
        "question": "You have a dataset that is stored in an Azure Machine Learning workspace.\n\nYou must perform a data analysis for differential privacy by using the SmartNoise SDK.\n\nYou need to measure the distribution of reports for repeated queries to ensure that they are balanced.\n\nWhich type of test should you perform?",
        "options": [
            {
                "id": "A",
                "text": "Bias",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Privacy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Accuracy",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Utility",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n50% for D\n43% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_797b380ea8df4a1a97c0441abe0b3098",
        "type": "multiple_choice_multiple_answer",
        "question": "You plan to provision an Azure Machine Learning Basic edition workspace for a data science project.\n\nYou need to identify the tasks you will be able to perform in the workspace.\n\nWhich three tasks will you be able to perform? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Create a Compute Instance and use it to run code in Jupyter notebooks.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create an Azure Kubernetes Service (AKS) inference cluster.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use the designer to train a model by dragging and dropping pre-defined modules.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a tabular dataset that supports versioning.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Use the Automated Machine Learning user interface to train a model.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: ABD.\nCommunity:\n100% for ACE\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c602b0ec0f24452d8cdda175a97a9e29",
        "type": "multiple_choice_multiple_answer",
        "question": "You create a multi-class image classification deep learning model that uses the PyTorch deep learning framework.\n\nYou must configure Azure Machine Learning Hyperdrive to optimize the hyperparameters for the classification model.\n\nYou need to define a primary metric to determine the hyperparameter values that result in the model with the best accuracy score.\n\nWhich three actions must you perform? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Set the primary_metric_goal of the estimator used to run the bird_classifier_train.py script to maximize.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Add code to the bird_classifier_train.py script to calculate the validation loss of the model and log it as a float value with the key loss.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Set the primary_metric_goal of the estimator used to run the bird_classifier_train.py script to minimize.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Set the primary_metric_name of the estimator used to run the bird_classifier_train.py script to accuracy.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Set the primary_metric_name of the estimator used to run the bird_classifier_train.py script to loss.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "Add code to the bird_classifier_train.py script to calculate the validation accuracy of the model and log it as a float value with the key accuracy.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_9d1e2f4c39484cd48f3417745a625cf1",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are using Azure Machine Learning to run an experiment that trains a classification model.\n\nYou want to use Hyperdrive to find parameters that optimize the AUC metric for the model. You configure a `HyperDriveConfig` for the experiment by running the following code:\n\n```python\nhyperdrive = HyperDriveConfig(\n    estimator=your_estimator,\n    hyperparameter_sampling=your_params,\n    policy=policy,\n    primary_metric_name='AUC',\n    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n    max_total_runs=6,\n    max_concurrent_runs=4\n)\n\n```\n\n\nYou plan to use this configuration to run a script that trains a random forest model and then tests it with validation data. The label values for the validation data are stored in a variable named `y_test` variable, and the predicted probabilities from the model are stored in a variable named `y_predicted`.\n\nYou need to add logging to the script to allow Hyperdrive to optimize hyperparameters for the AUC metric.\n\n**Proposed solution**\n\n Run the following code:\n\n```python\nimport json\nimport os\nfrom sklearn.metrics import roc_auc_score\n# code to train model omitted\nauc = roc_auc_score(y_ test, y_ predicted)\nos.makedirs(\"outputs\", exist_ok=True)\nwith open(\"outputs/AUC.txt\", \"w\") as fp:\n    fp.write(auc)\n\n```\n\nDoes the solution meet the goal",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6bc92c75fa68466086ab81b8e79fbcc6",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of creating a machine learning model. Your dataset includes rows with null and missing values.\n\nYou plan to make use of the Clean Missing Data module in Azure Machine Learning Studio to detect and fix the null and missing values in the dataset.\n\nRecommendation: You make use of the Remove entire row option.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_f3275d9d02b041028f269aa5270fc1d7",
        "type": "multiple_choice_single_answer",
        "question": "You train and register a machine learning model. You create a batch inference pipeline that uses the model to generate predictions from multiple data files.\n\nYou must publish the batch inference pipeline as a service that can be scheduled to run every night.\n\nYou need to select an appropriate compute target for the inference service.\n\nWhich compute target should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Machine Learning compute instance",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Machine Learning compute cluster",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Kubernetes Service (AKS)-based inference cluster",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Container Instance (ACI) compute target",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_1aa9fcb1714c4f01a3899de4d38b3d77",
        "type": "multiple_choice_single_answer",
        "question": "You are solving a classification task.\n\nYou must evaluate your model on a limited data sample by using k-fold cross-validation. You start by configuring a k parameter as the number of splits.\n\nYou need to configure the k parameter for the cross-validation.\n\nWhich value should you use?",
        "options": [
            {
                "id": "A",
                "text": "k=0.5",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "k=0.01",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "k=5",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "k=1",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_49a31b55c7b14833828cf5ffe5e1b6ad",
        "type": "multiple_choice_single_answer",
        "question": "You are implementing hyperparameter tuning for a model training from a notebook. The notebook is in an Azure Machine Learning workspace.\n\nYou must configure a grid sampling method over the search space for the num_hidden_layers and batch_size hyperparameters.\n\nYou need to identify the hyperparameters for the grid sampling.\n\nWhich hyperparameter sampling approach should you use?",
        "options": [
            {
                "id": "A",
                "text": "uniform",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "qlognormal",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "choice",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "normal",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for C\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6ef8f8e895f44e74b2e312a222f0c14a",
        "type": "multiple_choice_single_answer",
        "question": "You create a binary classification model by using Azure Machine Learning Studio.\n\nYou must tune hyperparameters by performing a parameter sweep of the model. The parameter sweep must meet the following requirements\n\n* iterate all possible combinations of hyperparameters\n* minimize computing resources required to perform the sweep\n\nYou need to perform a parameter sweep of the model.\n\nWhich parameter sweep mode should you use?",
        "options": [
            {
                "id": "A",
                "text": "Random sweep",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Sweep clustering",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Entire grid",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Random grid",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ec5b56f7b75545fbbff641220d40bcbd",
        "type": "multiple_choice_single_answer",
        "question": "You plan to build a team data science environment. Data for training models in machine learning pipelines will be over 20 GB in size.\n\nYou have the following requirements\n\n* Models must be built using Caffe2 or Chainer frameworks.\n* Data scientists must be able to use a data science environment to build the machine learning pipelines and train models on their personal devices in both connected and disconnected network environments.\n\nPersonal devices must support updating machine learning pipelines when connected to a network.\n\nYou need to select a data science environment.\n\nWhich environment should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Machine Learning Service",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Machine Learning Studio",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Kubernetes Service (AKS)",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_cf271f1f19f54ed9a5c192df086f25c6",
        "type": "multiple_choice_single_answer",
        "question": "You plan to use automated machine learning to train a regression model. You have data that has features which have missing values, and categorical features with few distinct values.\n\nYou need to configure automated machine learning to automatically impute missing values and encode categorical features as part of the training task.\n\nWhich parameter and value pair should you use in the AutoMLConfig class?",
        "options": [
            {
                "id": "A",
                "text": "featurization = 'auto'",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "enable_voting_ensemble = True",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "task = 'classification'",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "exclude_nan_labels = True",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "enable_tf = True",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_d47e9d74efb54f52b7946290d3f07adf",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou have been tasked with employing a machine learning model, which makes use of a PostgreSQL database and needs GPU processing, to forecast prices.\n\nYou are preparing to create a virtual machine that has the necessary tools built into it.\n\nYou need to make use of the correct virtual machine type.\n\nRecommendation: You make use of a Deep Learning Virtual Machine (DLVM) Windows edition.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n88% for A\n\nPersonal notes: VM Windows edition do not support PostgreSQL.\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/tools-included?view=azureml-api-2",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_f364cf8c51e543019ec4a9eecde5889a",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of creating a machine learning model. Your dataset includes rows with null and missing values.\n\nYou plan to make use of the Clean Missing Data module in Azure Machine Learning Studio to detect and fix the null and missing values in the dataset.\n\nRecommendation: You make use of the Replace with median option.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n74% for A\n26% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_0d6b04020df443f59d54bd497ecd95d8",
        "type": "multiple_choice_single_answer",
        "question": "You use Azure Machine Learning Studio to build a machine learning experiment.\n\nYou need to divide data into two distinct datasets.\n\nWhich module should you use?",
        "options": [
            {
                "id": "A",
                "text": "Split Data",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Load Trained Model",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Assign Data to Clusters",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Group Data into Bins",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n81% for A\n19% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_05f39a0b4cd0438eb5798f355c90d0dd",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are analyzing a numerical dataset which contains missing values in several columns.\n\nYou must clean the missing values using an appropriate operation without affecting the dimensionality of the feature set.\n\nYou need to analyze a full dataset to include all values.\n\nSolution: Calculate the column median value and use the median value as the replacement for any missing value in the column.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n90% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a4780749d14542d3b051a117795c8bad",
        "type": "multiple_choice_single_answer",
        "question": "You have the following code. The code prepares an experiment to run a script:\n\n```python\nfrom azureml.core import Workspace, Experiment, Run, ScriptRunConfig\n\nws = Workspace.from_config()\nscript_config = ScriptRunConfig(source_directory='experiment_files', script='experiment.py')\nscript_experiment = Experiment(workspace=ws, name='script-experiment')\n```\n\nThe experiment must be run on local computer using the default environment.\n\nYou need to add code to start the experiment and run the script.\n\nWhich code segment should you use?",
        "options": [
            {
                "id": "A",
                "text": "run = script_experiment.start_logging()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "run = Run(experiment=script_experiment)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "ws.get_run(run_id=experiment.id)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "run = script_experiment.submit(config=script_config)",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_54720b4bb03a4ecbab3d34c3b7ea5ed8",
        "type": "multiple_choice_single_answer",
        "question": "You are solving a classification task.\n\nThe dataset is imbalanced.\n\nYou need to select an Azure Machine Learning Studio module to improve the classification accuracy.\n\nWhich module should you use?",
        "options": [
            {
                "id": "A",
                "text": "Permutation Feature Importance",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Filter Based Feature Selection",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Fisher Linear Discriminant Analysis",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Synthetic Minority Oversampling Technique (SMOTE)",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_64be73d0e5d44e50b814636dbaf5a0a3",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train a classification model by using a logistic regression algorithm.\n\nYou must be able to explain the model's predictions by calculating the importance of each feature, both as an overall global relative importance value and as a measure of local importance for a specific set of predictions.\n\nYou need to create an explainer that you can use to retrieve the required global and local feature importance values.\n\nSolution: Create a MimicExplainer.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_be61351c1a444325863d09701f387adb",
        "type": "multiple_choice_multiple_answer",
        "question": "You have been tasked with ascertaining if two sets of data differ considerably. You will make use of Azure Machine Learning Studio to complete your task.\n\nYou plan to perform a paired t-test.\n\nWhich of the following are conditions that must apply to use a paired t-test? (Choose all that apply.)",
        "options": [
            {
                "id": "A",
                "text": "All scores are independent from each other.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You have a matched pairs of scores.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "The sampling distribution of d is normal.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "The sampling distribution of x1- x2 is normal.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_cbe0129746b346a6846d0daaa319a260",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou train and register a machine learning model.\n\nYou plan to deploy the model as a real-time web service. Applications must use key-based authentication to use the model.\n\nYou need to deploy the web service.\n\nSolution:\n\nCreate an AciWebservice instance.\n\nSet the value of the ssl_enabled property to True.\n\nDeploy the model to the service.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_5c836eadbe734f3f94e871b9be7a1fca",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nAn IT department creates the following Azure resource groups and resources:\n\n* an Azure Machine Leaming workspace named amlworkspace\n* an Azure Storage account named amlworkspace12345\n* an Application Insights instance named amlworkspace54321\n* an Azure Key Vault named amliworkspace67890\n* an Azure Container Registry named amlworkspace09876\n\nA virtual machine named mlivm with the following configuration:\n\n* general_compute\n* Operating system: Ubuntu Linux\n* Software installed: Python 3.6 and Jupyter Notebooks\n\nThe IT department creates an Azure Kubernetes Service (AKS)-based inference compute target named aks-cluster in the Azure Machine Learning workspace.\n\nYou have a Microsoft Surface Book computer with a GPU. Python 3.6 and Visual Studio Code are installed.\n\nYou need to run a script that trains a deep neural network (DNN) model and logs the loss and accuracy metrics.\n\n**Solution**\n\n* Install the Azure ML SDK on the Surface Book.\n* Run Python code to connect to the workspace and then run the training script as an experiment on local compute.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2089276a49de4739aca247e18b4c3444",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are a data scientist using Azure Machine Learning Studio.\n\nYou need to normalize values to produce an output column into bins to predict a target column.\n\nSolution: Apply a Quantiles normalization with a QuantileIndex normalization.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n67% for A\n33% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_92d188dc26094c55ac3ae58fe4984d76",
        "type": "multiple_choice_single_answer",
        "question": "You are using Azure Machine Learning Studio to perform feature engineering on a dataset.\n\nYou need to normalize values to produce a feature column grouped into bins.\n\nYou apply an Entropy Minimum Description Length (MDL) binning mode.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_5bc25dcdb3184995b4eb088bd2244844",
        "type": "multiple_choice_single_answer",
        "question": "You are with a time series dataset in Azure Machine Learning Studio.\n\nYou need to split your dataset into training and testing subsets by using the Split Data module.\n\nWhich splitting mode should you use?",
        "options": [
            {
                "id": "A",
                "text": "Recommender Split",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Regular Expression Split",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Relative Expression Split",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Split Rows with the Randomized split parameter set to true",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n83% for C\n17% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_1ece7b80f89c4e5da7b0599247b50e4a",
        "type": "multiple_choice_single_answer",
        "question": "You are developing a data science workspace that uses an Azure Machine Learning service.\n\nYou need to select a compute target to deploy the workspace.\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure Data Lake Analytics",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure Databricks",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Container Service",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Apache Spark for HDInsight",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_538668590d6f45d7aab5168b4ee1ef28",
        "type": "multiple_choice_single_answer",
        "question": "You construct a machine learning experiment via Azure Machine Learning Studio.\n\nYou would like to split data into two separate datasets.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should make use of the Split Data module.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should make use of the Group Categorical Values module.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should make use of the Clip Values module.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should make use of the Group Data into Bins module.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_91cbb8a0bab14755a6f320e2fcf83a8f",
        "type": "multiple_choice_single_answer",
        "question": "A set of CSV files contains sales records. All the CSV files have the same data schema.\n\nEach CSV file contains the sales record for a particular month and has the filename sales.csv. Each file is stored in a folder that indicates the month and year when the data was recorded. The folders are in an Azure blob container for which a datastore has been defined in an Azure\n\nMachine Learning workspace. The folders are organized in a parent folder named sales to create the following hierarchical structure:\n\n/sales\n/01-2019\n/sales.csv\n/02-2019\n/sales.csv\n/03-2019\n/sales.csv\n\n\nAt the end of each month, a new folder with that month's sales file is added to the sales folder.\n\nYou plan to use the sales data to train a machine learning model based on the following requirements\n\n* You must define a dataset that loads all of the sales data to date into a structure that can be easily converted to a dataframe.\n* You must be able to create experiments that use only data that was created before a specific previous month, ignoring any data that was added\n\nafter that month.\n* You must register the minimum number of datasets possible.\n\nYou need to register the sales data as a dataset in Azure Machine Learning service workspace.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file every month. Register the dataset with the name sales_dataset each month, replacing the existing dataset and specifying a tag named month indicating the month and year it was registered. Use this dataset for all experiments.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a tabular dataset that references the datastore and specifies the path 'sales/*/sales.csv', register the dataset with the name sales_dataset and a tag named month indicating the month and year it was registered, and use this dataset for all experiments.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create a new tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file every month.\n\nRegister the dataset with the name sales_dataset_MM-YYYY each month with appropriate MM and YYYY values for the month and year. Use the appropriate month-specific dataset for experiments.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file. Register the dataset with the name sales_dataset each month as a new version and with a tag named month indicating the month and year it was registered. Use this dataset for all experiments, identifying the version to be used based on the month tag as necessary.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_631594bcf1e54f13a057e84da4d87ffa",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou use Azure Machine Learning designer to load the following datasets into an experiment:\n\nDataset1 \n\n\nDataset2 \n\n\nYou need to create a dataset that has the same columns and header row as the input datasets and contains all rows from both input datasets.\n\nSolution: Use the Apply Transformation module.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3a6a31a385684a1788e8b984a85ceb37",
        "type": "multiple_choice_multiple_answer",
        "question": "You use the Two-Class Neural Network module in Azure Machine Learning Studio to build a binary classification model. You use the Tune Model\n\nHyperparameters module to tune accuracy for the model.\n\nYou need to configure the Tune Model Hyperparameters module.\n\nWhich two values should you use? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Number of hidden nodes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Learning Rate",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "The type of the normalizer",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Number of learning iterations",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Hidden layer specification",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: DE.\nCommunity:\n80% for BD\n20% for DE\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_09d5bc2621724967b60c968e755e721c",
        "type": "multiple_choice_multiple_answer",
        "question": "You create an Azure Machine Learning compute resource to train models. The compute resource is configured as follows\n\n* Minimum nodes: 2\n* Maximum nodes: 4\n\nYou must decrease the minimum number of nodes and increase the maximum number of nodes to the following values\n\n* Minimum nodes: 0\n* Maximum nodes: 8\n\nYou need to reconfigure the compute resource.\n\nWhat are three possible ways to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Use the Azure Machine Learning studio.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Run the update method of the AmlCompute class in the Python SDK.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use the Azure portal.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Use the Azure Machine Learning designer.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Run the refresh_state() method of the BatchCompute class in the Python SDK.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_7a014f4c87064ed1bf1acb4e422117a3",
        "type": "multiple_choice_single_answer",
        "question": "You are authoring a notebook in Azure Machine Learning studio.\n\nYou must install packages from the notebook into the currently running kernel. The installation must be limited to the currently running kernel only.\n\nYou need to install the packages.\n\nWhich magic function should you use?",
        "options": [
            {
                "id": "A",
                "text": "!pip",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "%pip",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "!conda",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "%load",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_5bae511856a84d258504528ad5073c20",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are analyzing a numerical dataset which contains missing values in several columns.\n\nYou must clean the missing values using an appropriate operation without affecting the dimensionality of the feature set.\n\nYou need to analyze a full dataset to include all values.\n\nSolution: Use the Last Observation Carried Forward (LOCF) method to impute the missing data points.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_148e11b92cef4ce8908c1c074898a2ad",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are a data scientist using Azure Machine Learning Studio.\n\nYou need to normalize values to produce an output column into bins to predict a target column.\n\nSolution: Apply a Quantiles binning mode with a PQuantile normalization.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n88% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_81092807b6e24b6fb23245ed1d11fd67",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou have been tasked with evaluating your model on a partial data sample via k-fold cross-validation.\n\nYou have already configured a k parameter as the number of splits. You now have to configure the k parameter for the cross-validation with the usual value choice.\n\nRecommendation: You configure the use of the value k=10.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_55d677eed23b47b8b4cee311b0d2ac48",
        "type": "multiple_choice_multiple_answer",
        "question": "You develop a machine learning project on a local machine. The project uses the Azure Machine Learning SDK for Python. You use Git as version control for scripts.\n\nYou submit a training run that returns a Run object.\n\nYou need to retrieve the active Git branch for the training run.\n\nWhich two code segments should you use? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "details = run.get_environment()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "details.properties['azureml.git.branch']",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "details.properties['azureml.git.commit']",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "details = run.get_details()",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: BC.\nCommunity:\n100% for BD\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3f06fd2e67594220a757201c9e79a3f2",
        "type": "multiple_choice_single_answer",
        "question": "You are creating a machine learning model. You have a dataset that contains null rows.\n\nYou need to use the Clean Missing Data module in Azure Machine Learning Studio to identify and resolve the null and missing data in the dataset.\n\nWhich parameter should you use?",
        "options": [
            {
                "id": "A",
                "text": "Replace with mean",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Remove entire column",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Remove entire row",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Hot Deck",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Custom substitution value",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "Replace with mode",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_bc612ccec5be4235901f018e817b39ef",
        "type": "multiple_choice_single_answer",
        "question": "You have an Azure Machine Learning workspace. You are connecting an Azure Data Lake Storage Gen2 account to the workspace as a data store.\n\nYou need to authorize access from the workspace to the Azure Data Lake Storage Gen2 account.\n\nWhat should you use?",
        "options": [
            {
                "id": "A",
                "text": "Service principal",
                "is_correct": false,
                "explanation": "Works, but you need to manage credentials (client secret or certificate), which increases the risk and management overhead."
            },
            {
                "id": "B",
                "text": "SAS token",
                "is_correct": false,
                "explanation": "Grants temporary access, but it's hard to rotate and audit at scale."
            },
            {
                "id": "C",
                "text": "Managed identity",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Account key",
                "is_correct": false,
                "explanation": "Grants full access to the storage account, which is not secure or recommended."
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n100% for A\n\n**Best practice**: Use Managed Identity to authorize Azure ML to access linked services like ADLS Gen2.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_950ee73198a94f20afaecc34e7bb6689",
        "type": "multiple_choice_multiple_answer",
        "question": "You are a data scientist building a deep convolutional neural network (CNN) for image classification.\n\nThe CNN model you build shows signs of overfitting.\n\nYou need to reduce overfitting and converge the model to an optimal fit.\n\nWhich two actions should you perform? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Add an additional dense layer with 512 input units.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Add L1/L2 regularization.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use training data augmentation.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Reduce the amount of training data.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Add an additional dense layer with 64 input units.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: BD.\nCommunity:\n100% for BC\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_247f0e4af5d343b8b54ef4844d5e94d8",
        "type": "multiple_choice_single_answer",
        "question": "You create a multi-class image classification deep learning model.\n\nYou train the model by using PyTorch version 1.2.\n\nYou need to ensure that the correct version of PyTorch can be identified for the inferencing environment when the model is deployed.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Save the model locally as a.pt file, and deploy the model as a local web service.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Deploy the model on computer that is configured to use the default Azure Machine Learning conda environment.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Register the model with a .pt file extension and the default version property.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Register the model, specifying the model_framework and model_framework_version properties.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ce5027f1ad1f4ef1bbd3fb975e2d9f19",
        "type": "multiple_choice_multiple_answer",
        "question": "You are training machine learning models in Azure Machine Learning. You use Hyperdrive to tune the hyperparameters.\n\nIn previous model training and tuning runs, many models showed similar performance.\n\nYou need to select an early termination policy that meets the following requirements\n\n* accounts for the performance of all previous runs when evaluating the current run\n* avoids comparing the current run with only the best performing run to date\n\nWhich two early termination policies should you use? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Median stopping",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Bandit",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Default",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Truncation selection",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: AC.\nCommunity:\n100% for AD\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c0385096823b4935883bd1efe768ae18",
        "type": "multiple_choice_single_answer",
        "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. modelfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou create an Azure Machine Learning service datastore in a workspace. The datastore contains the following files:\n\n* /data/2018/Q1.csv\n\n* /data/2018/Q2.csv\n\n* /data/2018/Q3.csv\n\n* /data/2018/Q4.csv\n\n* /data/2019/Q1.csv\n\nAll files store data in the following format:\n\nid,f1,f2,I\n\n1,1,2,0\n\n2,1,1,1\n\n3,2,1,0\n\n4,2,2,1\n\nYou run the following code:\n\n```python\ndata_store = Datastore.register_azure_blob_container(\n    workspace=ws,\n    datastore_name='data_store',\n    container_name='quarterly data',\n    account_name='companydata',\n    account_key='NRPxk8duxbM3...',\n    create_if_not_exists=False\n)\n```\n\nYou need to create a dataset named `training_data` and load the data from all files into a single data frame by using the following code:\n\n```python\ndata_frame = training_data.to_pandas_dataframe()\n```\n\nRun the following code:\n\n```python\nfrom azureml.core import Dataset\npaths = (data_store, 'data/*/*.csv')\ntraining_data = Dataset.Tabular.from_delimited_files(path)\n```\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n91% for A(91%)\n. Also completely outdated.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_94e7b68c91a440bd8e04f66d379a7de1",
        "type": "multiple_choice_single_answer",
        "question": "You use the Azure Machine Learning Python SDK to define a pipeline to train a model.\n\nThe data used to train the model is read from a folder in a datastore.\n\nYou need to ensure the pipeline runs automatically whenever the data in the folder changes.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Set the regenerate_outputs property of the pipeline to True",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a ScheduleRecurrance object with a Frequency of auto. Use the object to create a Schedule for the pipeline",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create a PipelineParameter with a default value that references the location where the training data is stored",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a Schedule for the pipeline. Specify the datastore in the datastore property, and the folder containing the training data in the path_on_datastore property",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3751e809be894fe7a745f207e35eeaae",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou have been tasked with evaluating your model on a partial data sample via k-fold cross-validation.\n\nYou have already configured a k parameter as the number of splits. You now have to configure the k parameter for the cross-validation with the usual value choice.\n\nRecommendation: You configure the use of the value k=3.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_af6ed30699f043348dadd182a353a52e",
        "type": "multiple_choice_single_answer",
        "question": "You have been tasked with creating a new Azure pipeline via the Machine Learning designer.\n\nYou have to makes sure that the pipeline trains a model using data in a comma-separated values (CSV) file that is published on a website. A dataset for the file for this file does not exist.\n\nData from the CSV file must be ingested into the designer pipeline with the least amount of administrative effort as possible.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should make use of the Convert to TXT module.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should add the Copy Data object to the pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should add the Import Data object to the pipeline.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should add the Dataset object to the pipeline.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: D.\nCommunity:\n100% for C(100%)\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c3d8963d316b4d32b7d9445e3bab389c",
        "type": "multiple_choice_single_answer",
        "question": "You run an experiment that uses an AutoMLConfig class to define an automated machine learning task with a maximum of ten model training iterations. The task will attempt to find the best performing model based on a metric named accuracy.\n\nYou submit the experiment with the following code:\n\nfrom azureml.core.experiment import Experiment\nautoml_experiment = Experiment (ws, \u2018automl_experiment\u2019)\nautoml_run = automl_experiment.submit (automl_config, show_output=True)\n\n\nYou need to create Python code that returns the best model that is generated by the automated machine learning task.\n\nWhich code segment should you use?",
        "options": [
            {
                "id": "A",
                "text": "best_model = automl_run.get_details()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "best_model = automl_run.get_metrics()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "best_model = automl_run.get_file_names()[1]",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "best_model = automl_run.get_output()[1]",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_209c8c1642044cf2801fb583a2cb9a36",
        "type": "multiple_choice_single_answer",
        "question": "You create an Azure Machine Learning pipeline named pipeline1 with two steps that contain Python scripts. Data processed by the first step is passed to the second step.\n\nYou must update the content of the downstream data source of pipeline1 and run the pipeline again.\n\nYou need to ensure the new run of pipeline1 fully processes the updated content.\n\nSolution: Set the allow_reuse parameter of the PythonScriptStep object of both steps to False.\n\nDoes the solution meet the goal?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for A\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_072a49ff403a4a288a3ba9b55aac23da",
        "type": "multiple_choice_single_answer",
        "question": "You manage an Azure Machine Learning workspace. You have an environment for training jobs which uses an existing Docker image.\n\nA new version of the Docker image is available.\n\nYou need to use the latest version of the Docker image for the environment configuration by using the Azure Machine Learning SDK v2.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Modify the conda_file to specify the new version of the Docker image.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Use the Environment class to create a new version of the environment.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Use the create_or_update method to change the tag of the image.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Change the description parameter of the environment configuration.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n90% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_590df2b4ba9248718bc5a1fffdaacbca",
        "type": "multiple_choice_single_answer",
        "question": "You plan to deliver a hands-on workshop to several students. The workshop will focus on creating data visualizations using Python. Each student will use a device that has internet access.\n\nStudent devices are not configured for Python development. Students do not have administrator access to install software on their devices. Azure subscriptions are not available for students.\n\nYou need to ensure that students can run Python-based data visualization code.\n\nWhich Azure tool should you use?",
        "options": [
            {
                "id": "A",
                "text": "Anaconda Data Science Platform",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Azure BatchAI",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Notebooks",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Machine Learning Service",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c011e0b5190a49babc317676f634e7ed",
        "type": "multiple_choice_multiple_answer",
        "question": "You plan to use the Hyperdrive feature of Azure Machine Learning to determine the optimal hyperparameter values when training a model.\n\nYou must use Hyperdrive to try combinations of the following hyperparameter values\n\n* learning_rate: any value between 0.001 and 0.1\n* batch_size: 16, 32, or 64\n\nYou need to configure the search space for the Hyperdrive experiment.\n\nWhich two parameter expressions should you use? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "a choice expression for learning_rate",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "a uniform expression for learning_rate",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "a normal expression for batch_size",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "a choice expression for batch_size",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "a uniform expression for batch_size",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_2e099b9351724c99ae724bfb7c062d93",
        "type": "multiple_choice_multiple_answer",
        "question": "You are building a binary classification model by using a supplied training set.\n\nThe training set is imbalanced between two classes.\n\nYou need to resolve the data imbalance.\n\nWhat are three possible ways to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Penalize the classification",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Resample the dataset using undersampling or oversampling",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Normalize the training feature set",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Generate synthetic samples in the minority class",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Use accuracy as the evaluation metric of the model",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_0808afbcf14940bf83cd4a23d2ac45be",
        "type": "multiple_choice_multiple_answer",
        "question": "You are analyzing a dataset containing historical data from a local taxi company. You are developing a regression model.\n\nYou must predict the fare of a taxi trip.\n\nYou need to select performance metrics to correctly evaluate the regression model.\n\nWhich two metrics can you use? Each correct answer presents a complete solution?",
        "options": [
            {
                "id": "A",
                "text": "a Root Mean Square Error value that is low",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "an R-Squared value close to 0",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "an F1 score that is low",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "an R-Squared value close to 1",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "an F1 score that is high",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "a Root Mean Square Error value that is high",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_90ffbe5735604cd1b50d1a4756659d2a",
        "type": "multiple_choice_single_answer",
        "question": "You are implementing a machine learning model to predict stock prices.\n\nThe model uses a PostgreSQL database and requires GPU processing.\n\nYou need to create a virtual machine that is pre-configured with the required tools.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Create a Data Science Virtual Machine (DSVM) Windows edition.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a Geo Al Data Science Virtual Machine (Geo-DSVM) Windows edition.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Create a Deep Learning Virtual Machine (DLVM) Linux edition.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a Deep Learning Virtual Machine (DLVM) Windows edition.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: A.\nCommunity:\n100% for C\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_8c243e10d68741189ef573d43822d3c8",
        "type": "multiple_choice_single_answer",
        "question": "You are in the process of constructing a deep convolutional neural network (CNN). The CNN will be used for image classification.\n\nYou notice that the CNN model you constructed displays hints of overfitting.\n\nYou want to make sure that overfitting is minimized, and that the model is converged to an optimal fit.\n\nWhich of the following is TRUE with regards to achieving your goal?",
        "options": [
            {
                "id": "A",
                "text": "You have to add an additional dense layer with 512 input units, and reduce the amount of training data.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You have to add L1/L2 regularization, and reduce the amount of training data.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You have to reduce the amount of training data and make use of training data augmentation.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You have to add L1/L2 regularization, and make use of training data augmentation.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "You have to add an additional dense layer with 512 input units, and add L1/L2 regularization.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "q_106",
        "type": "multiple_choice_multiple_answer",
        "question": "You are building a pipeline using the Azure Machine Learning SDK v2.\n\nYou define the pipeline steps using components and set up the following:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.dsl import pipeline\nfrom azure.identity import DefaultAzureCredential\n\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n@pipeline()\ndef my_pipeline():\n    step1 = component1()\n    step2 = component2(input=step1.output)\n    return step2\n\npipeline_job = my_pipeline()\n```\n\nYou need to submit the pipeline for execution.\n\nWhich two code segments can you use to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "ml_client.jobs.create_or_update(pipeline_job, experiment_name='pipeline-experiment')",
                "is_correct": true,
                "explanation": "This is the recommended method in SDK v2 to submit a PipelineJob using MLClient."
            },
            {
                "id": "B",
                "text": "pipeline_job.submit(experiment_name='pipeline-experiment')",
                "is_correct": false,
                "explanation": "PipelineJob objects do not have a `submit()` method in SDK v2. Jobs are submitted through MLClient."
            },
            {
                "id": "C",
                "text": "ml_client.pipeline_jobs.create_or_update(pipeline_job)",
                "is_correct": false,
                "explanation": "`pipeline_jobs` is not a valid property of MLClient. Use `ml_client.jobs` instead."
            },
            {
                "id": "D",
                "text": "submitted_job = ml_client.jobs.create_or_update(pipeline_job)",
                "is_correct": true,
                "explanation": "You can submit a PipelineJob without specifying an experiment name, and it will use the default or a name from the job definition."
            }
        ],
        "feedback": "In Azure ML SDK v2, all jobs — including pipelines — are submitted using `ml_client.jobs.create_or_update(...)`. The old SDK v1 style using `Experiment.submit()` is no longer valid.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ffd097f01d3c45aeb34c97f097af207d",
        "type": "multiple_choice_multiple_answer",
        "question": "You plan to run a Python script as an Azure Machine Learning experiment.\n\nThe script must read files from a hierarchy of folders. The files will be passed to the script as a dataset argument.\n\nYou must specify an appropriate mode for the dataset argument.\n\nWhich two modes can you use? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "to_pandas_dataframe()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "as_download()",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "as_upload()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "as_mount()",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "Exam topics say B only (despite select 2), community says BD.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_650c0648c428434c8489af438eeeb753",
        "type": "multiple_choice_single_answer",
        "question": "You deploy a real-time inference service for a trained model.\n\nThe deployed model supports a business-critical application, and it is important to be able to monitor the data submitted to the web service and the predictions the data generates.\n\nYou need to implement a monitoring solution for the deployed model using minimal administrative effort.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "View the explanations for the registered model in Azure ML studio.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Enable Azure Application Insights for the service endpoint and view logged data in the Azure portal.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "View the log files generated by the experiment used to train the model.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create an ML Flow tracking URI that references the endpoint, and view the data logged by ML Flow.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_0f00d238c9f7429b84fb6a3560af5ad0",
        "type": "multiple_choice_single_answer",
        "question": "You create a binary classification model. You use the Fairlearn package to assess model fairness.\n\nYou must eliminate the need to retrain the model.\n\nYou need to implement the Fairlearn package.\n\nWhich algorithm should you use?",
        "options": [
            {
                "id": "A",
                "text": "fairlearn.reductions.ExponentiatedGradient",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "fairlearn.postprocessing.ThresholdOptimizer",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "fairlearn.preprocessing.CorrelationRemover",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "fairlearn.reductions.GridSearch",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: C.\nCommunity:\n100% for B\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_4fa004c864e74c2babd29976641de939",
        "type": "multiple_choice_single_answer",
        "question": "You train a model and register it in your Azure Machine Learning workspace. You are ready to deploy the model as a real-time web service.\n\nYou deploy the model to an Azure Kubernetes Service (AKS) inference cluster, but the deployment fails because an error occurs when the service runs the entry script that is associated with the model deployment.\n\nYou need to debug the error by iteratively modifying the code and reloading the service, without requiring a re-deployment of the service for each code update.\n\nWhat should you do?",
        "options": [
            {
                "id": "A",
                "text": "Modify the AKS service deployment configuration to enable application insights and re-deploy to AKS.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create an Azure Container Instances (ACI) web service deployment configuration and deploy the model on ACI.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Add a breakpoint to the first line of the entry script and redeploy the service to AKS.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Create a local web service deployment configuration and deploy the model to a local Docker container.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Register a new version of the model and update the entry script to load the new version of the model from its registered path.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: B.\nCommunity:\n100% for D\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_78d9ea2b421d4d11892afecb9a120ab8",
        "type": "multiple_choice_multiple_answer",
        "question": "You are developing a hands-on workshop to introduce Docker for Windows to attendees.\n\nYou need to ensure that workshop attendees can install Docker on their devices.\n\nWhich two prerequisite components should attendees install on the devices? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Microsoft Hardware-Assisted Virtualization Detection Tool",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Kitematic",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "BIOS-enabled virtualization",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "VirtualBox",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Windows 10 64-bit Professional",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_56a5218fd50342fcbb075473a6f6cf24",
        "type": "multiple_choice_multiple_answer",
        "question": "You create a pipeline in designer to train a model that predicts automobile prices.\n\nBecause of non-linear relationships in the data, the pipeline calculates the natural log (ln) of the prices in the training data, trains a model to predict this natural log of price value, and then calculates the exponential of the scored label to get the predicted price.\n\nThe training pipeline is shown in the exhibit. (Click the Training pipeline tab.)\n\nTraining pipeline \n\n* Automobile data |\n\niS\n\n\u00a9 Apply Math Operation Qo |\n\nReplace price with Lniprice)\n\n|\n\n70% train / 30% validate\n\n\u00a9 Linear Regression ry | E Split Data al\n\n= fan\n\n| (fe Train Model i) |\n\nPredict Ln(price)\n\n{Score Model Qo\nGet Ln(price) prediction\n\nJ\n\n\u00a9 Apply Math Operation Qo\nReplace Scored Labels w. Exp(Scored Labels)\n\nCC\n\n& ApplySQLTransformation @\nSELECT [Scored Labels] AS predicted_price\n\n\n\nYou create a real-time inference pipeline from the training pipeline, as shown in the exhibit. (Click the Real-time pipeline tab.)\n\nReal-time pipeline \n\u00a9 Automobile data\n\n| Apply Math Operation\nReplace price with Ln(price)\n\n( \u00a9 MD-Automobile_Price_Regress... ]\n\n\u2014 an,\n\nGet Ln(price) prediction\n\n[i Score Model |\n\n\u00a9 Apply Math Operation\nReplace Scored Labels w. Exp(Scored Labels)\n\n\u2014J\n\n[ Apply SQL Transformation |\n\nSELECT [Scored Labels] AS predicted_price\n\n\n\nYou need to modify the inference pipeline to ensure that the web service returns the exponential of the scored label as the predicted automobile price and that client applications are not required to include a price value in the input values.\n\nWhich three modifications must you make to the inference pipeline? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "Connect the output of the Apply SQL Transformation to the Web Service Output module.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Replace the Web Service Input module with a data input that does not include the price column.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Add a Select Columns module before the Score Model module to select all columns other than price.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Replace the training dataset module with a data input that does not include the price column.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Remove the Apply Math Operation module that replaces price with its natural log from the data fiow.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "F",
                "text": "Remove the Apply SQL Transformation module from the data fiow.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_c50568bd1b6e403cb2c611f7e378bd82",
        "type": "multiple_choice_single_answer",
        "question": "You plan to use a Data Science Virtual Machine (DSVM) with the open source deep learning frameworks Caffe2 and PyTorch.\n\nYou need to select a pre-configured DSVM to support the frameworks.\n\nWhat should you create?",
        "options": [
            {
                "id": "A",
                "text": "Data Science Virtual Machine for Windows 2012",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Data Science Virtual Machine for Linux (CentOS)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Geo AI Data Science Virtual Machine with ArcGIS",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Data Science Virtual Machine for Windows 2016",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Data Science Virtual Machine for Linux (Ubuntu)",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ffcca60419154e0bb8ad6d9db697bd1c",
        "type": "multiple_choice_single_answer",
        "question": "You plan to use a Deep Learning Virtual Machine (DLVM) to train deep learning models using Compute Unified Device Architecture (CUDA)\n\ncomputations.\n\nYou need to configure the DLVM to support CUD",
        "options": [
            {
                "id": "A",
                "text": "Solid State Drives (SSD)",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Computer Processing Unit (CPU) speed increase by using overclocking",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Graphic Processing Unit (GPU)",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "High Random Access Memory (RAM) configuration",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Intel Software Guard Extensions (Intel SGX) technology",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_406717f3973e49f1812d6313aa3d22a4",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are in the process of carrying out feature engineering on a dataset.\n\nYou want to add a feature to the dataset and fill the column value.\n\nRecommendation: You must make use of the Join Data Azure Machine Learning Studio module.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3e59267b760945fa9c0897c0e5cdc6eb",
        "type": "multiple_choice_multiple_answer",
        "question": "You are building a regression model for estimating the number of calls during an event.\n\nYou need to determine whether the feature values achieve the conditions to build a Poisson regression model.\n\nWhich two conditions must the feature set contain? Each correct answer presents part of the solution.",
        "options": [
            {
                "id": "A",
                "text": "The label data must be a negative value.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "The label data must be whole numbers.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "The label data must be non-discrete.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "The label data must be a positive value.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "The label data can be positive or negative.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_43e29f8bc7ac41a789ef61542f28160a",
        "type": "multiple_choice_single_answer",
        "question": "This question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the recommendation satisfies the requirements.\n\nYou are planning to make use of Azure Machine Learning designer to train models.\n\nYou need choose a suitable compute type.\n\nRecommendation: You choose Attached compute.\n\nWill the requirements be satisfied?",
        "options": [
            {
                "id": "A",
                "text": "Yes",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "No",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_39f380e58d8349aaa2ba1474a7f7a717",
        "type": "multiple_choice_single_answer",
        "question": "You are using the Azure Machine Learning Python SDK v2 to run a training script that processes a large number of image files stored in a FileDataset.\n\nThe training script includes logic to iterate through `.jpg` files in a folder path provided as an argument. You want to ensure the dataset is **streamed directly from the source** without downloading the entire dataset to local storage.\n\nYou need to pass the dataset to the script as an input when creating a `command` job.\n\nWhich argument should you pass to the job definition?",
        "options": [
            {
                "id": "A",
                "text": "inputs={'input_data': ds.to_pandas_dataframe()}",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "inputs={'input_data': Input(type='uri_folder', path=ds, mode='ro_mount')}",
                "is_correct": true,
                "explanation": "When using Azure ML SDK v2, mounting a dataset with `Input(..., mode='ro_mount')` allows your script to stream large files directly from storage without downloading them first. This is suitable for large datasets like image folders."
            },
            {
                "id": "C",
                "text": "inputs={'input_data': ds}",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "inputs={'input_data': Input(type='uri_folder', path=ds, mode='download')}",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "To efficiently handle large file datasets in SDK v2, use `Input(type='uri_folder', path=..., mode='ro_mount')`. This ensures streaming access and avoids local disk space issues.",
        "include_in_bank": true
        },
    {
        "question_id": "examtopics_66f6d4936a024b419bbbda295aff9a67",
        "type": "multiple_choice_single_answer",
        "question": "You are preparing to train a regression model via automated machine learning. The data available to you has features with missing values, as well as categorical features with little discrete values.\n\nYou want to make sure that automated machine learning is configured as follows\n\n* missing values must be automatically imputed.\n* categorical features must be encoded as part of the training task.\n\nWhich of the following actions should you take?",
        "options": [
            {
                "id": "A",
                "text": "You should make use of the featurization parameter with the 'auto' value pair.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "You should make use of the featurization parameter with the 'off' value pair.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "You should make use of the featurization parameter with the 'on' value pair.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "You should make use of the featurization parameter with the 'FeaturizationConfig' value pair.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_ebde33f255e848988cb6e8eba66b7ca2",
        "type": "multiple_choice_multiple_answer",
        "question": "You use the Azure Machine Learning SDK to run a training experiment that trains a classification model and calculates its accuracy metric.\n\nThe model will be retrained each month as new data is available.\n\nYou must register the model for use in a batch inference pipeline.\n\nYou need to register the model and ensure that the models created by subsequent retraining experiments are registered only if their accuracy is higher than the currently registered model.\n\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {
                "id": "A",
                "text": "Specify a different name for the model each time you register it.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Register the model with the same name each time regardless of accuracy, and always use the latest version of the model in the batch inferencing pipeline.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Specify the model framework version when registering the model, and only register subsequent models if this value is higher.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Specify a property named accuracy with the accuracy metric as a value when registering the model, and only register subsequent models if their accuracy is higher than the accuracy property value of the currently registered model.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Specify a tag named accuracy with the accuracy metric as a value when registering the model, and only register subsequent models if their accuracy is higher than the accuracy tag value of the currently registered model.",
                "is_correct": true,
                "explanation": ""
            }
        ],
        "feedback": "ExamTopics answer: CE.\nCommunity:\n100% for DE\n",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_e9896efaf76145a3bd75b293fbe06667",
        "type": "multiple_choice_single_answer",
        "question": "You are a data scientist working for a hotel booking website company. You use the Azure Machine Learning service to train a model that identifies fraudulent transactions.\n\nYou must deploy the model as an Azure Machine Learning real-time web service using the Model.deploy method in the Azure Machine Learning SDK. The deployed web service must return real-time predictions of fraud based on transaction data input.\n\nYou need to create the script that is specified as the entry_script parameter for the InferenceConfig class used to deploy the model.\n\nWhat should the entry script do?",
        "options": [
            {
                "id": "A",
                "text": "Register the model with appropriate tags and properties.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "Create a Conda environment for the web service compute and install the necessary Python packages.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Load the model and use it to predict labels from input data.",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Start a node on the inference cluster where the web service is deployed.",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "E",
                "text": "Specify the number of cores and the amount of memory required for the inference compute.",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_3bf4f169748448008bae22dfa5f1f357",
        "type": "multiple_choice_single_answer",
        "question": "(hotspot) Fill in the blank.\nTo move a large dataset from Azure Machine Learning Studio to a Weka environment, the data must be converted to ______ format.",
        "options": [
            {"id": "A", "text": "CSV", "is_correct": false, "explanation": ""},
            {"id": "B", "text": "DOCX", "is_correct": false, "explanation": ""},
            {"id": "C", "text": "ARFF", "is_correct": true, "explanation": "Use the Convert to ARFF module in Azure Machine Learning Studio to convert datasets and results in Azure Machine Learning to the attribute relation file format used by the Weka toolset. This format is known as ARFF.\n\nThe ARFF data specification for Weka supports multiple machine learning tasks, including data preprocessing, classification, and feature selection. In this format, data is organized by entities and their attributes, and is contained in a single text file."},
            {"id": "D", "text": "TXT", "is_correct": false, "explanation": ""}
        ],
        "feedback": "Probably irrelevant. https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/convert-to-arff",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_be245ce466fe4811a39d819f31e69416",
        "type": "multiple_choice_multiple_answer",
        "question": "(drag and drop) You are in the process of constructing a regression model.\nYou would like to make it a Poisson regression model. To achieve your goal, the feature values need to meet certain conditions.Which of the following are relevant conditions with regards to the label data?",
        "options": [
            {"id": "A", "text": "It must be whole numbers", "is_correct": true, "explanation": ""},
            {"id": "B", "text": "It must be a negative value", "is_correct": false, "explanation": ""},
            {"id": "C", "text": "It must be fractions", "is_correct": false, "explanation": ""},
            {"id": "D", "text": "It must be non-discrete", "is_correct": false, "explanation": ""},
            {"id": "E", "text": "It must be a positive value", "is_correct": true, "explanation": ""}
        ],
        "feedback": "Poisson regression is intended for use in regression models that are used to predict numeric values, typically counts. Therefore, you should use this module to create your regression models only if the values you are trying to predict fit the following conditions:\n\nThe response variable has a Poisson distribution, Counts cannot be negative. A Poisson distribution is a discrete distribution, therefore it is not meaningful to use this mehtod with non-integer numbers.\n\nReference:\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/poisson-regression",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_b887da7a7772462e9ee4f3dcf7986c77",
        "type": "multiple_choice_multiple_answer",
        "question": "(drag and drop) You have been tasked with evaluating the performance of a binary classification model that you created.\nYou need to choose evaluation metrics to achieve your goal\n\n Which of the following are the metrics you would choose?",
        "options": [
            {"id": "A", "text": "Relative Absolute Error", "is_correct": false, "explanation": ""},
            {"id": "B", "text": "Accuracy", "is_correct": true, "explanation": ""},
            {"id": "C", "text": "Coefficient of determination", "is_correct": false, "explanation": ""},
            {"id": "D", "text": "Precision", "is_correct": true, "explanation": ""},
            {"id": "E", "text": "Mean absolute error", "is_correct": false, "explanation": ""}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_6bc0ca779ab94e97b0335cdb81892daf",
        "type": "multiple_choice_multiple_answer",
        "question": "You build a binary classification model using the Azure Machine Learning Studio Two-Class Neural Network module.\nYou are preparing to configure the Tune Model Hyperparameters module for the purpose of tuning accuracy for the model.\n\nWhich of the following are valid parameters for the Two-Class Neural Network module? Answer by dragging the correct options from the list to the answer area",
        "options": [
            {"id": "A", "text": "Depth of the tree", "is_correct": false, "explanation": ""},
            {"id": "B", "text": "Random number seed", "is_correct": true, "explanation": ""},
            {"id": "C", "text": "Optimization tolerance", "is_correct": false, "explanation": ""},
            {"id": "D", "text": "The initial learning weights diameters", "is_correct": true, "explanation": ""},
            {"id": "E", "text": "Lambda", "is_correct": false, "explanation": ""},
            {"id": "F", "text": "Number of learning iterations", "is_correct": true, "explanation": ""},
            {"id": "G", "text": "Project to the unit-sphere", "is_correct": false, "explanation": ""}
        ],
        "feedback": "Reference:\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-neural-network",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a4dec7241d654887ae0973772861e0a3",
        "type": "ordering",
        "question": "You create an Azure Machine Learning workspace.\n\nYou must implement dedicated compute for model training in the workspace by using Azure Synapse compute resources. The solution must attach the dedicated compute and start an Azure Synapse session.\n\nYou need to implement the computer resources.\n\nWhich three actions should you perform in sequence?",
        "options": [
            {
                "id": "A",
                "text": "Create compute clusters by using Azure Machine Learning Studio."
            },
            {
                "id": "B",
                "text": "Create a linked service by using Azure Synapse Studio."
            },
            {
                "id": "C",
                "text": "Attach the Compute in Azure Machine Learning Studio."
            },
            {
                "id": "D",
                "text": "Create an Azure Synapse workspace by using the Azure Portal."
            },
            {
                "id": "E",
                "text": "Create an Azure Synapse Compute"
            }
        ],
        "correct_order": [
            "D",
            "E",
            "C"
        ],
        "feedback": "Should be correct. Not really part of any learning path.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_7896f4ca46514b9b98d7edfe1d75d338",
        "type": "ordering",
        "question": "You are building an intelligent solution using machine learning models.\n\nThe environment must support the following requirements:\n\n* Data scientists must build notebooks in a cloud environment\n* Data scientists must use automatic feature engineering and model building in machine learning pipelines.\n* Notebooks must be deployed to retrain using Spark instances with dynamic worker allocation.\n* Notebooks must be exportable to be version controlled locally.\n\nYou need to create the environment.\n\nWhich four actions should you perform in sequence?",
        "options": [
            {
                "id": "A",
                "text": "Install the Azure Machine Learning SDK for Python on the cluster."
            },
            {
                "id": "B",
                "text": "When the cluster is ready, export Zeppelin notebooks to a local environment"
            },
            {
                "id": "C",
                "text": "Create and execute a Jupyter notebook by using automated machine learning (AutoML) on the cluster."
            },
            {
                "id": "D",
                "text": "Install Microsoft Machine Learning for Apache Spark."
            },
            {
                "id": "E",
                "text": "When the cluster is ready and has processed the notebook, export your Jupyter notebook to a local environment."
            },
            {
                "id": "F",
                "text": "Create an Azure HDInsight cluster to include in Apache Spark MLib library."
            },
            {
                "id": "G",
                "text": "Create and execute the Zeppelin notebooks on the cluster."
            },
            {
                "id": "H",
                "text": "Create an Azure Databricks cluster."
            }
        ],
        "correct_order": [
            "H",
            "A",
            "G",
            "B"
        ],
        "feedback": "Should be correct. Not really part of any learning path.",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_b934b2f66c8444c788989a6f1d4985e8",
        "type": "ordering",
        "question": "You are creating an experiment by using Azure Machine Learning Studio.\n\nYou must divide the data into four subsets for evaluation. There is a high degree of missing values in the data. You must prepare the data for analysis.\n\nYou need to select appropriate methods for producing the experiment.\n\nWhich three modules should you run in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\n\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.",
        "options": [
            {
                "id": "A",
                "text": "Build Counting Transform"
            },
            {
                "id": "B",
                "text": "Missing Values Scrubber"
            },
            {
                "id": "C",
                "text": "Feature Hashing"
            },
            {
                "id": "D",
                "text": "Clean Missing Data"
            },
            {
                "id": "E",
                "text": "Replace Discrete Values"
            },
            {
                "id": "F",
                "text": "Import Data"
            },
            {
                "id": "G",
                "text": "Latent Dirichlet Transformation"
            },
            {
                "id": "H",
                "text": "Partition and Sample"
            }
        ],
        "correct_order": [
            "F",
            "D",
            "H"
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_32b4f0d788bc427fa1465b3344dae16a",
        "type": "multiple_choice_multiple_answer",
        "question": "You are retrieving data from a large datastore by using Azure Machine Learning Studio.\n\nYou must create a subset of the data for testing purposes using a random sampling seed based on the system clock.\n\nYou add the Partition and Sample module to your experiment.\n\nYou need to select the properties for the module.\n\nWhich values should you select? To answer, select the appropriate options in the answer area.",
        "options": [
            {"id": "A", "text": "(Partition or sample mode) Assign to Folds", "is_correct": false, "explanation": ""},
            {"id": "B", "text": "(Partition or sample mode) Pick Fold", "is_correct": false, "explanation": ""},
            {"id": "C", "text": "(Partition or sample mode) Sampling", "is_correct": true, "explanation": ""},
            {"id": "D", "text": "(Partition or sample mode) Head", "is_correct": false, "explanation": ""},
            {"id": "E", "text": "(Random seed for sampling) 0", "is_correct": true, "explanation": "Default value is 0. It generates the random seed based on the system clock."},
            {"id": "F", "text": "(Random seed for sampling) 1", "is_correct": false, "explanation": ""},
            {"id": "G", "text": "(Random seed for sampling) time.clock()", "is_correct": false, "explanation": ""},
            {"id": "H", "text": "(Random seed for sampling) utcNow()", "is_correct": false, "explanation": ""}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_b7ab5de6cbaa40c8a4ad547cf4ef7e58",
        "type": "multiple_choice_multiple_answer",
        "question": "The finance team asks you to train a model using data in an Azure Storage blob container named finance-data.\n\nYou need to register the container as a datastore in an Azure Machine Learning workspace and ensure that an error will be raised if the container does not exist.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area\n\n```python\ndatstore = Datastore.____(\n\tworkspace=ws,\n\tdatastore_name='finance_datastore',\n\tcontainer_name='finance-data',\n\taccount_name='fintrainingdatastorage',\n\taccount_key='...',\n\t____\n)\n```",
        "options": [
            {"id": "A", "text": "(first) `register_azure_blob_container`", "is_correct": true, "explanation": ""},
            {"id": "B", "text": "(first) `register_azure_file_share`", "is_correct": false, "explanation": ""},
            {"id": "C", "text": "(first) `register_azure_data_lake`", "is_correct": false, "explanation": ""},
            {"id": "D", "text": "(first) `register_azure_sql_database`", "is_correct": false, "explanation": ""},
            {"id": "E", "text": "(second) `create_if_not_exists=True`", "is_correct": false, "explanation": ""},
            {"id": "F", "text": "(second) `create_if_not_exists=False`", "is_correct": true, "explanation": ""},
            {"id": "G", "text": "(second) `overwrite=True`", "is_correct": false, "explanation": ""},
            {"id": "H", "text": "(second) `overwrite=False`", "is_correct": false, "explanation": ""}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_bd4277d8f900454e8eb5a5a6393d17be",
        "type": "ordering",
        "question": "An organization uses Azure Machine Learning service and wants to expand their use of machine learning.\n\nYou have the following compute environments (name, compute type):\n\n* nb_server, Compute Instance\n* aks_cluster, Azure Kubernetes Service\n* mlc_cluster, Machine Learning Compute\n\nThe organization does not want to create another compute environment. You need to determine which compute environment to use for the following scenarios\n\n1. Run an Azure Machine Learning Designer training pipeline\n2. Deploying a web service from the Azure Machine Learning Designer.\n\nWhich compute types should you use?",
        "options": [
            {
                "id": "A",
                "text": "nb_server"
            },
            {
                "id": "B",
                "text": "aks_cluster"
            },
            {
                "id": "C",
                "text": "mlc_cluster"
            }
        ],
        "correct_order": [
            "C",
            "B"
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_83695f3795174891a8242b26ce78f793",
        "type": "ordering",
        "question": "You are analyzing a raw dataset that requires cleaning.\n\nYou must perform transformations and manipulations by using Azure Machine Learning Studio.\n\nYou need to identify the correct modules to perform the transformations.\n\nWhich modules should you choose? To answer, drag the appropriate modules to the correct scenarios. Each module may be used once, more than once, or not at all.\n\nYou may need to drag the split bar between panes or scroll to view content.\n\n1. Replace missing values by removing rows and columns\n2. Increase the number of low-incidence examples in the dataset\n3. Convert a categorical features into a binary indicator\n 4. Remove potential duplicates from a dataset",
        "options": [
            {
                "id": "A",
                "text": "Convert to Indicator Values"
            },
            {
                "id": "B",
                "text": "SMOTE"
            },
            {
                "id": "C",
                "text": "Threshold Filter"
            },
            {
                "id": "D",
                "text": "Clean Missing Data"
            },
            {
                "id": "E",
                "text": "Remove Duplicate Rows"
            }
        ],
        "correct_order": [
            "D",
            "B",
            "A",
            "E"
        ],
        "feedback": "Reference:\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote\n\nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/convert-to-indicator-value",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a895f01bbec94649bc3107733235f510",
        "type": "multiple_choice_single_answer",
        "question": "You create an Azure Machine Learning workspace.\n\nYou must create a custom role named DataScientist that meets the following requirements:\n\n* Role members must not be able to delete the workspace.\n* Role members must not be able to create, update, or delete compute resources in the workspace.\n* Role members must not be able to add new users to the workspace.\n\nYou need to create a JSON file for the DataScientist role in the Azure Machine Learning workspace.\n\nThe custom role must enforce the restrictions specified by the IT Operations team.\n\nWhich JSON code segment should you use?",
        "options": [
            {
                "id": "A", 
                "text": "\n```json\n{\n  \"Name\": \"DataScientist\",\n  \"IsCustom\": true,\n  \"Actions\": [\"*\"],\n  \"NotActions\": [\n    \"Microsoft.MachineLearningServices/workspaces/*/delete\",\n    \"Microsoft.MachineLearningServices/workspaces/computes/*/write\",\n    \"Microsoft.MachineLearningServices/workspaces/computes/*/delete\",\n    \"Microsoft.Authorization/*/write\"\n  ]\n}\n```\n",
                "is_correct": true,
                "explanation": ""
            },
            {
                "id": "B", 
                "text": "\n```json\n{\n  \"Name\": \"DataScientist\",\n  \"IsCustom\": true,\n  \"Actions\": [\"*\"],\n  \"NotActions\": []\n}\n```\n",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C", 
                "text": "\n```json\n{\n  \"Name\": \"DataScientist\",\n  \"IsCustom\": true,\n  \"Actions\": [\n    \"Microsoft.MachineLearningServices/workspaces/*/delete\",\n    \"Microsoft.MachineLearningServices/workspaces/computes/*/write\",\n    \"Microsoft.MachineLearningServices/workspaces/computes/*/delete\",\n    \"Microsoft.Authorization/*/write\"\n  ],\n  \"NotActions\": []\n}\n```\n",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D", 
                "text": "\n```json\n{\n  \"Name\": \"DataScientist\",\n  \"IsCustom\": true,\n  \"Actions\": [],\n  \"NotActions\": [\"*\"]\n}\n```\n",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_a8877b060c7b41f49402bd6c143c872e",
        "type": "multiple_choice_multiple_answer",
        "question": "You are performing a classification task in Azure Machine Learning Studio.\n\nYou must prepare balanced testing and training samples based on a provided data set.\n\nYou need to split the data with a 0.75:0.25 ratio.\n\nWhich value should you use for each parameter? To answer, select the appropriate options in the answer area.\n\n1. Splitting mode\n2. Fraction of rows in the first output dataset\n3. Randomized split\n4. Stratified split",
        "options": [
            {"id": "A", "text": "(1) Split rows", "is_correct": true, "explanation": ""},
            {"id": "B", "text": "(1) Recommender split", "is_correct": false, "explanation": ""},
            {"id": "C", "text": "(1) Regular expression split", "is_correct": false, "explanation": ""},
            {"id": "D", "text": "(1) Relative expression split", "is_correct": false, "explanation": ""},
            {"id": "E", "text": "(2) 0.75", "is_correct": true, "explanation": ""},
            {"id": "F", "text": "(2) 0.25", "is_correct": false, "explanation": ""},
            {"id": "G", "text": "(2) 0.5", "is_correct": false, "explanation": ""},
            {"id": "H", "text": "(2) 1", "is_correct": false, "explanation": ""},
            {"id": "I", "text": "(3) True", "is_correct": true, "explanation": ""},
            {"id": "J", "text": "(3) False", "is_correct": false, "explanation": ""},
            {"id": "K", "text": "(4) True", "is_correct": false, "explanation": ""},
            {"id": "L", "text": "(4) False", "is_correct": true, "explanation": ""}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_b0b943a902cd448cb313fdadb2cb6887",
        "type": "multiple_choice_multiple_answer",
        "question": "You create a new Azure subscription. No resources are provisioned in the subscription.\n\nYou need to create an Azure Machine Learning workspace.\n\nWhat are possible ways to achieve this goal? Each correct answer presents a complete solution.",
        "options": [
            {"id": "A", "text": "Use Azure ML Python SDK (v2)\n\n```python\nml_client = MLClient.from_config()\nworkspace = ml_client.begin_create(workspace_name)\n```\nwill create resource group automatically", "is_correct": false, "explanation": "Resource group is not created automatically."},
            {"id": "B", "text": "Use Azure Portal", "is_correct": true, "explanation": "In Azure Portal, one can create ML workspace (incl. new resource group if needed)"},
            {"id": "C", "text": "Use Azure ML Studio", "is_correct": true, "explanation": "In Azure ML Studio, one can create ML workspace (incl. new resource group if needed)"},
            {"id": "D", "text": "Use Azure CLI v2 `az group create -n <resource-group> -l <location> && az ml workspace create -n <workspace-name> -g <resource-group>`", "is_correct": true, "explanation": "This will work."},
            {"id": "E", "text": "Use Azure ML Python SDK (v2) after creating resource group\n\n```python\nml_client = MLClient.from_config()\nworkspace = ml_client.get(workspace_name)\n```", "is_correct": false, "explanation": "`.get` does not create."},
            {"id": "F", "text": "Use Azure ML Python SDK (v2) after creating resource group\n\n```python\nml_client = MLClient.from_config()\nworkspace = ml_client.begin_create(workspace_name)\n```", "is_correct": true, "explanation": "Correct if resource group exists."}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_88ef956645bf49df953f08f089e8ffab",
        "type": "ordering",
        "question": "You are using a Git repository to track work in an Azure Machine Learning workspace.\n\nYou need to authenticate a Git account by using SSH.\n\nWhich three actions should you perform in sequence?",
        "options": [
            {
                "id": "A",
                "text": "Generate a public/private key pair",
                "explanation": "`ssh-keygen -t ed25519` you@example.com"
            },
            {
                "id": "B",
                "text": "Add the private key to the Git account",
                "explanation": "yeah.. maybe don't."
            },
            {
                "id": "C",
                "text": "Clone the Git repository by using a SSH repository URL",
                "explanation": "`git clone git@github.com:your-username/your-repo.git`"
            },
            {
                "id": "D",
                "text": "Add the public key to the Git account",
                "explanation": "yes"
            },
            {
                "id": "E",
                "text": "Create a new Azue Key Vault resources"
            }
        ],
        "correct_order": [
            "A",
            "D",
            "C"
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_01a083e685344e038d9298339b77d790",
        "type": "multiple_choice_single_answer",
        "question": "You train classification models by using automated machine learning.\n\nYou must evaluate automated machine learning experiment results. You must use charts generated by automated machine learning.\n\nYou need to choose a chart type for each model type.\n\nWhich chart types should you use?",
        "options": [
            {"id": "A", "text": "Confusion Matrix", "is_correct": true},
            {"id": "B", "text": "Predicted vs. True", "is_correct": false},
            {"id": "C", "text": "Calibration curve", "is_correct": false}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_01a083e685344e038d9298339b77d791",
        "type": "multiple_choice_single_answer",
        "question": "You train regression models by using automated machine learning.\n\nYou must evaluate automated machine learning experiment results. You must use charts generated by automated machine learning.\n\nYou need to choose a chart type for each model type.\n\nWhich chart types should you use?",
        "options": [
            {"id": "A", "text": "Confusion Matrix", "is_correct": false},
            {"id": "B", "text": "Predicted vs. True", "is_correct": true},
            {"id": "C", "text": "Calibration curve", "is_correct": false}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_1547e562708f46888215bf97233dac65",
        "type": "multiple_choice_multiple_answer",
        "question": "You create an Azure Data Lake Storage Gen2 storage account named storage1containing a file system named fs1 and a folder named folder1.\n\nThe contents of folder1 must be accessible from jobs on compute targets in the Azure Machine Learning workspace.\n\nYou need to construct a URI to reference folder1.\n\nHow should you construct the URI `<1>://<2>`?",
        "options": [
            {"id": "A", "text": "(1) `https`", "is_correct": false},
            {"id": "B", "text": "(1) `abfss`", "is_correct": true},
            {"id": "C", "text": "(1) `azureml`", "is_correct": false},
            {"id": "D", "text": "(2) `fs1@storage1.dfs.core.windows.net/folder1/`", "is_correct": true},
            {"id": "E", "text": "(2) `storage1.blobl.core.windows.net/fs1/folder1/`", "is_correct": false},
            {"id": "F", "text": "(2) `datastores/storages1/paths/fs1/folder1/`", "is_correct": false}
        ],
        "feedback": "",
        "include_in_bank": true
    },
    {
        "question_id": "examtopics_e3081345094d46f8be1903596a73a7c3",
        "type": "multiple_choice_multiple_answer",
        "question": "You train a model by using Azure Machine Learning. You use Azure Blob Storage to store production data.\n\nThe model must be re-trained when new data is uploaded to Azure Blob Storage. You need to minimize development and coding.\n\nYou need to configure Azure services to develop a re-training solution.\n\nWhich Azure services should you use to:\n\n1. Identify when new data is uploaded.\n\n2. Trigger re-training",
        "options": [
            {"id": "A", "text": "(1) Event Grid", "is_correct": true},
            {"id": "B", "text": "(1) Event Hubs", "is_correct": false},
            {"id": "C", "text": "(1) Functions", "is_correct": false},
            {"id": "D", "text": "(2) Event Grid", "is_correct": false},
            {"id": "E", "text": "(2) Functions", "is_correct": false},
            {"id": "F", "text": "(2) Logic Apps", "is_correct": true}
        ],
        "feedback": "Answers say Function -> Event Grid\n\nCommunity says Event Grid -> Logic Apps.",
        "include_in_bank": true
    },
    {
        "question_id": "ai-1",
        "type": "multiple_choice_single_answer",
        "question": "You want to train multiple regression models automatically and select the best one based on performance. You also want to minimize manual feature engineering and model selection.\n\nWhich Azure ML feature should you use?",
        "options": [
            {
                "id": "A",
                "text": "Azure AutoML",
                "is_correct": true,
                "explanation": "Azure AutoML automates model selection, hyperparameter tuning, and feature engineering, making it ideal for tasks like regression without manual intervention."
            },
            {
                "id": "B",
                "text": "Azure ML Pipelines",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Data Factory",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Monitor",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "Azure AutoML is designed to automate the end-to-end process of model training, especially for regression, classification, and time series forecasting tasks.",
        "include_in_bank": true
    },
    {
        "question_id": "ai-2",
        "type": "multiple_choice_single_answer",
        "question": "You trained a machine learning model in Azure Machine Learning and want to deploy it as a REST endpoint for low-latency predictions.\n\nWhich deployment target should you choose?",
        "options": [
            {
                "id": "A",
                "text": "Azure Kubernetes Service (AKS)",
                "is_correct": true,
                "explanation": "AKS is the recommended option for deploying models requiring high scalability and low-latency predictions in real-time."
            },
            {
                "id": "B",
                "text": "Azure Data Lake Gen2",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "C",
                "text": "Azure Machine Learning Compute Instance",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "Azure Batch",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "Azure Kubernetes Service (AKS) supports real-time inference with autoscaling and high availability. Azure Batch is suitable for batch inference scenarios.",
        "include_in_bank": true
    },
    {
        "question_id": "ai-3",
        "type": "multiple_choice_single_answer",
        "question": "You are working in Azure Machine Learning Studio and need to load training data from an Azure SQL Database.\n\nWhich method should you use to read the data into a DataFrame using Python?",
        "options": [
            {
                "id": "A",
                "text": "pandas.read_csv('azure://mydatabase.database.windows.net/mytable')",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "B",
                "text": "pandas.read_sql_query('SELECT * FROM mytable', connection)",
                "is_correct": true,
                "explanation": "Using `pandas.read_sql_query()` with a valid SQL connection is the correct method to load data from Azure SQL Database into a DataFrame."
            },
            {
                "id": "C",
                "text": "open('azure://mydatabase.database.windows.net/mytable').read()",
                "is_correct": false,
                "explanation": ""
            },
            {
                "id": "D",
                "text": "spark.read.text('azure://mydatabase.database.windows.net/mytable')",
                "is_correct": false,
                "explanation": ""
            }
        ],
        "feedback": "Use `pandas.read_sql_query()` to load data from a relational database like Azure SQL into a DataFrame. Ensure you have an established connection using `pyodbc` or `sqlalchemy`.",
        "include_in_bank": true
    }
]